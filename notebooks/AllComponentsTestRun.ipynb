{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllComponents Test Run\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AseiSugiyama/TFXTestRun/blob/master/notebooks/AllComponentsTestRun.ipynb)\n",
    "\n",
    "## Set up\n",
    "\n",
    "TFX requires apache-airflow and docker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install 'apache-airflow[gcp_api]' docker tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TFMA (TensorFlow Model Analysis)](https://github.com/tensorflow/model-analysis) requires some jupyter extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this notebook on python3 runtime of Google Colab, you should install TFMA in python 2 environment.\n",
    "# See also : https://github.com/googlecolab/colabtools/issues/60\n",
    "# !pip2 install tensorflow_model_analysis -U\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!jupyter nbextension install --py --symlink tensorflow_model_analysis\n",
    "!jupyter nbextension enable --py tensorflow_model_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use TFX version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tfx\n",
    "tfx.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX requires TensorFlow >= 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX supports Python 3.5 from version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This enables you to run this notebook twice.\n",
    "# There should not be train/eval files at ~/taxi/data, since TFX can handle only single file with version 0.13.0\n",
    "if [ -e ~/taxi/data ]; then\n",
    "    rm -rf ~/taxi/data\n",
    "fi\n",
    "\n",
    "# download taxi data\n",
    "mkdir -p ~/taxi/data/simple\n",
    "mkdir -p ~/taxi/serving_model/taxi_simple\n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv -O ~/taxi/data/simple/data.csv\n",
    "\n",
    "# download \n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/r0.13/tfx/examples/chicago_taxi_pipeline/taxi_utils.py -O ~/taxi/taxi_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.protobuf import json_format\n",
    "\n",
    "from tfx.components.base.base_component import ComponentOutputs\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.orchestration.tfx_runner import TfxRunner\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import csv_input\n",
    "from tfx.utils.channel import Channel\n",
    "from tfx.utils import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This example assumes that the taxi data is stored in ~/taxi/data and the\n",
    "# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n",
    "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
    "_data_root = os.path.join(_taxi_root, 'data/simple')\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')\n",
    "\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(_taxi_root, 'serving_model/taxi_simple')\n",
    "\n",
    "# Directory and data locations.  This example assumes all of the chicago taxi\n",
    "# example code and metadata library is relative to $HOME, but you can store\n",
    "# these files anywhere on your local filesystem.\n",
    "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines')\n",
    "_metadata_db_root = os.path.join(_tfx_root, 'metadata')\n",
    "_log_root = os.path.join(_tfx_root, 'logs')\n",
    "\n",
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "_airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2019, 1, 1),\n",
    "}\n",
    "\n",
    "# Logging overrides\n",
    "logger_overrides = {'log_root': _log_root, 'log_level': logging.INFO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Implements the chicago taxi pipeline with TFX.\"\"\"\n",
    "examples = csv_input(_data_root)\n",
    "\n",
    "# Brings data into the pipeline or otherwise joins/converts training data.\n",
    "train_config = example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2)\n",
    "eval_config = example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        train_config,\n",
    "        eval_config\n",
    "    ]))\n",
    "\n",
    "# Create outputs\n",
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root, 'csv_example_gen/train/')\n",
    "\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root, 'csv_example_gen/eval/')\n",
    "\n",
    "example_outputs = ComponentOutputs({\n",
    "    'examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    ),\n",
    "    'training_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples]\n",
    "    ),\n",
    "    'eval_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[eval_examples]\n",
    "    ),    \n",
    "})\n",
    "\n",
    "example_gen = CsvExampleGen(\n",
    "    name=\"CSV ExampleGen Component\",\n",
    "    input_base=examples, # A Channel of 'ExternalPath' type, it contains path of data source.\n",
    "    output_config=output_config,  # An example_gen_pb2.Output instance, it contains train-eval split ratio.\n",
    "    outputs=example_outputs # dict from name to output channel, it will be stored example_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='train')\n",
    "train_statistics.uri = os.path.join(_data_root, 'statistics_gen/train/')\n",
    "\n",
    "eval_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='eval')\n",
    "eval_statistics.uri = os.path.join(_data_root, 'statistics_gen/eval/')\n",
    "\n",
    "statistics_outputs = ComponentOutputs({\n",
    "    'output': Channel(\n",
    "        type_name='ExampleStatisticsPath',\n",
    "        static_artifact_collection=[train_statistics, eval_statistics]\n",
    "    )\n",
    "})\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    name='StatisticsGen Component', # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    input_data=example_gen.outputs.examples, # A Channel of 'ExamplesPath' type, it is equal to example_outputs\n",
    "    outputs=statistics_outputs # dict from name to output channel, it will be stored statistics_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_schema_path = types.TfxType(type_name='SchemaPath', split='train')\n",
    "train_schema_path.uri = os.path.join(_data_root, 'schema_gen/')\n",
    "\n",
    "# NOTE: SchemaGen.executor can handle JUST ONE SchemaPath.\n",
    "# Two or more SchemaPaths will cause ValueError\n",
    "# such as \"ValueError: expected list length of one but got 2\".\n",
    "schema_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='SchemaPath',\n",
    "        static_artifact_collection=[train_schema_path] \n",
    "    )\n",
    "})\n",
    "\n",
    "infer_schema = SchemaGen(\n",
    "    name='SchemaGen Component',  # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    stats=statistics_gen.outputs.output, # A Channel of 'ExampleStatisticsPath' type, it is equal to statistics_outputs\n",
    "    outputs=schema_outputs # dict from name to output channel, it will be stored schema_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_validator_path = types.TfxType('ExampleValidationPath', split='eval')\n",
    "eval_validator_path.uri = os.path.join(_data_root, 'eval/validation/')\n",
    "\n",
    "# NOTE: ExampleValidator.executor can handle JUST ONE ExampleValidationPath\n",
    "# since ExampleValidator should check only 'eval' data.\n",
    "# Two or more ExampleValidationPath will cause ValueError\n",
    "# such as \"ValueError: expected list length of one but got 2\".\n",
    "validator_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='ExampleValidationPath',\n",
    "        static_artifact_collection=[eval_validator_path]\n",
    "    )\n",
    "})\n",
    "\n",
    "example_validator = ExampleValidator(\n",
    "    stats=statistics_gen.outputs.output,\n",
    "    schema=infer_schema.outputs.output,\n",
    "    name='Example Validator',\n",
    "    outputs=validator_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root,\n",
    "                                  'transform/transformed_examples/train/')\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root,\n",
    "                                 'transform/transformed_examples/eval/')\n",
    "transform_output = types.TfxType(type_name='TransformPath')\n",
    "transform_output.uri = os.path.join(_data_root,\n",
    "                                    'transform/transform_output/')\n",
    "\n",
    "transform_outputs = ComponentOutputs({\n",
    "    # Output of 'tf.Transform', which includes an exported \n",
    "    # Tensorflow graph suitable for both training and serving\n",
    "    'transform_output':Channel(\n",
    "        type_name='TransformPath',\n",
    "        static_artifact_collection=[transform_output]\n",
    "    ),\n",
    "    # transformed_examples: Materialized transformed examples, which includes \n",
    "    # both 'train' and 'eval' splits.\n",
    "    'transformed_examples':Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    )\n",
    "})\n",
    "\n",
    "transform = Transform(\n",
    "    name=\"Transform Component\",\n",
    "    input_data=example_gen.outputs.examples,\n",
    "    schema=infer_schema.outputs.output,\n",
    "    module_file=_taxi_module_file,\n",
    "    outputs=transform_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_exports = types.TfxType(type_name='ModelExportPath')\n",
    "model_exports.uri = os.path.join(_data_root, 'trainer/current/')\n",
    "\n",
    "trainer_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='ModelExportPath',\n",
    "        static_artifact_collection=[model_exports]\n",
    "    )\n",
    "})\n",
    "\n",
    "trainer = Trainer(\n",
    "    name='Trainer Component',\n",
    "    module_file=_taxi_module_file,\n",
    "    transformed_examples=transform.outputs.transformed_examples,\n",
    "    schema=infer_schema.outputs.output,\n",
    "    transform_output=transform.outputs.transform_output,\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=5000),\n",
    "    outputs=trainer_outputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_output = types.TfxType('ModelEvalPath')\n",
    "eval_output.uri = os.path.join(_data_root, 'eval_output/')\n",
    "\n",
    "model_analyzer_outputs = ComponentOutputs({\n",
    "    'output':\n",
    "    Channel(\n",
    "        type_name='ModelEvalPath',\n",
    "        static_artifact_collection=[eval_output]),\n",
    "})\n",
    "\n",
    "feature_slicing_spec = evaluator_pb2.FeatureSlicingSpec(specs=[\n",
    "    evaluator_pb2.SingleSlicingSpec(\n",
    "        column_for_slicing=['trip_start_hour'])\n",
    "])\n",
    "\n",
    "model_analyzer = Evaluator(\n",
    "    name='Evaluator Component',\n",
    "    examples=example_gen.outputs.examples,\n",
    "    model_exports=trainer.outputs.output,\n",
    "    feature_slicing_spec=feature_slicing_spec,\n",
    "    outputs=model_analyzer_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _Do(self, input_dict, output_dict, exec_properties):\n",
    "    import apache_beam as beam\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_model_analysis as tfma\n",
    "    from typing import Any, Dict, List, Text\n",
    "    from tfx.components.base import base_executor\n",
    "    from tfx.proto import evaluator_pb2\n",
    "    from tfx.utils import io_utils\n",
    "    from tfx.utils import path_utils\n",
    "    from tfx.utils import types\n",
    "    from google.protobuf import json_format\n",
    "\n",
    "    \"\"\"Runs a batch job to evaluate the eval_model against the given input.\n",
    "    Args:\n",
    "      input_dict: Input dict from input key to a list of Artifacts.\n",
    "        - model_exports: exported model.\n",
    "        - examples: examples for eval the model.\n",
    "      output_dict: Output dict from output key to a list of Artifacts.\n",
    "        - output: model evaluation results.\n",
    "      exec_properties: A dict of execution properties.\n",
    "        - feature_slicing_spec: JSON string of evaluator_pb2.FeatureSlicingSpec\n",
    "          instance, providing the way to slice the data.\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    if 'model_exports' not in input_dict:\n",
    "      raise ValueError('\\'model_exports\\' is missing in input dict.')\n",
    "    if 'examples' not in input_dict:\n",
    "      raise ValueError('\\'examples\\' is missing in input dict.')\n",
    "    if 'output' not in output_dict:\n",
    "      raise ValueError('\\'output\\' is missing in output dict.')\n",
    "\n",
    "    self._log_startup(input_dict, output_dict, exec_properties)\n",
    "\n",
    "    # Extract input artifacts\n",
    "    model_exports_uri = types.get_single_uri(input_dict['model_exports'])\n",
    "\n",
    "    feature_slicing_spec = evaluator_pb2.FeatureSlicingSpec()\n",
    "    json_format.Parse(exec_properties['feature_slicing_spec'],\n",
    "                      feature_slicing_spec)\n",
    "    slice_spec = self._get_slice_spec_from_feature_slicing_spec(\n",
    "        feature_slicing_spec)\n",
    "\n",
    "    output_uri = types.get_single_uri(output_dict['output'])\n",
    "\n",
    "    eval_model_path = path_utils.eval_model_path(model_exports_uri)\n",
    "\n",
    "    tf.logging.info('Using {} for model eval.'.format(eval_model_path))\n",
    "    eval_shared_model = tfma.default_eval_shared_model(\n",
    "        add_metrics_callbacks=[\n",
    "                        # calibration_plot_and_prediction_histogram computes calibration plot and prediction\n",
    "                        # distribution at different thresholds.\n",
    "                        tfma.post_export_metrics.calibration_plot_and_prediction_histogram(),\n",
    "                        # auc_plots enables precision-recall curve and ROC visualization at different thresholds.\n",
    "                        tfma.post_export_metrics.auc_plots()\n",
    "                    ],\n",
    "        eval_saved_model_path=eval_model_path)\n",
    "\n",
    "    tf.logging.info('Evaluating model.')\n",
    "    with beam.Pipeline(argv=self._get_beam_pipeline_args()) as pipeline:\n",
    "      # pylint: disable=expression-not-assigned\n",
    "      (pipeline\n",
    "       | 'ReadData' >> beam.io.ReadFromTFRecord(\n",
    "           file_pattern=io_utils.all_files_pattern(\n",
    "               types.get_split_uri(input_dict['examples'], 'eval')))\n",
    "       |\n",
    "       'ExtractEvaluateAndWriteResults' >> tfma.ExtractEvaluateAndWriteResults(\n",
    "           eval_shared_model=eval_shared_model,\n",
    "           slice_spec=slice_spec,\n",
    "           output_path=output_uri))\n",
    "    tf.logging.info(\n",
    "        'Evaluation complete. Results written to {}.'.format(output_uri))\n",
    "\n",
    "model_analyzer.executor.Do = _Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blessing = types.TfxType(type_name='ModelBlessingPath')\n",
    "blessing.uri = os.path.join(_data_root, 'model_validator/blessed/')\n",
    "\n",
    "results = types.TfxType(type_name='ModelValidationPath')\n",
    "results.uri = os.path.join(_data_root, 'model_validator/results/')\n",
    "\n",
    "model_validator_outputs = ComponentOutputs({\n",
    "    'blessing':\n",
    "    Channel(\n",
    "        type_name='ModelBlessingPath',\n",
    "        static_artifact_collection=[blessing]),\n",
    "    'results':\n",
    "    Channel(\n",
    "        type_name='ModelValidationPath',\n",
    "        static_artifact_collection=[results]),\n",
    "})\n",
    "\n",
    "model_validator = ModelValidator(\n",
    "    name='Model Validator Component',\n",
    "    examples=example_gen.outputs.examples, \n",
    "    model=trainer.outputs.output,\n",
    "    outputs=model_validator_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pusher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config={\n",
    "    # If custom_config contains 'cmle_serving_args', Pusher will try to push ml model to AI Platform (GCE).\n",
    "    # However, this config will be deplecated in next release of TFX.\n",
    "    # To run it localy, we use empty dictionary .\n",
    "}\n",
    "\n",
    "pusher = Pusher(\n",
    "    name='Pusher Component',\n",
    "    model_export=trainer.outputs.output,\n",
    "    model_blessing=model_validator.outputs.blessing,\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=_serving_model_dir)),\n",
    "    outputs='',\n",
    "    custom_config=custom_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"TFX Pipeline\",\n",
    "    pipeline_root=_pipeline_root,\n",
    "    components=[\n",
    "        example_gen, \n",
    "        statistics_gen, \n",
    "        infer_schema, \n",
    "        example_validator,\n",
    "        transform, \n",
    "        trainer, \n",
    "        model_analyzer, \n",
    "        model_validator,\n",
    "        pusher,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_directrun_config = {\n",
    "    'metadata_db_root':None,\n",
    "    'metadata_connection_config':None,\n",
    "    'additional_pipeline_args':{\n",
    "        'beam_pipeline_args':None,\n",
    "    },\n",
    "    'docker_operator_cfg':None,\n",
    "    'enable_cache':None,\n",
    "}\n",
    "\n",
    "class DirectRunner(TfxRunner):\n",
    "    \"\"\"Tfx runner on local\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self._config = config or {}\n",
    "    \n",
    "    def run(self, pipeline):\n",
    "        # Merge airflow-specific configs with pipeline args\n",
    "        self._config.update(pipeline.pipeline_args)\n",
    "        \n",
    "        for component in pipeline.components:\n",
    "            self._execute_component(component)\n",
    "            \n",
    "        return pipeline\n",
    "            \n",
    "    def _execute_component(self, component):\n",
    "        # parse parameters\n",
    "        input_dict = {key:value.get() for key, value in component.input_dict.items()}\n",
    "        output_dict = {key: value.get() for key, value in component.outputs.get_all().items()}\n",
    "        exec_properties = component.exec_properties\n",
    "        \n",
    "        # create executor\n",
    "        additional_pipeline_args = self._config.get('additional_pipeline_args') or {}\n",
    "        executor = component.executor(beam_pipeline_args=additional_pipeline_args.get('beam_pipeline_args'))\n",
    "        \n",
    "        executor.Do(input_dict, output_dict, exec_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DirectRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Result\n",
    "\n",
    "We will check followings:\n",
    "\n",
    "0. Artifacts\n",
    "1. Statistics\n",
    "2. Schema\n",
    "3. Anomalies\n",
    "4. Model Evaluation\n",
    "\n",
    "### Check artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -Rlhs /root/taxi/data/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check statistics\n",
    "\n",
    "`anomalies.pbtxt` is only simple text file. To visualize it, we are going to;\n",
    "\n",
    "1. get the path of `anomalies.pbtxt`from `example_validator`\n",
    "2. parse `anomalies.pbtxt` into anomalies (protobuf)\n",
    "3. visualize schema with [tfdv](https://www.tensorflow.org/tfx/data_validation/get_started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_directories(statistics_gen):\n",
    "    output_dict = {key: value.get() for key, value in statistics_gen.outputs.get_all().items()}\n",
    "    input_dict = {key:value.get() for key, value in statistics_gen.input_dict.items()}\n",
    "    split_to_instance = {x.split: x for x in input_dict['input_data']}\n",
    "    directories = [types.get_split_uri(output_dict['output'], split) for split, instance in split_to_instance.items()]\n",
    "    return directories\n",
    "\n",
    "directories = get_stats_directories(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from tfx.utils import io_utils\n",
    "\n",
    "train_stats = tfdv.load_statistics(io_utils.get_only_uri_in_dir(directories[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(train_stats)\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Schema\n",
    "\n",
    "`schema.pbtxt` is only simple text file, we are going to;\n",
    "\n",
    "1. get the path of `schema.pbtxt`from `schema_gen`\n",
    "2. parse `schema.pbtxt` into schema (protobuf)\n",
    "3. visualize schema with [tfdv](https://www.tensorflow.org/tfx/data_validation/get_started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the path of `schema.pbtxt`\n",
    "def get_schema_directory(infer_schema):\n",
    "    output_dict = {key: value.get() for key, value in infer_schema.outputs.get_all().items()}\n",
    "    input_dict = {key:value.get() for key, value in infer_schema.input_dict.items()}\n",
    "    split_to_instance = {x.split: x for x in input_dict['stats']}\n",
    "    directory = types.get_split_uri(output_dict['output'], 'train')\n",
    "    return directory\n",
    "\n",
    "schema_directory = get_schema_directory(infer_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. parse schema.pbtxt\n",
    "\n",
    "from pathlib import Path\n",
    "from google.protobuf.text_format import Parse\n",
    "from google.protobuf.message import Message\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "def parse_schema_proto_string(schema_directory):\n",
    "    path = Path(schema_directory)\n",
    "    for filepath in path.glob('*'):\n",
    "        with open(str(filepath), 'r') as file: # since we are using python 3.5, not 3.6+\n",
    "            schema_proto_string = file.read()\n",
    "            schema = schema_pb2.Schema()\n",
    "            Parse(schema_proto_string, schema)\n",
    "            return schema\n",
    "\n",
    "schema = parse_schema_proto_string(schema_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. visualize schema with tfdv\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check anomalies\n",
    "\n",
    "`anomalies.pbtxt` is only simple text file. To visualize it, we are going to;\n",
    "\n",
    "1. get the path of `anomalies.pbtxt`from `example_validator`\n",
    "2. parse `anomalies.pbtxt` into anomalies (protobuf)\n",
    "3. visualize schema with [tfdv](https://www.tensorflow.org/tfx/data_validation/get_started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the path of `schema.pbtxt`\n",
    "def get_validation_directory(example_validator):\n",
    "    output_dict = {key: value.get() for key, value in example_validator.outputs.get_all().items()}\n",
    "    directory = types.get_split_uri(output_dict['output'], 'eval')\n",
    "    return directory\n",
    "\n",
    "validation_directory = get_validation_directory(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. parse schema.pbtxt\n",
    "\n",
    "from pathlib import Path\n",
    "from google.protobuf.text_format import Parse\n",
    "from google.protobuf.message import Message\n",
    "from google.protobuf import text_format\n",
    "from tensorflow_metadata.proto.v0 import anomalies_pb2\n",
    "\n",
    "def parse_anomalies_proto_string(validation_directory):\n",
    "    path = Path(validation_directory)\n",
    "    for filepath in path.glob('*'):\n",
    "        with open(str(filepath), 'r') as file: # since we are using python 3.5, not 3.6+\n",
    "            anomalies_proto_string = file.read()\n",
    "            anomalies = anomalies_pb2.Anomalies()\n",
    "            text_format.Parse(anomalies_proto_string, anomalies)\n",
    "            return anomalies\n",
    "\n",
    "anomalies = parse_anomalies_proto_string(validation_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. visualize anomalies with tfdv\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_dir(model_analyzer):\n",
    "    artifact = model_analyzer.outputs.output.get()\n",
    "    return types.get_single_uri(artifact)\n",
    "    \n",
    "eval_dir = get_eval_dir(model_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "result = tfma.load_eval_result(eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(result, slicing_column='trip_start_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "tfma.view.render_plot(result, tfma.slicer.SingleSliceSpec(features=[('trip_start_hour', 10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
