{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Test Run\n",
    "\n",
    "## Set up\n",
    "\n",
    "TFX requires apache-airflow and docker SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-airflow[gcp] in /usr/local/lib/python3.5/dist-packages (1.10.3)\n",
      "\u001b[33m  WARNING: apache-airflow 1.10.3 does not provide the extra 'gcp'\u001b[0m\n",
      "Requirement already satisfied: docker in /usr/local/lib/python3.5/dist-packages (4.0.2)\n",
      "Requirement already satisfied: tfx in /usr/local/lib/python3.5/dist-packages (0.13.0)\n",
      "Requirement already satisfied: gitpython>=2.0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.11)\n",
      "Requirement already satisfied: pandas<1.0.0,>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.24.2)\n",
      "Requirement already satisfied: gunicorn<20.0,>=19.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (19.9.0)\n",
      "Requirement already satisfied: alembic<1.0,>=0.9 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.9.10)\n",
      "Requirement already satisfied: tabulate<0.9,>=0.7.5 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.8.3)\n",
      "Requirement already satisfied: iso8601>=0.1.12 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.1.12)\n",
      "Requirement already satisfied: psutil<6.0.0,>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (5.6.3)\n",
      "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: dill<0.3,>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.9)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.22.0)\n",
      "Requirement already satisfied: croniter<0.4,>=0.3.17 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.3.30)\n",
      "Requirement already satisfied: sqlalchemy<1.3.0,>=1.1.15 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2.19)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.8.0)\n",
      "Requirement already satisfied: pygments<3.0,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.3.1)\n",
      "Requirement already satisfied: flask-appbuilder==1.12.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.12.3)\n",
      "Requirement already satisfied: flask-swagger==0.2.13 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.13)\n",
      "Requirement already satisfied: setproctitle<2,>=1.1.8 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.1.10)\n",
      "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2)\n",
      "Requirement already satisfied: pendulum==1.4.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.4.4)\n",
      "Requirement already satisfied: flask-caching<1.4.0,>=1.3.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.3.3)\n",
      "Requirement already satisfied: flask<2.0,>=1.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.3)\n",
      "Requirement already satisfied: funcsigs==1.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.0)\n",
      "Requirement already satisfied: json-merge-patch==0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2)\n",
      "Requirement already satisfied: thrift>=0.9.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.11.0)\n",
      "Requirement already satisfied: markdown<3.0,>=2.5.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.6.11)\n",
      "Requirement already satisfied: tzlocal>=1.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.1)\n",
      "Requirement already satisfied: flask-wtf<0.15,>=0.14.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.2)\n",
      "Requirement already satisfied: configparser<3.6.0,>=3.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (3.5.3)\n",
      "Requirement already satisfied: tenacity==4.12.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.12.0)\n",
      "Requirement already satisfied: flask-admin==1.5.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.3)\n",
      "Requirement already satisfied: future<0.17,>=0.16.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.16.0)\n",
      "Requirement already satisfied: werkzeug<0.15.0,>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: jinja2<=2.10.0,>=2.7.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.10)\n",
      "Requirement already satisfied: lxml>=4.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.3.4)\n",
      "Requirement already satisfied: zope.deprecation<5.0,>=4.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.4.0)\n",
      "Requirement already satisfied: python-daemon<2.2,>=2.1.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.2)\n",
      "Requirement already satisfied: flask-login<0.5,>=0.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.4.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.5/dist-packages (from docker) (0.56.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.5/dist-packages (from docker) (1.12.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.5/dist-packages (from tfx) (1.7.9)\n",
      "Requirement already satisfied: protobuf<4,>=3.7 in /usr/local/lib/python3.5/dist-packages (from tfx) (3.7.1)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.12 in /usr/local/lib/python3.5/dist-packages (from tfx) (2.13.0)\n",
      "Requirement already satisfied: absl-py<1,>=0.1.6 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.7.1)\n",
      "Requirement already satisfied: tensorflow-data-validation<0.14,>=0.13.1 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.1)\n",
      "Requirement already satisfied: tensorflow-transform<0.14,>=0.13 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.0)\n",
      "Requirement already satisfied: ml-metadata<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas<1.0.0,>=0.17.1->apache-airflow[gcp]) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas<1.0.0,>=0.17.1->apache-airflow[gcp]) (1.16.3)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.4)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.12)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2019.6.16)\n",
      "Requirement already satisfied: Flask-OpenID<2,>=1.2.5 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (1.2.5)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.4.1)\n",
      "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.4.0)\n",
      "Requirement already satisfied: click<8,>=6.7 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (7.0)\n",
      "Requirement already satisfied: Flask-Babel<1,>=0.11.1 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.5/dist-packages (from flask-swagger==0.2.13->apache-airflow[gcp]) (3.13)\n",
      "Requirement already satisfied: pytzdata>=2018.3.0.0 in /usr/local/lib/python3.5/dist-packages (from pendulum==1.4.4->apache-airflow[gcp]) (2019.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.5/dist-packages (from flask<2.0,>=1.0->apache-airflow[gcp]) (1.1.0)\n",
      "Requirement already satisfied: ordereddict in /usr/local/lib/python3.5/dist-packages (from funcsigs==1.0.0->apache-airflow[gcp]) (1.1)\n",
      "Requirement already satisfied: WTForms in /usr/local/lib/python3.5/dist-packages (from flask-wtf<0.15,>=0.14.2->apache-airflow[gcp]) (2.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.5/dist-packages (from jinja2<=2.10.0,>=2.7.3->apache-airflow[gcp]) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from zope.deprecation<5.0,>=4.0->apache-airflow[gcp]) (41.0.1)\n",
      "Requirement already satisfied: docutils in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.14)\n",
      "Requirement already satisfied: lockfile>=0.10 in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.6.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.12.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (7.4.2)\n",
      "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.1.0)\n",
      "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.0.0)\n",
      "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.21.24)\n",
      "Requirement already satisfied: grpcio<2,>=1.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.20.1)\n",
      "Requirement already satisfied: pydot<1.3,>=1.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.2.4)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.0.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.5.7)\n",
      "Requirement already satisfied: pyarrow<0.14.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.13.0)\n",
      "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.9.0)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.0.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.39.1)\n",
      "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.1.1)\n",
      "Requirement already satisfied: google-cloud-core<0.30.0,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.29.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.6.1)\n",
      "Requirement already satisfied: google-cloud-bigtable<0.33.0,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.32.2)\n",
      "Requirement already satisfied: google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7.4)\n",
      "Requirement already satisfied: google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.5.28)\n",
      "Requirement already satisfied: scikit-learn<1,>=0.18 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.21.2)\n",
      "Requirement already satisfied: joblib<1,>=0.12 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.2)\n",
      "Requirement already satisfied: IPython>=5.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (7.5.0)\n",
      "Requirement already satisfied: tensorflow-metadata<0.14,>=0.12.1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.0)\n",
      "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitdb2>=2.0.0->gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: python3-openid>=2.0 in /usr/local/lib/python3.5/dist-packages (from Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (3.1.0)\n",
      "Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.5/dist-packages (from Flask-Babel<1,>=0.11.1->flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.7.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (0.2.5)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.4.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.3.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.1.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.5.0)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.3)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.7.8)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot<1.3,>=1.2.0->apache-beam[gcp]<3,>=2.12->tfx) (2.4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (0.4.5)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.5/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.12->tfx) (0.6.2)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.5/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.12->tfx) (5.2.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.12dev,>=0.11.4 in /usr/local/lib/python3.5/dist-packages (from google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.11.4)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.6.0 in /usr/local/lib/python3.5/dist-packages (from google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.13.0)\n",
      "Requirement already satisfied: google-resumable-media>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.3.2)\n",
      "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.5/dist-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.4.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.7.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (2.0.9)\n",
      "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.5/dist-packages (from tensorflow-metadata<0.14,>=0.12.1->tensorflow-data-validation<0.14,>=0.13.1->tfx) (1.6.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.5/dist-packages (from python3-openid>=2.0->Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.0.1)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.5/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.2)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.5/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.2.4)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.4.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.4.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.1.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.3)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.4)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (18.0.1)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.6.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.2)\n",
      "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.5/dist-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.5)\n",
      "Requirement already satisfied: parso>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from jedi>=0.10->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.4.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.5/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.15.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.5/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.5.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'apache-airflow[gcp]' docker tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use TFX version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.0'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tfx\n",
    "tfx.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX requires TensorFlow >= 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX supports Python 3.5 from version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 (default, Nov 12 2018, 13:43:14) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-06-25 11:22:15--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1922668 (1.8M) [text/plain]\n",
      "Saving to: ‘/root/taxi/data/simple/data.csv’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2% 3.76M 0s\n",
      "    50K .......... .......... .......... .......... ..........  5% 4.83M 0s\n",
      "   100K .......... .......... .......... .......... ..........  7% 6.48M 0s\n",
      "   150K .......... .......... .......... .......... .......... 10% 5.43M 0s\n",
      "   200K .......... .......... .......... .......... .......... 13% 5.97M 0s\n",
      "   250K .......... .......... .......... .......... .......... 15% 5.88M 0s\n",
      "   300K .......... .......... .......... .......... .......... 18% 10.1M 0s\n",
      "   350K .......... .......... .......... .......... .......... 21% 4.94M 0s\n",
      "   400K .......... .......... .......... .......... .......... 23% 8.34M 0s\n",
      "   450K .......... .......... .......... .......... .......... 26% 5.57M 0s\n",
      "   500K .......... .......... .......... .......... .......... 29% 22.0M 0s\n",
      "   550K .......... .......... .......... .......... .......... 31% 4.35M 0s\n",
      "   600K .......... .......... .......... .......... .......... 34% 12.9M 0s\n",
      "   650K .......... .......... .......... .......... .......... 37% 10.0M 0s\n",
      "   700K .......... .......... .......... .......... .......... 39% 9.95M 0s\n",
      "   750K .......... .......... .......... .......... .......... 42% 5.47M 0s\n",
      "   800K .......... .......... .......... .......... .......... 45% 10.0M 0s\n",
      "   850K .......... .......... .......... .......... .......... 47% 14.2M 0s\n",
      "   900K .......... .......... .......... .......... .......... 50% 8.12M 0s\n",
      "   950K .......... .......... .......... .......... .......... 53% 8.64M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 55% 10.0M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 58% 13.2M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 61% 6.81M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 63% 4.39M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 66% 8.85M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 69% 10.1M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 71% 4.92M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 74% 67.5M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 77% 13.4M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 79% 14.9M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 82% 13.8M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 85% 9.65M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 87% 11.4M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 90% 16.9M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 93% 12.6M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 95% 16.9M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 98% 10.8M 0s\n",
      "  1850K .......... .......... .......                         100%  137M=0.2s\n",
      "\n",
      "2019-06-25 11:22:15 (8.21 MB/s) - ‘/root/taxi/data/simple/data.csv’ saved [1922668/1922668]\n",
      "\n",
      "--2019-06-25 11:22:15--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12084 (12K) [text/plain]\n",
      "Saving to: ‘/root/taxi/taxi_utils.py’\n",
      "\n",
      "     0K .......... .                                          100% 20.7M=0.001s\n",
      "\n",
      "2019-06-25 11:22:16 (20.7 MB/s) - ‘/root/taxi/taxi_utils.py’ saved [12084/12084]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# This enables you to run this notebook twice.\n",
    "# There should not be train/eval files at ~/taxi/data, since TFX can handle only single file with version 0.13.0\n",
    "if [ -e ~/taxi/data ]; then\n",
    "    rm -rf ~/taxi/data\n",
    "fi\n",
    "\n",
    "# download taxi data\n",
    "mkdir -p ~/taxi/data/simple\n",
    "mkdir -p ~/taxi/serving_model/taxi_simple\n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv -O ~/taxi/data/simple/data.csv\n",
    "\n",
    "# download \n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py -O ~/taxi/taxi_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.protobuf import json_format\n",
    "\n",
    "from tfx.components.base.base_component import ComponentOutputs\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.orchestration.tfx_runner import TfxRunner\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import csv_input\n",
    "from tfx.utils.channel import Channel\n",
    "from tfx.utils import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This example assumes that the taxi data is stored in ~/taxi/data and the\n",
    "# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n",
    "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
    "_data_root = os.path.join(_taxi_root, 'data/simple')\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')\n",
    "\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(_taxi_root, 'serving_model/taxi_simple')\n",
    "\n",
    "# Directory and data locations.  This example assumes all of the chicago taxi\n",
    "# example code and metadata library is relative to $HOME, but you can store\n",
    "# these files anywhere on your local filesystem.\n",
    "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines')\n",
    "_metadata_db_root = os.path.join(_tfx_root, 'metadata')\n",
    "_log_root = os.path.join(_tfx_root, 'logs')\n",
    "\n",
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "_airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2019, 1, 1),\n",
    "}\n",
    "\n",
    "# Logging overrides\n",
    "logger_overrides = {'log_root': _log_root, 'log_level': logging.INFO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Implements the chicago taxi pipeline with TFX.\"\"\"\n",
    "examples = csv_input(_data_root)\n",
    "\n",
    "# Brings data into the pipeline or otherwise joins/converts training data.\n",
    "train_config = example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2)\n",
    "eval_config = example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        train_config,\n",
    "        eval_config\n",
    "    ]))\n",
    "\n",
    "# Create outputs\n",
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root, 'csv_example_gen/train/')\n",
    "\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root, 'csv_example_gen/eval/')\n",
    "\n",
    "example_outputs = ComponentOutputs({\n",
    "    'examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    ),\n",
    "    'training_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples]\n",
    "    ),\n",
    "    'eval_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[eval_examples]\n",
    "    ),    \n",
    "})\n",
    "\n",
    "example_gen = CsvExampleGen(\n",
    "    name=\"CSV ExampleGen Component\",\n",
    "    input_base=examples, # A Channel of 'ExternalPath' type, it contains path of data source.\n",
    "    output_config=output_config,  # An example_gen_pb2.Output instance, it contains train-eval split ratio.\n",
    "    outputs=example_outputs # dict from name to output channel, it will be stored example_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='train')\n",
    "train_statistics.uri = os.path.join(_data_root, 'statistics_gen/train/')\n",
    "\n",
    "eval_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='eval')\n",
    "eval_statistics.uri = os.path.join(_data_root, 'statistics_gen/eval/')\n",
    "\n",
    "statistics_outputs = ComponentOutputs({\n",
    "    'output': Channel(\n",
    "        type_name='ExampleStatisticsPath',\n",
    "        static_artifact_collection=[train_statistics, eval_statistics]\n",
    "    )\n",
    "})\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    name='StatisticsGen Component', # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    input_data=example_gen.outputs.examples, # A Channel of 'ExamplesPath' type, it is equal to example_outputs\n",
    "    outputs=statistics_outputs # dict from name to output channel, it will be stored statistics_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_schema_path = types.TfxType(type_name='SchemaPath', split='train')\n",
    "train_schema_path.uri = os.path.join(_data_root, 'schema_gen/')\n",
    "\n",
    "# NOTE: SchemaGen.executor can handle JUST ONE SchemaPath.\n",
    "# Two or more SchemaPaths will cause ValueError\n",
    "# such as \"ValueError: expected list length of one but got 2\".\n",
    "schema_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='SchemaPath',\n",
    "        static_artifact_collection=[train_schema_path] \n",
    "    )\n",
    "})\n",
    "\n",
    "infer_schema = SchemaGen(\n",
    "    name='SchemaGen Component',  # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    stats=statistics_gen.outputs.output, # A Channel of 'ExampleStatisticsPath' type, it is equal to statistics_outputs\n",
    "    outputs=schema_outputs # dict from name to output channel, it will be stored schema_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root,\n",
    "                                  'transform/transformed_examples/train/')\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root,\n",
    "                                 'transform/transformed_examples/eval/')\n",
    "transform_output = types.TfxType(type_name='TransformPath')\n",
    "transform_output.uri = os.path.join(_data_root,\n",
    "                                    'transform/transform_output/')\n",
    "\n",
    "transform_outputs = ComponentOutputs({\n",
    "    # Output of 'tf.Transform', which includes an exported \n",
    "    # Tensorflow graph suitable for both training and serving\n",
    "    'transform_output':Channel(\n",
    "        type_name='TransformPath',\n",
    "        static_artifact_collection=[transform_output]\n",
    "    ),\n",
    "    # transformed_examples: Materialized transformed examples, which includes \n",
    "    # both 'train' and 'eval' splits.\n",
    "    'transformed_examples':Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    )\n",
    "})\n",
    "\n",
    "transform = Transform(\n",
    "    name=\"Transform Component\",\n",
    "    input_data=example_gen.outputs.examples,\n",
    "    schema=infer_schema.outputs.output,\n",
    "    module_file=_taxi_module_file,\n",
    "    outputs=transform_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_exports = types.TfxType(type_name='ModelExportPath')\n",
    "model_exports.uri = os.path.join(_data_root, 'trainer/current/')\n",
    "\n",
    "trainer_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='ModelExportPath',\n",
    "        static_artifact_collection=[model_exports]\n",
    "    )\n",
    "})\n",
    "\n",
    "trainer = Trainer(\n",
    "    name='Trainer Component',\n",
    "    module_file=_taxi_module_file,\n",
    "    transformed_examples=transform.outputs.transformed_examples,\n",
    "    schema=infer_schema.outputs.output,\n",
    "    transform_output=transform.outputs.transform_output,\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=5000),\n",
    "    outputs=trainer_outputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_output = types.TfxType('ModelEvalPath')\n",
    "eval_output.uri = os.path.join(_data_root, 'eval_output/')\n",
    "\n",
    "model_analyzer_outputs = ComponentOutputs({\n",
    "    'output':\n",
    "    Channel(\n",
    "        type_name='ModelEvalPath',\n",
    "        static_artifact_collection=[eval_output]),\n",
    "})\n",
    "\n",
    "feature_slicing_spec = evaluator_pb2.FeatureSlicingSpec(specs=[\n",
    "    evaluator_pb2.SingleSlicingSpec(\n",
    "        column_for_slicing=['trip_start_hour'])\n",
    "])\n",
    "\n",
    "model_analyzer = Evaluator(\n",
    "    name='Evaluator Component',\n",
    "    examples=example_gen.outputs.examples,\n",
    "    model_exports=trainer.outputs.output,\n",
    "    feature_slicing_spec=feature_slicing_spec,\n",
    "    outputs=model_analyzer_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _Do(self, input_dict, output_dict, exec_properties):\n",
    "    import apache_beam as beam\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_model_analysis as tfma\n",
    "    from typing import Any, Dict, List, Text\n",
    "    from tfx.components.base import base_executor\n",
    "    from tfx.proto import evaluator_pb2\n",
    "    from tfx.utils import io_utils\n",
    "    from tfx.utils import path_utils\n",
    "    from tfx.utils import types\n",
    "    from google.protobuf import json_format\n",
    "\n",
    "    \"\"\"Runs a batch job to evaluate the eval_model against the given input.\n",
    "    Args:\n",
    "      input_dict: Input dict from input key to a list of Artifacts.\n",
    "        - model_exports: exported model.\n",
    "        - examples: examples for eval the model.\n",
    "      output_dict: Output dict from output key to a list of Artifacts.\n",
    "        - output: model evaluation results.\n",
    "      exec_properties: A dict of execution properties.\n",
    "        - feature_slicing_spec: JSON string of evaluator_pb2.FeatureSlicingSpec\n",
    "          instance, providing the way to slice the data.\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    if 'model_exports' not in input_dict:\n",
    "      raise ValueError('\\'model_exports\\' is missing in input dict.')\n",
    "    if 'examples' not in input_dict:\n",
    "      raise ValueError('\\'examples\\' is missing in input dict.')\n",
    "    if 'output' not in output_dict:\n",
    "      raise ValueError('\\'output\\' is missing in output dict.')\n",
    "\n",
    "    self._log_startup(input_dict, output_dict, exec_properties)\n",
    "\n",
    "    # Extract input artifacts\n",
    "    model_exports_uri = types.get_single_uri(input_dict['model_exports'])\n",
    "\n",
    "    feature_slicing_spec = evaluator_pb2.FeatureSlicingSpec()\n",
    "    json_format.Parse(exec_properties['feature_slicing_spec'],\n",
    "                      feature_slicing_spec)\n",
    "    slice_spec = self._get_slice_spec_from_feature_slicing_spec(\n",
    "        feature_slicing_spec)\n",
    "\n",
    "    output_uri = types.get_single_uri(output_dict['output'])\n",
    "\n",
    "    eval_model_path = path_utils.eval_model_path(model_exports_uri)\n",
    "\n",
    "    tf.logging.info('Using {} for model eval.'.format(eval_model_path))\n",
    "    eval_shared_model = tfma.default_eval_shared_model(\n",
    "        add_metrics_callbacks=[\n",
    "                        # calibration_plot_and_prediction_histogram computes calibration plot and prediction\n",
    "                        # distribution at different thresholds.\n",
    "                        tfma.post_export_metrics.calibration_plot_and_prediction_histogram(),\n",
    "                        # auc_plots enables precision-recall curve and ROC visualization at different thresholds.\n",
    "                        tfma.post_export_metrics.auc_plots()\n",
    "                    ],\n",
    "        eval_saved_model_path=eval_model_path)\n",
    "\n",
    "    tf.logging.info('Evaluating model.')\n",
    "    with beam.Pipeline(argv=self._get_beam_pipeline_args()) as pipeline:\n",
    "      # pylint: disable=expression-not-assigned\n",
    "      (pipeline\n",
    "       | 'ReadData' >> beam.io.ReadFromTFRecord(\n",
    "           file_pattern=io_utils.all_files_pattern(\n",
    "               types.get_split_uri(input_dict['examples'], 'eval')))\n",
    "       |\n",
    "       'ExtractEvaluateAndWriteResults' >> tfma.ExtractEvaluateAndWriteResults(\n",
    "           eval_shared_model=eval_shared_model,\n",
    "           slice_spec=slice_spec,\n",
    "           output_path=output_uri))\n",
    "    tf.logging.info(\n",
    "        'Evaluation complete. Results written to {}.'.format(output_uri))\n",
    "\n",
    "model_analyzer.executor.Do = _Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blessing = types.TfxType(type_name='ModelBlessingPath')\n",
    "blessing.uri = os.path.join(_data_root, 'model_validator/blessed/')\n",
    "\n",
    "results = types.TfxType(type_name='ModelValidationPath')\n",
    "results.uri = os.path.join(_data_root, 'model_validator/results/')\n",
    "\n",
    "model_validator_outputs = ComponentOutputs({\n",
    "    'blessing':\n",
    "    Channel(\n",
    "        type_name='ModelBlessingPath',\n",
    "        static_artifact_collection=[blessing]),\n",
    "    'results':\n",
    "    Channel(\n",
    "        type_name='ModelValidationPath',\n",
    "        static_artifact_collection=[results]),\n",
    "})\n",
    "\n",
    "model_validator = ModelValidator(\n",
    "    name='Model Validator Component',\n",
    "    examples=example_gen.outputs.examples, \n",
    "    model=trainer.outputs.output,\n",
    "    outputs=model_validator_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pusher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config={\n",
    "    # If custom_config contains 'cmle_serving_args', Pusher will try to push ml model to AI Platform (GCE).\n",
    "    # However, this config will be deplecated in next release of TFX.\n",
    "    # To run it localy, we use empty dictionary .\n",
    "}\n",
    "\n",
    "pusher = Pusher(\n",
    "    name='Pusher Component',\n",
    "    model_export=trainer.outputs.output,\n",
    "    model_blessing=model_validator.outputs.blessing,\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=_serving_model_dir)),\n",
    "    outputs='',\n",
    "    custom_config=custom_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"TFX Pipeline\",\n",
    "    pipeline_root=_pipeline_root,\n",
    "    components=[\n",
    "        example_gen, \n",
    "        statistics_gen, \n",
    "        infer_schema, \n",
    "        transform, \n",
    "        trainer, \n",
    "        model_analyzer, \n",
    "        model_validator,\n",
    "        pusher,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectRunner(TfxRunner):\n",
    "    \"\"\"Tfx runner on local\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self._config = config or {}\n",
    "    \n",
    "    def run(self, pipeline):\n",
    "        for component in pipeline.components:\n",
    "            self._execute_component(component)\n",
    "            \n",
    "        return pipeline\n",
    "            \n",
    "    def _execute_component(self, component):\n",
    "        input_dict = {key:value.get() for key, value in component.input_dict.items()}\n",
    "        output_dict = {key: value.get() for key, value in component.outputs.get_all().items()}\n",
    "        exec_properties = component.exec_properties\n",
    "        executor = component.executor()\n",
    "        executor.Do(input_dict, output_dict, exec_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 11:22:19,975] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExternalPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExternalPath\"}}]}\n",
      "[2019-06-25 11:22:19,983] {base_executor.py:74} INFO - Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExternalPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExternalPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"training_examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"eval_examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "[2019-06-25 11:22:19,995] {base_executor.py:76} INFO - Outputs for Executor is: {\"examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"training_examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"eval_examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"name\\\": \\\"train\\\",\\n        \\\"hashBuckets\\\": 2\\n      },\\n      {\\n        \\\"name\\\": \\\"eval\\\",\\n        \\\"hashBuckets\\\": 1\\n      }\\n    ]\\n  }\\n}\"}\n",
      "[2019-06-25 11:22:20,003] {base_executor.py:78} INFO - Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"name\\\": \\\"train\\\",\\n        \\\"hashBuckets\\\": 2\\n      },\\n      {\\n        \\\"name\\\": \\\"eval\\\",\\n        \\\"hashBuckets\\\": 1\\n      }\\n    ]\\n  }\\n}\"}\n",
      "INFO:tensorflow:Generating examples.\n",
      "[2019-06-25 11:22:20,007] {base_example_gen_executor.py:122} INFO - Generating examples.\n",
      "[2019-06-25 11:22:20,017] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-25 11:22:20,035] {executor.py:70} INFO - Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-25 11:22:20,815] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7fe7a1f2c510> ====================\n",
      "[2019-06-25 11:22:20,819] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7fe7a1f2c620> ====================\n",
      "[2019-06-25 11:22:20,836] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7fe7a1f2c6a8> ====================\n",
      "[2019-06-25 11:22:20,839] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7fe7a1f2c730> ====================\n",
      "[2019-06-25 11:22:20,842] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7fe7a1f2c7b8> ====================\n",
      "[2019-06-25 11:22:20,867] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7fe7a1f2c8c8> ====================\n",
      "[2019-06-25 11:22:20,873] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7fe7a1f2c950> ====================\n",
      "[2019-06-25 11:22:20,881] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7fe7a1f2c9d8> ====================\n",
      "[2019-06-25 11:22:20,885] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7fe7a1f2ca60> ====================\n",
      "[2019-06-25 11:22:20,897] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7fe7a1f2cbf8> ====================\n",
      "[2019-06-25 11:22:20,914] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7fe7a1f2cc80> ====================\n",
      "[2019-06-25 11:22:20,917] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7fe7a1f2cd08> ====================\n",
      "[2019-06-25 11:22:20,945] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_InputSourceToExample/ReadFromText/Read_4)+(((ref_AppliedPTransform_InputSourceToExample/ParseCSV/ParseCSVRecords_6)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/KeyWithVoid_8))+(ref_PCollection_PCollection_2/Write)))+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Precombine))+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Write)\n",
      "[2019-06-25 11:22:22,331] {fn_api_runner.py:437} INFO - Running (InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Read)+(((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Merge)+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/UnKey_16)+(ref_PCollection_PCollection_8/Write)))\n",
      "[2019-06-25 11:22:22,353] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/DoOnce/Read_18)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/InjectDefault_19))+(ref_PCollection_PCollection_10/Write)\n",
      "[2019-06-25 11:22:22,370] {fn_api_runner.py:437} INFO - Running (((((ref_PCollection_PCollection_2/Read)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/CreateInMemoryDict_20))+((ref_AppliedPTransform_InputSourceToExample/ToTFExample_21)+(ref_AppliedPTransform_SerializeDeterministically_22)))+((ref_AppliedPTransform_SplitData/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_25)+(ref_AppliedPTransform_ShuffleSpliteval/AddRandomKeys_53)))+((ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/Map(reify_timestamps)_55)+(ShuffleSpliteval/ReshufflePerKey/GroupByKey/Write)))+((ref_AppliedPTransform_ShuffleSplittrain/AddRandomKeys_27)+((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/Map(reify_timestamps)_29)+(ShuffleSplittrain/ReshufflePerKey/GroupByKey/Write)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:27,625] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/DoOnce/Read_66)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/InitializeWrite_67))+(ref_PCollection_PCollection_42/Write))+(ref_PCollection_PCollection_43/Write)\n",
      "[2019-06-25 11:22:27,643] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/DoOnce/Read_40)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/InitializeWrite_41))+(ref_PCollection_PCollection_24/Write))+(ref_PCollection_PCollection_25/Write)\n",
      "[2019-06-25 11:22:27,660] {fn_api_runner.py:437} INFO - Running ((ShuffleSpliteval/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/FlatMap(restore_timestamps)_60))+(((ref_AppliedPTransform_ShuffleSpliteval/RemoveRandomKeys_61)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WriteBundles_68))+(((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Pair_69)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WindowInto(WindowIntoFn)_70))+(OutputSpliteval/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-25 11:22:27,933] {fn_api_runner.py:437} INFO - Running ((OutputSpliteval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Extract_75))+(ref_PCollection_PCollection_50/Write)\n",
      "[2019-06-25 11:22:27,948] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/PreFinalize_76)+(ref_PCollection_PCollection_51/Write))\n",
      "[2019-06-25 11:22:27,964] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/FinalizeWrite_77)\n",
      "[2019-06-25 11:22:27,978] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:22:28,082] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:22:28,106] {fn_api_runner.py:437} INFO - Running ((((ShuffleSplittrain/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/FlatMap(restore_timestamps)_34))+(ref_AppliedPTransform_ShuffleSplittrain/RemoveRandomKeys_35))+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WriteBundles_42))+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Pair_43)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WindowInto(WindowIntoFn)_44)+(OutputSplittrain/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-25 11:22:28,577] {fn_api_runner.py:437} INFO - Running ((OutputSplittrain/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Extract_49))+(ref_PCollection_PCollection_32/Write)\n",
      "[2019-06-25 11:22:28,593] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/PreFinalize_50)+(ref_PCollection_PCollection_33/Write))\n",
      "[2019-06-25 11:22:28,610] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/FinalizeWrite_51)\n",
      "[2019-06-25 11:22:28,629] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:22:28,737] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "INFO:tensorflow:Examples generated.\n",
      "[2019-06-25 11:22:28,759] {base_example_gen_executor.py:145} INFO - Examples generated.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 11:22:28,764] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "[2019-06-25 11:22:28,775] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/statistics_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}}]}\n",
      "[2019-06-25 11:22:28,781] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/statistics_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {}\n",
      "[2019-06-25 11:22:28,786] {base_executor.py:78} INFO - Execution properties for Executor is: {}\n",
      "[2019-06-25 11:22:28,796] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Generating statistics for split eval\n",
      "[2019-06-25 11:22:28,809] {executor.py:62} INFO - Generating statistics for split eval\n",
      "INFO:tensorflow:Generating statistics for split train\n",
      "[2019-06-25 11:22:29,787] {executor.py:62} INFO - Generating statistics for split train\n",
      "INFO:tensorflow:Statistics written to /root/taxi/data/simple/statistics_gen/train/.\n",
      "[2019-06-25 11:22:30,462] {executor.py:78} INFO - Statistics written to /root/taxi/data/simple/statistics_gen/train/.\n",
      "[2019-06-25 11:22:33,172] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7fe7a1f2c510> ====================\n",
      "[2019-06-25 11:22:33,176] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7fe7a1f2c620> ====================\n",
      "[2019-06-25 11:22:33,179] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7fe7a1f2c6a8> ====================\n",
      "[2019-06-25 11:22:33,187] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7fe7a1f2c730> ====================\n",
      "[2019-06-25 11:22:33,190] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7fe7a1f2c7b8> ====================\n",
      "[2019-06-25 11:22:33,201] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7fe7a1f2c8c8> ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:33,207] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7fe7a1f2c950> ====================\n",
      "[2019-06-25 11:22:33,223] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7fe7a1f2c9d8> ====================\n",
      "[2019-06-25 11:22:33,227] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7fe7a1f2ca60> ====================\n",
      "[2019-06-25 11:22:33,230] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7fe7a1f2cbf8> ====================\n",
      "[2019-06-25 11:22:33,235] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7fe7a1f2cc80> ====================\n",
      "[2019-06-25 11:22:33,237] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7fe7a1f2cd08> ====================\n",
      "[2019-06-25 11:22:33,265] {fn_api_runner.py:437} INFO - Running (((((((ref_AppliedPTransform_ReadData.train/Read_106)+((((ref_AppliedPTransform_DecodeData.train/ParseTFExamples_108)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/KeyWithVoid_111))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_115))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_135)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_116))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_136)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_138)))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write))\n",
      "[2019-06-25 11:22:38,769] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_146)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+((((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_156)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_158))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)))\n",
      "[2019-06-25 11:22:38,805] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_166))))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1)\n",
      "[2019-06-25 11:22:38,826] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_155)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0))\n",
      "[2019-06-25 11:22:38,852] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_168)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)))\n",
      "[2019-06-25 11:22:38,877] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_124)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_125))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1))\n",
      "[2019-06-25 11:22:39,122] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:39,274] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)))\n",
      "[2019-06-25 11:22:39,560] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write))\n",
      "[2019-06-25 11:22:39,578] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadData.eval/Read_3)+((((ref_AppliedPTransform_DecodeData.eval/ParseTFExamples_5)+((((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/KeyWithVoid_8)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_12))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_32))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_33)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_35))))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_13)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write))))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)\n",
      "[2019-06-25 11:22:42,470] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge))+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_21))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_22)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1)))\n",
      "[2019-06-25 11:22:42,958] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "[2019-06-25 11:22:43,113] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_177)))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_180)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)))\n",
      "[2019-06-25 11:22:43,143] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs)))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0))\n",
      "[2019-06-25 11:22:43,464] {fn_api_runner.py:437} INFO - Running (((((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_43))))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_53))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_55))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "[2019-06-25 11:22:43,501] {fn_api_runner.py:437} INFO - Running ((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_52))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:43,520] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_63)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1))))\n",
      "[2019-06-25 11:22:43,539] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_65)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)))\n",
      "[2019-06-25 11:22:43,562] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "[2019-06-25 11:22:43,581] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_74))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_77))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)))\n",
      "[2019-06-25 11:22:43,615] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_85))+(ref_PCollection_PCollection_51/Write))\n",
      "[2019-06-25 11:22:43,638] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_188))+(ref_PCollection_PCollection_115/Write))\n",
      "[2019-06-25 11:22:43,671] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/DoOnce/Read_94)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/InitializeWrite_95))+(ref_PCollection_PCollection_55/Write))+(ref_PCollection_PCollection_56/Write)\n",
      "[2019-06-25 11:22:43,689] {fn_api_runner.py:437} INFO - Running (((((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_87)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_88))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_89))+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/Map(<lambda at iobase.py:984>)_96))+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WindowInto(WindowIntoFn)_97))+(WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Write)\n",
      "[2019-06-25 11:22:43,730] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WriteBundles_102))+(ref_PCollection_PCollection_62/Write)\n",
      "[2019-06-25 11:22:43,760] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/PreFinalize_103))+(ref_PCollection_PCollection_63/Write)\n",
      "[2019-06-25 11:22:43,779] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/FinalizeWrite_104)\n",
      "[2019-06-25 11:22:43,797] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:22:43,902] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:22:43,925] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/DoOnce/Read_197)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/InitializeWrite_198)+(ref_PCollection_PCollection_120/Write)))+(ref_PCollection_PCollection_119/Write)\n",
      "[2019-06-25 11:22:43,953] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_190)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_191)+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_192)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/Map(<lambda at iobase.py:984>)_199)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WindowInto(WindowIntoFn)_200)))+(WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-25 11:22:44,003] {fn_api_runner.py:437} INFO - Running (WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WriteBundles_205)+(ref_PCollection_PCollection_126/Write))\n",
      "[2019-06-25 11:22:44,021] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/PreFinalize_206)+(ref_PCollection_PCollection_127/Write))\n",
      "[2019-06-25 11:22:44,046] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/FinalizeWrite_207)\n",
      "[2019-06-25 11:22:44,057] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:22:44,166] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "INFO:tensorflow:Infering schema from statistics.\n",
      "[2019-06-25 11:22:44,190] {executor.py:62} INFO - Infering schema from statistics.\n",
      "INFO:tensorflow:Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
      "[2019-06-25 11:22:44,205] {executor.py:66} INFO - Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 11:22:44,212] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"schema\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"SchemaPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/schema_gen/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"SchemaPath\"}}], \"input_data\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:44,219] {base_executor.py:74} INFO - Inputs for Executor is: {\"schema\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"SchemaPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/schema_gen/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"SchemaPath\"}}], \"input_data\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"transform_output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"TransformPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/transform/transform_output/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"TransformPath\"}}], \"transformed_examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "[2019-06-25 11:22:44,244] {base_executor.py:76} INFO - Outputs for Executor is: {\"transform_output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"TransformPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/transform/transform_output/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"TransformPath\"}}], \"transformed_examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\"}\n",
      "[2019-06-25 11:22:44,249] {base_executor.py:78} INFO - Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\"}\n",
      "INFO:tensorflow:Inputs to executor.Transform function: {'examples_data_format': 'FORMAT_TF_EXAMPLE', 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'preprocessing_fn': '/root/taxi/taxi_utils.py', 'compute_statistics': False, 'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt', 'tft_statistics_use_tfdv': True, 'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*'}\n",
      "[2019-06-25 11:22:44,303] {executor.py:567} INFO - Inputs to executor.Transform function: {'examples_data_format': 'FORMAT_TF_EXAMPLE', 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'preprocessing_fn': '/root/taxi/taxi_utils.py', 'compute_statistics': False, 'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt', 'tft_statistics_use_tfdv': True, 'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*'}\n",
      "INFO:tensorflow:Outputs to executor.Transform function: {'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path', 'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples'], 'transform_output_path': '/root/taxi/data/simple/transform/transform_output/'}\n",
      "[2019-06-25 11:22:44,306] {executor.py:569} INFO - Outputs to executor.Transform function: {'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path', 'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples'], 'transform_output_path': '/root/taxi/data/simple/transform/transform_output/'}\n",
      "INFO:tensorflow:Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
      "[2019-06-25 11:22:44,589] {executor.py:653} INFO - Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
      "INFO:tensorflow:Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
      "[2019-06-25 11:22:44,591] {executor.py:655} INFO - Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
      "INFO:tensorflow:Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n",
      "[2019-06-25 11:22:44,592] {executor.py:657} INFO - Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n",
      "INFO:tensorflow:Transform output path: /root/taxi/data/simple/transform/transform_output/\n",
      "[2019-06-25 11:22:44,598] {executor.py:658} INFO - Transform output path: /root/taxi/data/simple/transform/transform_output/\n",
      "[2019-06-25 11:22:45,036] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 11:22:45,290] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "[2019-06-25 11:22:45,293] {builder_impl.py:449} INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/dafaf7894a254cc18f85fc05267516b6/saved_model.pb\n",
      "[2019-06-25 11:22:45,339] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/dafaf7894a254cc18f85fc05267516b6/saved_model.pb\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 11:22:46,921] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "[2019-06-25 11:22:46,922] {builder_impl.py:449} INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/0cf6666b162f4989881d4a0915b5db56/saved_model.pb\n",
      "[2019-06-25 11:22:46,960] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/0cf6666b162f4989881d4a0915b5db56/saved_model.pb\n",
      "[2019-06-25 11:22:51,397] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7fe7a1f2c510> ====================\n",
      "[2019-06-25 11:22:51,422] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7fe7a1f2c620> ====================\n",
      "[2019-06-25 11:22:51,431] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7fe7a1f2c6a8> ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:51,447] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7fe7a1f2c730> ====================\n",
      "[2019-06-25 11:22:51,456] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7fe7a1f2c7b8> ====================\n",
      "[2019-06-25 11:22:51,471] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7fe7a1f2c8c8> ====================\n",
      "[2019-06-25 11:22:51,477] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7fe7a1f2c950> ====================\n",
      "[2019-06-25 11:22:51,508] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7fe7a1f2c9d8> ====================\n",
      "[2019-06-25 11:22:51,514] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7fe7a1f2ca60> ====================\n",
      "[2019-06-25 11:22:51,520] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7fe7a1f2cbf8> ====================\n",
      "[2019-06-25 11:22:51,525] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7fe7a1f2cc80> ====================\n",
      "[2019-06-25 11:22:51,532] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7fe7a1f2cd08> ====================\n",
      "[2019-06-25 11:22:51,586] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_ReadAnalysisDataset[0]/Read/Read_4)+(ref_AppliedPTransform_ReadAnalysisDataset[0]/AddKey_5))+((ref_AppliedPTransform_ReadAnalysisDataset[0]/ParseExamples_6)+((ref_AppliedPTransform_DecodeAnalysisDataset[0]/ApplyDecodeFn_8)+(FlattenAnalysisDatasets/Write/0)))\n",
      "[2019-06-25 11:22:53,161] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CreateSavedModelForAnalyzerInputs[0]/CreateSavedModel/Read_13)+(ref_PCollection_PCollection_6/Write)\n",
      "[2019-06-25 11:22:53,189] {fn_api_runner.py:437} INFO - Running (((((((((((((((((FlattenAnalysisDatasets/Read)+(ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/BatchInputs/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_17))+((((((((((((((ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/ApplySavedModel_18)+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score/mean_and_var]_19))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_1/mean_and_var]_46))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_2/mean_and_var]_73))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary/vocabulary]_100))+((ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_1/vocabulary]_158)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FlattenStringsAndMaybeWeightsLabels_160)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CountPerString:PairWithVoid_162))))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize/quantiles]_216))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_1/quantiles]_248))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_2/quantiles]_280))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_3/quantiles]_312))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/KeyWithVoid_49))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/KeyWithVoid_76))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/KeyWithVoid_22))+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine)))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/KeyWithVoid_315))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/KeyWithVoid_283))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/KeyWithVoid_251)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write)))+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write)))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write)))+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/KeyWithVoid_219)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write))+((((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FlattenStringsAndMaybeWeightsLabels_102)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CountPerString:PairWithVoid_104))+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine))+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write))\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:22:53,574] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:22:54,681] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/UnKey_259)))+(ref_PCollection_PCollection_161/Write))\n",
      "[2019-06-25 11:22:54,817] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/DoOnce/Read_261)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/InjectDefault_262))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/KeyWithVoid_265)+((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:54,906] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/UnKey_273)+(ref_PCollection_PCollection_169/Write)))\n",
      "[2019-06-25 11:22:55,043] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/DoOnce/Read_275)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/InjectDefault_276))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_278))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_1/quantiles/Placeholder]_279))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/11)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/11))\n",
      "[2019-06-25 11:22:55,072] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs)))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FilterProblematicStrings_112))+((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Precombine)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Write))\n",
      "[2019-06-25 11:22:55,095] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/UnKey_227))+(ref_PCollection_PCollection_141/Write)\n",
      "[2019-06-25 11:22:55,230] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/DoOnce/Read_229)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/InjectDefault_230)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/KeyWithVoid_233)+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine))))+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)\n",
      "[2019-06-25 11:22:55,314] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+((((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/UnKey_84))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_87)))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write)\n",
      "[2019-06-25 11:22:55,343] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/UnKey_95)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_97))+(((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder]_98)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/6))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/6)))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder_1]_99))))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/7)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/7))\n",
      "[2019-06-25 11:22:55,385] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/UnKey_323)+(ref_PCollection_PCollection_201/Write)))\n",
      "[2019-06-25 11:22:55,520] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_127)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1)\n",
      "[2019-06-25 11:22:55,538] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Read)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Merge))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/ExtractOutputs))+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/SwapStringsAndCounts_121)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_125))+((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0)))\n",
      "[2019-06-25 11:22:55,572] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
      "[2019-06-25 11:22:55,593] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/UnKey_30)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_33)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:55,623] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/UnKey_41)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_43)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder]_44))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder_1]_45)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/3))))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/2)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/2)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/3))\n",
      "[2019-06-25 11:22:55,664] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/UnKey_291)+(ref_PCollection_PCollection_181/Write)))\n",
      "[2019-06-25 11:22:55,807] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/DoOnce/Read_293)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/InjectDefault_294))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/KeyWithVoid_297)+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)\n",
      "[2019-06-25 11:22:55,894] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/UnKey_305)+(ref_PCollection_PCollection_189/Write)))\n",
      "[2019-06-25 11:22:56,033] {fn_api_runner.py:437} INFO - Running (((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/DoOnce/Read_307)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/InjectDefault_308))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_310))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_2/quantiles/Placeholder]_311))+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/0))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/0)\n",
      "[2019-06-25 11:22:56,071] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+(((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/UnKey_241))+(ref_PCollection_PCollection_149/Write)))\n",
      "[2019-06-25 11:22:56,233] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/DoOnce/Read_243)+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/InjectDefault_244)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_246))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize/quantiles/Placeholder]_247)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/10)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/10))\n",
      "[2019-06-25 11:22:56,270] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/DoOnce/Read_325)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/InjectDefault_326)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/KeyWithVoid_329))+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)\n",
      "[2019-06-25 11:22:56,354] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/UnKey_337)+(ref_PCollection_PCollection_209/Write))\n",
      "[2019-06-25 11:22:56,518] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/DoOnce/Read_339)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/InjectDefault_340))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_342)+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_3/quantiles/Placeholder]_343)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/1)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/1))\n",
      "[2019-06-25 11:22:56,550] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_143)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_144)+(ref_PCollection_PCollection_87/Write)))+(ref_PCollection_PCollection_86/Write)\n",
      "[2019-06-25 11:22:56,580] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_133)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_134)+(ref_PCollection_PCollection_83/Write)))\n",
      "[2019-06-25 11:22:56,605] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/Prepare/Read_137)+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/OrderElements_138)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_145))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_146)+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:56,642] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_151)+(ref_PCollection_PCollection_93/Write))\n",
      "[2019-06-25 11:22:56,667] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_86/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_152))+(ref_PCollection_PCollection_94/Write)\n",
      "[2019-06-25 11:22:56,697] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_86/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_153)+(ref_PCollection_PCollection_95/Write))\n",
      "[2019-06-25 11:22:56,715] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:22:56,817] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:22:56,842] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/CreatePath/Read_155)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WaitForVocabularyFile_156))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary/vocabulary/Placeholder]_157)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/8)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/8)))\n",
      "[2019-06-25 11:22:56,872] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_201)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_202))+(ref_PCollection_PCollection_122/Write))+(ref_PCollection_PCollection_123/Write)\n",
      "[2019-06-25 11:22:56,894] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge))+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FilterProblematicStrings_170))+((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Precombine)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Write))\n",
      "[2019-06-25 11:22:56,931] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Read)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Merge))+(((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/SwapStringsAndCounts_179))+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_183)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0)))\n",
      "[2019-06-25 11:22:56,960] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_185)+((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1))\n",
      "[2019-06-25 11:22:56,985] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
      "[2019-06-25 11:22:57,004] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_191)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_192)+(ref_PCollection_PCollection_119/Write)))\n",
      "[2019-06-25 11:22:57,039] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/Prepare/Read_195)+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/OrderElements_196)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_203)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_204)))+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-25 11:22:57,076] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_209)+(ref_PCollection_PCollection_129/Write))\n",
      "[2019-06-25 11:22:57,097] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_122/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_210))+(ref_PCollection_PCollection_130/Write)\n",
      "[2019-06-25 11:22:57,118] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_122/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_211)+(ref_PCollection_PCollection_131/Write))\n",
      "[2019-06-25 11:22:57,139] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:22:57,243] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:22:57,265] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/CreatePath/Read_213)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WaitForVocabularyFile_214)+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1/vocabulary/Placeholder]_215)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/9)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/9))))\n",
      "[2019-06-25 11:22:57,292] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/UnKey_57)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_60)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:22:57,329] {fn_api_runner.py:437} INFO - Running ((((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/UnKey_68)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_70))))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder]_71))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/4)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/4)))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder_1]_72)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/5)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/5)\n",
      "[2019-06-25 11:22:57,376] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CreateSavedModel/Flatten/Read)+(ref_PCollection_PCollection_216/Write)\n",
      "[2019-06-25 11:22:57,390] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/CreateSavedModel/Read_346)+((((ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/BindTensors_348)+(((ref_AppliedPTransform_AnalyzeDataset/ComputeDeferredMetadata_349)+(ref_AppliedPTransform_WriteTransformFn/WriteMetadata/WriteMetadata_360))+(ref_PCollection_PCollection_224/Write)))+((ref_AppliedPTransform_AnalyzeDataset/MakeCheapBarrier_350)+(ref_PCollection_PCollection_219/Write)))+(ref_PCollection_PCollection_217/Write))\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:22:57,723] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 11:22:57,753] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/2a70ed19411c439197abc6965fb03506/assets\n",
      "[2019-06-25 11:22:57,759] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/2a70ed19411c439197abc6965fb03506/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/2a70ed19411c439197abc6965fb03506/saved_model.pb\n",
      "[2019-06-25 11:22:57,810] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/2a70ed19411c439197abc6965fb03506/saved_model.pb\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:22:57,897] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:22:57,899] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:22:57,902] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:22:57,979] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/DoOnce/Read_405)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/InitializeWrite_406))+(ref_PCollection_PCollection_250/Write))+(ref_PCollection_PCollection_251/Write)\n",
      "[2019-06-25 11:22:58,007] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_ReadTransformDataset[0]/Read/Read_365)+((ref_AppliedPTransform_ReadTransformDataset[0]/AddKey_366)+(((ref_AppliedPTransform_ReadTransformDataset[0]/ParseExamples_367)+(ref_AppliedPTransform_DecodeTransformDataset[0]/ApplyDecodeFn_369))+((((ref_AppliedPTransform_TransformDataset[0]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_373)+(ref_AppliedPTransform_TransformDataset[0]/Transform_374))+((((ref_AppliedPTransform_TransformDataset[0]/ConvertAndUnbatch_375)+(ref_AppliedPTransform_TransformDataset[0]/MakeCheapBarrier_376))+(ref_AppliedPTransform_EncodeTransformedDataset[0]_380))+(ref_PCollection_PCollection_234/Write)))+(ref_AppliedPTransform_Materialize[0]/DropNoneKeys_400)))))+(((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WriteBundles_407)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Pair_408))+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_409)+(Materialize[0]/Write/Write/WriteImpl/GroupByKey/Write)))\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:22:58,156] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:22:58,158] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:22:58,162] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:23:02,045] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[0]/PrepareToClearSharedKeepAlives/Read_378)+(ref_AppliedPTransform_TransformDataset[0]/WaitAndClearSharedKeepAlives_379)\n",
      "[2019-06-25 11:23:02,068] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/DoOnce/Read_423)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/InitializeWrite_424))+(ref_PCollection_PCollection_262/Write))+(ref_PCollection_PCollection_263/Write)\n",
      "[2019-06-25 11:23:02,090] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadTransformDataset[1]/Read/Read_383)+((ref_AppliedPTransform_ReadTransformDataset[1]/AddKey_384)+((ref_AppliedPTransform_ReadTransformDataset[1]/ParseExamples_385)+(ref_AppliedPTransform_DecodeTransformDataset[1]/ApplyDecodeFn_387))))+(ref_AppliedPTransform_TransformDataset[1]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_391))+(((((ref_AppliedPTransform_TransformDataset[1]/Transform_392)+(ref_AppliedPTransform_TransformDataset[1]/ConvertAndUnbatch_393))+((ref_AppliedPTransform_TransformDataset[1]/MakeCheapBarrier_394)+(ref_PCollection_PCollection_245/Write)))+((ref_AppliedPTransform_EncodeTransformedDataset[1]_398)+(ref_AppliedPTransform_Materialize[1]/DropNoneKeys_418)))+(((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WriteBundles_425)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Pair_426))+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_427)+(Materialize[1]/Write/Write/WriteImpl/GroupByKey/Write))))\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:23:02,232] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:23:02,235] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:23:02,249] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:23:04,312] {fn_api_runner.py:437} INFO - Running (Materialize[1]/Write/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Extract_432)+(ref_PCollection_PCollection_270/Write))\n",
      "[2019-06-25 11:23:04,329] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_262/Read)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/PreFinalize_433)+(ref_PCollection_PCollection_271/Write))\n",
      "[2019-06-25 11:23:04,354] {fn_api_runner.py:437} INFO - Running (Materialize[0]/Write/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Extract_414)+(ref_PCollection_PCollection_258/Write))\n",
      "[2019-06-25 11:23:04,376] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/PreFinalize_415))+(ref_PCollection_PCollection_259/Write)\n",
      "[2019-06-25 11:23:04,405] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_262/Read)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/FinalizeWrite_434)\n",
      "[2019-06-25 11:23:04,421] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:23:04,525] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:23:04,547] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/PrepareToClearSharedKeepAlives/Read_352)+(ref_AppliedPTransform_AnalyzeDataset/WaitAndClearSharedKeepAlives_353)\n",
      "[2019-06-25 11:23:04,579] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_217/Read)+((ref_AppliedPTransform_WriteTransformFn/WriteTransformFn_361)+(ref_AppliedPTransform_WriteTransformFn/WaitOnWriteMetadataDone_362))\n",
      "[2019-06-25 11:23:04,608] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[1]/PrepareToClearSharedKeepAlives/Read_396)+(ref_AppliedPTransform_TransformDataset[1]/WaitAndClearSharedKeepAlives_397)\n",
      "[2019-06-25 11:23:04,639] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/FinalizeWrite_416)\n",
      "[2019-06-25 11:23:04,663] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:23:04,771] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "[2019-06-25 11:23:04,795] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_WriteMetadata/Create/Read_356)+(ref_AppliedPTransform_WriteMetadata/WriteMetadata_357)\n",
      "INFO:tensorflow:Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n",
      "[2019-06-25 11:23:04,838] {executor.py:248} INFO - Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 11:23:04,853] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"transformed_examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"transform_output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"TransformPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/transform/transform_output/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"TransformPath\"}}], \"schema\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"SchemaPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/schema_gen/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"SchemaPath\"}}]}\n",
      "[2019-06-25 11:23:04,863] {base_executor.py:74} INFO - Inputs for Executor is: {\"transformed_examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"transform_output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"TransformPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/transform/transform_output/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"TransformPath\"}}], \"schema\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"SchemaPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/schema_gen/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"SchemaPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelExportPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}]}\n",
      "[2019-06-25 11:23:04,871] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelExportPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"eval_args\": \"{\\n  \\\"numSteps\\\": 5000\\n}\", \"module_file\": \"/root/taxi/taxi_utils.py\", \"train_args\": \"{\\n  \\\"numSteps\\\": 10000\\n}\", \"custom_config\": null}\n",
      "[2019-06-25 11:23:04,878] {base_executor.py:78} INFO - Execution properties for Executor is: {\"eval_args\": \"{\\n  \\\"numSteps\\\": 5000\\n}\", \"module_file\": \"/root/taxi/taxi_utils.py\", \"train_args\": \"{\\n  \\\"numSteps\\\": 10000\\n}\", \"custom_config\": null}\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_keep_checkpoint_max': 1, '_protocol': None, '_is_chief': True, '_save_checkpoints_secs': None, '_service': None, '_evaluation_master': '', '_device_fn': None, '_model_dir': '/root/taxi/data/simple/trainer/current/serving_model_dir', '_save_checkpoints_steps': 999, '_task_id': 0, '_master': '', '_tf_random_seed': None, '_experimental_distribute': None, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_train_distribute': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe776436a20>, '_eval_distribute': None, '_log_step_count_steps': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:23:04,909] {estimator.py:201} INFO - Using config: {'_save_summary_steps': 100, '_keep_checkpoint_max': 1, '_protocol': None, '_is_chief': True, '_save_checkpoints_secs': None, '_service': None, '_evaluation_master': '', '_device_fn': None, '_model_dir': '/root/taxi/data/simple/trainer/current/serving_model_dir', '_save_checkpoints_steps': 999, '_task_id': 0, '_master': '', '_tf_random_seed': None, '_experimental_distribute': None, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_train_distribute': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe776436a20>, '_eval_distribute': None, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Training model.\n",
      "[2019-06-25 11:23:04,929] {executor.py:141} INFO - Training model.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "[2019-06-25 11:23:04,938] {estimator_training.py:185} INFO - Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "[2019-06-25 11:23:04,943] {training.py:610} INFO - Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 999 or save_checkpoints_secs None.\n",
      "[2019-06-25 11:23:04,946] {training.py:698} INFO - Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 999 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-25 11:23:05,075] {estimator.py:1111} INFO - Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 11:23:07,013] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "[2019-06-25 11:23:07,019] {basic_session_run_hooks.py:527} INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2019-06-25 11:23:07,378] {monitored_session.py:222} INFO - Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2019-06-25 11:23:07,558] {session_manager.py:491} INFO - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2019-06-25 11:23:07,620] {session_manager.py:493} INFO - Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:08,821] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 0 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:loss = 27.722275, step = 1\n",
      "[2019-06-25 11:23:09,762] {basic_session_run_hooks.py:249} INFO - loss = 27.722275, step = 1\n",
      "INFO:tensorflow:global_step/sec: 138.86\n",
      "[2019-06-25 11:23:10,481] {basic_session_run_hooks.py:680} INFO - global_step/sec: 138.86\n",
      "INFO:tensorflow:loss = 18.453712, step = 101 (0.729 sec)\n",
      "[2019-06-25 11:23:10,491] {basic_session_run_hooks.py:247} INFO - loss = 18.453712, step = 101 (0.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.968\n",
      "[2019-06-25 11:23:10,828] {basic_session_run_hooks.py:680} INFO - global_step/sec: 287.968\n",
      "INFO:tensorflow:loss = 15.570987, step = 201 (0.347 sec)\n",
      "[2019-06-25 11:23:10,838] {basic_session_run_hooks.py:247} INFO - loss = 15.570987, step = 201 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 334.876\n",
      "[2019-06-25 11:23:11,128] {basic_session_run_hooks.py:680} INFO - global_step/sec: 334.876\n",
      "INFO:tensorflow:loss = 16.306086, step = 301 (0.294 sec)\n",
      "[2019-06-25 11:23:11,132] {basic_session_run_hooks.py:247} INFO - loss = 16.306086, step = 301 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.3\n",
      "[2019-06-25 11:23:11,484] {basic_session_run_hooks.py:680} INFO - global_step/sec: 280.3\n",
      "INFO:tensorflow:loss = 18.472464, step = 401 (0.359 sec)\n",
      "[2019-06-25 11:23:11,491] {basic_session_run_hooks.py:247} INFO - loss = 18.472464, step = 401 (0.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.481\n",
      "[2019-06-25 11:23:11,818] {basic_session_run_hooks.py:680} INFO - global_step/sec: 299.481\n",
      "INFO:tensorflow:loss = 22.253315, step = 501 (0.338 sec)\n",
      "[2019-06-25 11:23:11,828] {basic_session_run_hooks.py:247} INFO - loss = 22.253315, step = 501 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.101\n",
      "[2019-06-25 11:23:12,158] {basic_session_run_hooks.py:680} INFO - global_step/sec: 294.101\n",
      "INFO:tensorflow:loss = 22.657928, step = 601 (0.336 sec)\n",
      "[2019-06-25 11:23:12,164] {basic_session_run_hooks.py:247} INFO - loss = 22.657928, step = 601 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 384.928\n",
      "[2019-06-25 11:23:12,417] {basic_session_run_hooks.py:680} INFO - global_step/sec: 384.928\n",
      "INFO:tensorflow:loss = 17.121773, step = 701 (0.259 sec)\n",
      "[2019-06-25 11:23:12,423] {basic_session_run_hooks.py:247} INFO - loss = 17.121773, step = 701 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.489\n",
      "[2019-06-25 11:23:12,703] {basic_session_run_hooks.py:680} INFO - global_step/sec: 350.489\n",
      "INFO:tensorflow:loss = 20.138666, step = 801 (0.285 sec)\n",
      "[2019-06-25 11:23:12,708] {basic_session_run_hooks.py:247} INFO - loss = 20.138666, step = 801 (0.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 352.359\n",
      "[2019-06-25 11:23:12,987] {basic_session_run_hooks.py:680} INFO - global_step/sec: 352.359\n",
      "INFO:tensorflow:loss = 16.755257, step = 901 (0.286 sec)\n",
      "[2019-06-25 11:23:12,994] {basic_session_run_hooks.py:247} INFO - loss = 16.755257, step = 901 (0.286 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 999 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:13,231] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 999 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-25 11:23:13,480] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 11:23:15,069] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 11:23:15,104] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 11:23:15,139] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-06-25T11:23:15Z\n",
      "[2019-06-25 11:23:15,174] {evaluation.py:257} INFO - Starting evaluation at 2019-06-25T11:23:15Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2019-06-25 11:23:15,379] {monitored_session.py:222} INFO - Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "[2019-06-25 11:23:15,383] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2019-06-25 11:23:15,508] {session_manager.py:491} INFO - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2019-06-25 11:23:15,559] {session_manager.py:493} INFO - Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [500/5000]\n",
      "[2019-06-25 11:23:18,275] {evaluation.py:169} INFO - Evaluation [500/5000]\n",
      "INFO:tensorflow:Evaluation [1000/5000]\n",
      "[2019-06-25 11:23:20,356] {evaluation.py:169} INFO - Evaluation [1000/5000]\n",
      "INFO:tensorflow:Evaluation [1500/5000]\n",
      "[2019-06-25 11:23:22,009] {evaluation.py:169} INFO - Evaluation [1500/5000]\n",
      "INFO:tensorflow:Evaluation [2000/5000]\n",
      "[2019-06-25 11:23:24,759] {evaluation.py:169} INFO - Evaluation [2000/5000]\n",
      "INFO:tensorflow:Evaluation [2500/5000]\n",
      "[2019-06-25 11:23:26,235] {evaluation.py:169} INFO - Evaluation [2500/5000]\n",
      "INFO:tensorflow:Evaluation [3000/5000]\n",
      "[2019-06-25 11:23:27,790] {evaluation.py:169} INFO - Evaluation [3000/5000]\n",
      "INFO:tensorflow:Evaluation [3500/5000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:23:29,299] {evaluation.py:169} INFO - Evaluation [3500/5000]\n",
      "INFO:tensorflow:Evaluation [4000/5000]\n",
      "[2019-06-25 11:23:30,736] {evaluation.py:169} INFO - Evaluation [4000/5000]\n",
      "INFO:tensorflow:Evaluation [4500/5000]\n",
      "[2019-06-25 11:23:32,133] {evaluation.py:169} INFO - Evaluation [4500/5000]\n",
      "INFO:tensorflow:Evaluation [5000/5000]\n",
      "[2019-06-25 11:23:33,413] {evaluation.py:169} INFO - Evaluation [5000/5000]\n",
      "INFO:tensorflow:Finished evaluation at 2019-06-25-11:23:33\n",
      "[2019-06-25 11:23:33,506] {evaluation.py:277} INFO - Finished evaluation at 2019-06-25-11:23:33\n",
      "INFO:tensorflow:Saving dict for global step 999: accuracy = 0.769535, accuracy_baseline = 0.769735, auc = 0.9024479, auc_precision_recall = 0.63227355, average_loss = 0.45378688, global_step = 999, label/mean = 0.230265, loss = 18.151476, precision = 0.0, prediction/mean = 0.22809385, recall = 0.0\n",
      "[2019-06-25 11:23:33,510] {estimator.py:1979} INFO - Saving dict for global step 999: accuracy = 0.769535, accuracy_baseline = 0.769735, auc = 0.9024479, auc_precision_recall = 0.63227355, average_loss = 0.45378688, global_step = 999, label/mean = 0.230265, loss = 18.151476, precision = 0.0, prediction/mean = 0.22809385, recall = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 999: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "[2019-06-25 11:23:33,513] {estimator.py:2039} INFO - Saving 'checkpoint_path' summary for global step 999: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "INFO:tensorflow:global_step/sec: 4.86692\n",
      "[2019-06-25 11:23:33,533] {basic_session_run_hooks.py:680} INFO - global_step/sec: 4.86692\n",
      "INFO:tensorflow:loss = 17.923382, step = 1001 (20.545 sec)\n",
      "[2019-06-25 11:23:33,539] {basic_session_run_hooks.py:247} INFO - loss = 17.923382, step = 1001 (20.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 404.602\n",
      "[2019-06-25 11:23:33,781] {basic_session_run_hooks.py:680} INFO - global_step/sec: 404.602\n",
      "INFO:tensorflow:loss = 17.702503, step = 1101 (0.248 sec)\n",
      "[2019-06-25 11:23:33,787] {basic_session_run_hooks.py:247} INFO - loss = 17.702503, step = 1101 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.932\n",
      "[2019-06-25 11:23:34,067] {basic_session_run_hooks.py:680} INFO - global_step/sec: 348.932\n",
      "INFO:tensorflow:loss = 15.509855, step = 1201 (0.283 sec)\n",
      "[2019-06-25 11:23:34,070] {basic_session_run_hooks.py:247} INFO - loss = 15.509855, step = 1201 (0.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 404.843\n",
      "[2019-06-25 11:23:34,314] {basic_session_run_hooks.py:680} INFO - global_step/sec: 404.843\n",
      "INFO:tensorflow:loss = 22.961994, step = 1301 (0.247 sec)\n",
      "[2019-06-25 11:23:34,317] {basic_session_run_hooks.py:247} INFO - loss = 22.961994, step = 1301 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.978\n",
      "[2019-06-25 11:23:34,656] {basic_session_run_hooks.py:680} INFO - global_step/sec: 292.978\n",
      "INFO:tensorflow:loss = 18.030575, step = 1401 (0.344 sec)\n",
      "[2019-06-25 11:23:34,660] {basic_session_run_hooks.py:247} INFO - loss = 18.030575, step = 1401 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.783\n",
      "[2019-06-25 11:23:34,898] {basic_session_run_hooks.py:680} INFO - global_step/sec: 412.783\n",
      "INFO:tensorflow:loss = 19.217838, step = 1501 (0.241 sec)\n",
      "[2019-06-25 11:23:34,901] {basic_session_run_hooks.py:247} INFO - loss = 19.217838, step = 1501 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.834\n",
      "[2019-06-25 11:23:35,147] {basic_session_run_hooks.py:680} INFO - global_step/sec: 401.834\n",
      "INFO:tensorflow:loss = 18.148983, step = 1601 (0.250 sec)\n",
      "[2019-06-25 11:23:35,151] {basic_session_run_hooks.py:247} INFO - loss = 18.148983, step = 1601 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.936\n",
      "[2019-06-25 11:23:35,436] {basic_session_run_hooks.py:680} INFO - global_step/sec: 345.936\n",
      "INFO:tensorflow:loss = 24.860218, step = 1701 (0.290 sec)\n",
      "[2019-06-25 11:23:35,441] {basic_session_run_hooks.py:247} INFO - loss = 24.860218, step = 1701 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 363.425\n",
      "[2019-06-25 11:23:35,711] {basic_session_run_hooks.py:680} INFO - global_step/sec: 363.425\n",
      "INFO:tensorflow:loss = 19.603046, step = 1801 (0.274 sec)\n",
      "[2019-06-25 11:23:35,715] {basic_session_run_hooks.py:247} INFO - loss = 19.603046, step = 1801 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.228\n",
      "[2019-06-25 11:23:36,048] {basic_session_run_hooks.py:680} INFO - global_step/sec: 296.228\n",
      "INFO:tensorflow:loss = 15.541386, step = 1901 (0.341 sec)\n",
      "[2019-06-25 11:23:36,056] {basic_session_run_hooks.py:247} INFO - loss = 15.541386, step = 1901 (0.341 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1998 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:36,318] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 1998 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:23:36,501] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 205.16\n",
      "[2019-06-25 11:23:36,536] {basic_session_run_hooks.py:680} INFO - global_step/sec: 205.16\n",
      "INFO:tensorflow:loss = 16.908092, step = 2001 (0.495 sec)\n",
      "[2019-06-25 11:23:36,551] {basic_session_run_hooks.py:247} INFO - loss = 16.908092, step = 2001 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.681\n",
      "[2019-06-25 11:23:36,878] {basic_session_run_hooks.py:680} INFO - global_step/sec: 292.681\n",
      "INFO:tensorflow:loss = 19.693363, step = 2101 (0.335 sec)\n",
      "[2019-06-25 11:23:36,886] {basic_session_run_hooks.py:247} INFO - loss = 19.693363, step = 2101 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 355.945\n",
      "[2019-06-25 11:23:37,159] {basic_session_run_hooks.py:680} INFO - global_step/sec: 355.945\n",
      "INFO:tensorflow:loss = 18.855537, step = 2201 (0.281 sec)\n",
      "[2019-06-25 11:23:37,167] {basic_session_run_hooks.py:247} INFO - loss = 18.855537, step = 2201 (0.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.747\n",
      "[2019-06-25 11:23:37,490] {basic_session_run_hooks.py:680} INFO - global_step/sec: 301.747\n",
      "INFO:tensorflow:loss = 17.236763, step = 2301 (0.327 sec)\n",
      "[2019-06-25 11:23:37,494] {basic_session_run_hooks.py:247} INFO - loss = 17.236763, step = 2301 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.015\n",
      "[2019-06-25 11:23:37,743] {basic_session_run_hooks.py:680} INFO - global_step/sec: 395.015\n",
      "INFO:tensorflow:loss = 19.002981, step = 2401 (0.252 sec)\n",
      "[2019-06-25 11:23:37,746] {basic_session_run_hooks.py:247} INFO - loss = 19.002981, step = 2401 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.048\n",
      "[2019-06-25 11:23:38,077] {basic_session_run_hooks.py:680} INFO - global_step/sec: 299.048\n",
      "INFO:tensorflow:loss = 20.423687, step = 2501 (0.346 sec)\n",
      "[2019-06-25 11:23:38,091] {basic_session_run_hooks.py:247} INFO - loss = 20.423687, step = 2501 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.949\n",
      "[2019-06-25 11:23:38,318] {basic_session_run_hooks.py:680} INFO - global_step/sec: 415.949\n",
      "INFO:tensorflow:loss = 15.845441, step = 2601 (0.237 sec)\n",
      "[2019-06-25 11:23:38,329] {basic_session_run_hooks.py:247} INFO - loss = 15.845441, step = 2601 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 402.042\n",
      "[2019-06-25 11:23:38,567] {basic_session_run_hooks.py:680} INFO - global_step/sec: 402.042\n",
      "INFO:tensorflow:loss = 17.0261, step = 2701 (0.244 sec)\n",
      "[2019-06-25 11:23:38,572] {basic_session_run_hooks.py:247} INFO - loss = 17.0261, step = 2701 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.794\n",
      "[2019-06-25 11:23:38,853] {basic_session_run_hooks.py:680} INFO - global_step/sec: 348.794\n",
      "INFO:tensorflow:loss = 9.689221, step = 2801 (0.286 sec)\n",
      "[2019-06-25 11:23:38,858] {basic_session_run_hooks.py:247} INFO - loss = 9.689221, step = 2801 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.551\n",
      "[2019-06-25 11:23:39,181] {basic_session_run_hooks.py:680} INFO - global_step/sec: 305.551\n",
      "INFO:tensorflow:loss = 13.586737, step = 2901 (0.328 sec)\n",
      "[2019-06-25 11:23:39,187] {basic_session_run_hooks.py:247} INFO - loss = 13.586737, step = 2901 (0.328 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2997 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:39,466] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 2997 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:23:39,687] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 185.237\n",
      "[2019-06-25 11:23:39,720] {basic_session_run_hooks.py:680} INFO - global_step/sec: 185.237\n",
      "INFO:tensorflow:loss = 15.338256, step = 3001 (0.542 sec)\n",
      "[2019-06-25 11:23:39,729] {basic_session_run_hooks.py:247} INFO - loss = 15.338256, step = 3001 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.675\n",
      "[2019-06-25 11:23:39,985] {basic_session_run_hooks.py:680} INFO - global_step/sec: 377.675\n",
      "INFO:tensorflow:loss = 13.534388, step = 3101 (0.260 sec)\n",
      "[2019-06-25 11:23:39,988] {basic_session_run_hooks.py:247} INFO - loss = 13.534388, step = 3101 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.926\n",
      "[2019-06-25 11:23:40,287] {basic_session_run_hooks.py:680} INFO - global_step/sec: 330.926\n",
      "INFO:tensorflow:loss = 14.332558, step = 3201 (0.303 sec)\n",
      "[2019-06-25 11:23:40,291] {basic_session_run_hooks.py:247} INFO - loss = 14.332558, step = 3201 (0.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.074\n",
      "[2019-06-25 11:23:40,605] {basic_session_run_hooks.py:680} INFO - global_step/sec: 315.074\n",
      "INFO:tensorflow:loss = 14.4254265, step = 3301 (0.318 sec)\n",
      "[2019-06-25 11:23:40,608] {basic_session_run_hooks.py:247} INFO - loss = 14.4254265, step = 3301 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.364\n",
      "[2019-06-25 11:23:40,892] {basic_session_run_hooks.py:680} INFO - global_step/sec: 348.364\n",
      "INFO:tensorflow:loss = 20.622198, step = 3401 (0.290 sec)\n",
      "[2019-06-25 11:23:40,898] {basic_session_run_hooks.py:247} INFO - loss = 20.622198, step = 3401 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 285.467\n",
      "[2019-06-25 11:23:41,242] {basic_session_run_hooks.py:680} INFO - global_step/sec: 285.467\n",
      "INFO:tensorflow:loss = 10.671097, step = 3501 (0.348 sec)\n",
      "[2019-06-25 11:23:41,246] {basic_session_run_hooks.py:247} INFO - loss = 10.671097, step = 3501 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.068\n",
      "[2019-06-25 11:23:41,606] {basic_session_run_hooks.py:680} INFO - global_step/sec: 275.068\n",
      "INFO:tensorflow:loss = 12.6379795, step = 3601 (0.365 sec)\n",
      "[2019-06-25 11:23:41,611] {basic_session_run_hooks.py:247} INFO - loss = 12.6379795, step = 3601 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 304.309\n",
      "[2019-06-25 11:23:41,934] {basic_session_run_hooks.py:680} INFO - global_step/sec: 304.309\n",
      "INFO:tensorflow:loss = 15.643571, step = 3701 (0.330 sec)\n",
      "[2019-06-25 11:23:41,942] {basic_session_run_hooks.py:247} INFO - loss = 15.643571, step = 3701 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.534\n",
      "[2019-06-25 11:23:42,278] {basic_session_run_hooks.py:680} INFO - global_step/sec: 290.534\n",
      "INFO:tensorflow:loss = 13.3749075, step = 3801 (0.347 sec)\n",
      "[2019-06-25 11:23:42,288] {basic_session_run_hooks.py:247} INFO - loss = 13.3749075, step = 3801 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.652\n",
      "[2019-06-25 11:23:42,531] {basic_session_run_hooks.py:680} INFO - global_step/sec: 395.652\n",
      "INFO:tensorflow:loss = 15.425265, step = 3901 (0.259 sec)\n",
      "[2019-06-25 11:23:42,548] {basic_session_run_hooks.py:247} INFO - loss = 15.425265, step = 3901 (0.259 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3996 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:42,810] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 3996 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:23:43,007] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 189.99\n",
      "[2019-06-25 11:23:43,060] {basic_session_run_hooks.py:680} INFO - global_step/sec: 189.99\n",
      "INFO:tensorflow:loss = 14.409432, step = 4001 (0.523 sec)\n",
      "[2019-06-25 11:23:43,070] {basic_session_run_hooks.py:247} INFO - loss = 14.409432, step = 4001 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 320.559\n",
      "[2019-06-25 11:23:43,370] {basic_session_run_hooks.py:680} INFO - global_step/sec: 320.559\n",
      "INFO:tensorflow:loss = 14.634495, step = 4101 (0.302 sec)\n",
      "[2019-06-25 11:23:43,372] {basic_session_run_hooks.py:247} INFO - loss = 14.634495, step = 4101 (0.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.825\n",
      "[2019-06-25 11:23:43,610] {basic_session_run_hooks.py:680} INFO - global_step/sec: 415.825\n",
      "INFO:tensorflow:loss = 16.794651, step = 4201 (0.240 sec)\n",
      "[2019-06-25 11:23:43,612] {basic_session_run_hooks.py:247} INFO - loss = 16.794651, step = 4201 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.074\n",
      "[2019-06-25 11:23:43,861] {basic_session_run_hooks.py:680} INFO - global_step/sec: 398.074\n",
      "INFO:tensorflow:loss = 14.102964, step = 4301 (0.253 sec)\n",
      "[2019-06-25 11:23:43,866] {basic_session_run_hooks.py:247} INFO - loss = 14.102964, step = 4301 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.478\n",
      "[2019-06-25 11:23:44,130] {basic_session_run_hooks.py:680} INFO - global_step/sec: 371.478\n",
      "INFO:tensorflow:loss = 10.666025, step = 4401 (0.267 sec)\n",
      "[2019-06-25 11:23:44,133] {basic_session_run_hooks.py:247} INFO - loss = 10.666025, step = 4401 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 355.115\n",
      "[2019-06-25 11:23:44,412] {basic_session_run_hooks.py:680} INFO - global_step/sec: 355.115\n",
      "INFO:tensorflow:loss = 14.246722, step = 4501 (0.282 sec)\n",
      "[2019-06-25 11:23:44,415] {basic_session_run_hooks.py:247} INFO - loss = 14.246722, step = 4501 (0.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 276.099\n",
      "[2019-06-25 11:23:44,774] {basic_session_run_hooks.py:680} INFO - global_step/sec: 276.099\n",
      "INFO:tensorflow:loss = 10.81937, step = 4601 (0.362 sec)\n",
      "[2019-06-25 11:23:44,777] {basic_session_run_hooks.py:247} INFO - loss = 10.81937, step = 4601 (0.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.85\n",
      "[2019-06-25 11:23:45,042] {basic_session_run_hooks.py:680} INFO - global_step/sec: 372.85\n",
      "INFO:tensorflow:loss = 17.52636, step = 4701 (0.272 sec)\n",
      "[2019-06-25 11:23:45,049] {basic_session_run_hooks.py:247} INFO - loss = 17.52636, step = 4701 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.036\n",
      "[2019-06-25 11:23:45,378] {basic_session_run_hooks.py:680} INFO - global_step/sec: 298.036\n",
      "INFO:tensorflow:loss = 12.775141, step = 4801 (0.345 sec)\n",
      "[2019-06-25 11:23:45,393] {basic_session_run_hooks.py:247} INFO - loss = 12.775141, step = 4801 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.012\n",
      "[2019-06-25 11:23:45,713] {basic_session_run_hooks.py:680} INFO - global_step/sec: 298.012\n",
      "INFO:tensorflow:loss = 15.248242, step = 4901 (0.333 sec)\n",
      "[2019-06-25 11:23:45,726] {basic_session_run_hooks.py:247} INFO - loss = 15.248242, step = 4901 (0.333 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4995 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:46,043] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 4995 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:23:46,327] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 153.144\n",
      "[2019-06-25 11:23:46,367] {basic_session_run_hooks.py:680} INFO - global_step/sec: 153.144\n",
      "INFO:tensorflow:loss = 17.761421, step = 5001 (0.648 sec)\n",
      "[2019-06-25 11:23:46,374] {basic_session_run_hooks.py:247} INFO - loss = 17.761421, step = 5001 (0.648 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.654\n",
      "[2019-06-25 11:23:46,655] {basic_session_run_hooks.py:680} INFO - global_step/sec: 346.654\n",
      "INFO:tensorflow:loss = 13.828413, step = 5101 (0.291 sec)\n",
      "[2019-06-25 11:23:46,664] {basic_session_run_hooks.py:247} INFO - loss = 13.828413, step = 5101 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.026\n",
      "[2019-06-25 11:23:47,007] {basic_session_run_hooks.py:680} INFO - global_step/sec: 284.026\n",
      "INFO:tensorflow:loss = 16.317938, step = 5201 (0.346 sec)\n",
      "[2019-06-25 11:23:47,010] {basic_session_run_hooks.py:247} INFO - loss = 16.317938, step = 5201 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 356.641\n",
      "[2019-06-25 11:23:47,288] {basic_session_run_hooks.py:680} INFO - global_step/sec: 356.641\n",
      "INFO:tensorflow:loss = 14.322586, step = 5301 (0.288 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:23:47,298] {basic_session_run_hooks.py:247} INFO - loss = 14.322586, step = 5301 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.118\n",
      "[2019-06-25 11:23:47,588] {basic_session_run_hooks.py:680} INFO - global_step/sec: 333.118\n",
      "INFO:tensorflow:loss = 14.110179, step = 5401 (0.297 sec)\n",
      "[2019-06-25 11:23:47,595] {basic_session_run_hooks.py:247} INFO - loss = 14.110179, step = 5401 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.706\n",
      "[2019-06-25 11:23:47,882] {basic_session_run_hooks.py:680} INFO - global_step/sec: 339.706\n",
      "INFO:tensorflow:loss = 17.12027, step = 5501 (0.305 sec)\n",
      "[2019-06-25 11:23:47,900] {basic_session_run_hooks.py:247} INFO - loss = 17.12027, step = 5501 (0.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.226\n",
      "[2019-06-25 11:23:48,179] {basic_session_run_hooks.py:680} INFO - global_step/sec: 336.226\n",
      "INFO:tensorflow:loss = 14.580484, step = 5601 (0.283 sec)\n",
      "[2019-06-25 11:23:48,183] {basic_session_run_hooks.py:247} INFO - loss = 14.580484, step = 5601 (0.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.072\n",
      "[2019-06-25 11:23:48,536] {basic_session_run_hooks.py:680} INFO - global_step/sec: 280.072\n",
      "INFO:tensorflow:loss = 14.037632, step = 5701 (0.358 sec)\n",
      "[2019-06-25 11:23:48,542] {basic_session_run_hooks.py:247} INFO - loss = 14.037632, step = 5701 (0.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 329.824\n",
      "[2019-06-25 11:23:48,840] {basic_session_run_hooks.py:680} INFO - global_step/sec: 329.824\n",
      "INFO:tensorflow:loss = 12.465472, step = 5801 (0.305 sec)\n",
      "[2019-06-25 11:23:48,847] {basic_session_run_hooks.py:247} INFO - loss = 12.465472, step = 5801 (0.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 364.937\n",
      "[2019-06-25 11:23:49,114] {basic_session_run_hooks.py:680} INFO - global_step/sec: 364.937\n",
      "INFO:tensorflow:loss = 14.681528, step = 5901 (0.275 sec)\n",
      "[2019-06-25 11:23:49,121] {basic_session_run_hooks.py:247} INFO - loss = 14.681528, step = 5901 (0.275 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5994 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:49,434] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 5994 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:23:49,602] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 185.55\n",
      "[2019-06-25 11:23:49,653] {basic_session_run_hooks.py:680} INFO - global_step/sec: 185.55\n",
      "INFO:tensorflow:loss = 12.969502, step = 6001 (0.537 sec)\n",
      "[2019-06-25 11:23:49,659] {basic_session_run_hooks.py:247} INFO - loss = 12.969502, step = 6001 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.074\n",
      "[2019-06-25 11:23:49,927] {basic_session_run_hooks.py:680} INFO - global_step/sec: 365.074\n",
      "INFO:tensorflow:loss = 15.597416, step = 6101 (0.275 sec)\n",
      "[2019-06-25 11:23:49,934] {basic_session_run_hooks.py:247} INFO - loss = 15.597416, step = 6101 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.524\n",
      "[2019-06-25 11:23:50,228] {basic_session_run_hooks.py:680} INFO - global_step/sec: 331.524\n",
      "INFO:tensorflow:loss = 16.826675, step = 6201 (0.306 sec)\n",
      "[2019-06-25 11:23:50,241] {basic_session_run_hooks.py:247} INFO - loss = 16.826675, step = 6201 (0.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 318.239\n",
      "[2019-06-25 11:23:50,542] {basic_session_run_hooks.py:680} INFO - global_step/sec: 318.239\n",
      "INFO:tensorflow:loss = 12.948369, step = 6301 (0.307 sec)\n",
      "[2019-06-25 11:23:50,546] {basic_session_run_hooks.py:247} INFO - loss = 12.948369, step = 6301 (0.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.017\n",
      "[2019-06-25 11:23:50,864] {basic_session_run_hooks.py:680} INFO - global_step/sec: 311.017\n",
      "INFO:tensorflow:loss = 15.406439, step = 6401 (0.333 sec)\n",
      "[2019-06-25 11:23:50,880] {basic_session_run_hooks.py:247} INFO - loss = 15.406439, step = 6401 (0.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.837\n",
      "[2019-06-25 11:23:51,161] {basic_session_run_hooks.py:680} INFO - global_step/sec: 336.837\n",
      "INFO:tensorflow:loss = 12.449081, step = 6501 (0.290 sec)\n",
      "[2019-06-25 11:23:51,170] {basic_session_run_hooks.py:247} INFO - loss = 12.449081, step = 6501 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.472\n",
      "[2019-06-25 11:23:51,455] {basic_session_run_hooks.py:680} INFO - global_step/sec: 339.472\n",
      "INFO:tensorflow:loss = 12.150339, step = 6601 (0.293 sec)\n",
      "[2019-06-25 11:23:51,462] {basic_session_run_hooks.py:247} INFO - loss = 12.150339, step = 6601 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.173\n",
      "[2019-06-25 11:23:51,729] {basic_session_run_hooks.py:680} INFO - global_step/sec: 365.173\n",
      "INFO:tensorflow:loss = 10.450157, step = 6701 (0.278 sec)\n",
      "[2019-06-25 11:23:51,740] {basic_session_run_hooks.py:247} INFO - loss = 10.450157, step = 6701 (0.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.371\n",
      "[2019-06-25 11:23:52,007] {basic_session_run_hooks.py:680} INFO - global_step/sec: 360.371\n",
      "INFO:tensorflow:loss = 14.453775, step = 6801 (0.272 sec)\n",
      "[2019-06-25 11:23:52,012] {basic_session_run_hooks.py:247} INFO - loss = 14.453775, step = 6801 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.893\n",
      "[2019-06-25 11:23:52,273] {basic_session_run_hooks.py:680} INFO - global_step/sec: 374.893\n",
      "INFO:tensorflow:loss = 14.048001, step = 6901 (0.273 sec)\n",
      "[2019-06-25 11:23:52,285] {basic_session_run_hooks.py:247} INFO - loss = 14.048001, step = 6901 (0.273 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6993 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:52,574] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 6993 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:23:52,738] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 186.744\n",
      "[2019-06-25 11:23:52,809] {basic_session_run_hooks.py:680} INFO - global_step/sec: 186.744\n",
      "INFO:tensorflow:loss = 13.311334, step = 7001 (0.529 sec)\n",
      "[2019-06-25 11:23:52,814] {basic_session_run_hooks.py:247} INFO - loss = 13.311334, step = 7001 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.455\n",
      "[2019-06-25 11:23:53,083] {basic_session_run_hooks.py:680} INFO - global_step/sec: 365.455\n",
      "INFO:tensorflow:loss = 12.297697, step = 7101 (0.272 sec)\n",
      "[2019-06-25 11:23:53,086] {basic_session_run_hooks.py:247} INFO - loss = 12.297697, step = 7101 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.764\n",
      "[2019-06-25 11:23:53,374] {basic_session_run_hooks.py:680} INFO - global_step/sec: 342.764\n",
      "INFO:tensorflow:loss = 12.707218, step = 7201 (0.293 sec)\n",
      "[2019-06-25 11:23:53,378] {basic_session_run_hooks.py:247} INFO - loss = 12.707218, step = 7201 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.878\n",
      "[2019-06-25 11:23:53,639] {basic_session_run_hooks.py:680} INFO - global_step/sec: 377.878\n",
      "INFO:tensorflow:loss = 14.7917, step = 7301 (0.266 sec)\n",
      "[2019-06-25 11:23:53,644] {basic_session_run_hooks.py:247} INFO - loss = 14.7917, step = 7301 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 351.78\n",
      "[2019-06-25 11:23:53,923] {basic_session_run_hooks.py:680} INFO - global_step/sec: 351.78\n",
      "INFO:tensorflow:loss = 10.413122, step = 7401 (0.283 sec)\n",
      "[2019-06-25 11:23:53,928] {basic_session_run_hooks.py:247} INFO - loss = 10.413122, step = 7401 (0.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.985\n",
      "[2019-06-25 11:23:54,182] {basic_session_run_hooks.py:680} INFO - global_step/sec: 386.985\n",
      "INFO:tensorflow:loss = 9.994717, step = 7501 (0.257 sec)\n",
      "[2019-06-25 11:23:54,185] {basic_session_run_hooks.py:247} INFO - loss = 9.994717, step = 7501 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 327.441\n",
      "[2019-06-25 11:23:54,487] {basic_session_run_hooks.py:680} INFO - global_step/sec: 327.441\n",
      "INFO:tensorflow:loss = 12.925981, step = 7601 (0.313 sec)\n",
      "[2019-06-25 11:23:54,497] {basic_session_run_hooks.py:247} INFO - loss = 12.925981, step = 7601 (0.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.539\n",
      "[2019-06-25 11:23:54,850] {basic_session_run_hooks.py:680} INFO - global_step/sec: 275.539\n",
      "INFO:tensorflow:loss = 13.34712, step = 7701 (0.358 sec)\n",
      "[2019-06-25 11:23:54,855] {basic_session_run_hooks.py:247} INFO - loss = 13.34712, step = 7701 (0.358 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 358.419\n",
      "[2019-06-25 11:23:55,129] {basic_session_run_hooks.py:680} INFO - global_step/sec: 358.419\n",
      "INFO:tensorflow:loss = 13.523454, step = 7801 (0.282 sec)\n",
      "[2019-06-25 11:23:55,137] {basic_session_run_hooks.py:247} INFO - loss = 13.523454, step = 7801 (0.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.111\n",
      "[2019-06-25 11:23:55,370] {basic_session_run_hooks.py:680} INFO - global_step/sec: 415.111\n",
      "INFO:tensorflow:loss = 17.571627, step = 7901 (0.237 sec)\n",
      "[2019-06-25 11:23:55,375] {basic_session_run_hooks.py:247} INFO - loss = 17.571627, step = 7901 (0.237 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7992 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:55,695] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 7992 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:23:55,922] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 169.352\n",
      "[2019-06-25 11:23:55,960] {basic_session_run_hooks.py:680} INFO - global_step/sec: 169.352\n",
      "INFO:tensorflow:loss = 12.063576, step = 8001 (0.588 sec)\n",
      "[2019-06-25 11:23:55,963] {basic_session_run_hooks.py:247} INFO - loss = 12.063576, step = 8001 (0.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.291\n",
      "[2019-06-25 11:23:56,224] {basic_session_run_hooks.py:680} INFO - global_step/sec: 379.291\n",
      "INFO:tensorflow:loss = 11.667074, step = 8101 (0.265 sec)\n",
      "[2019-06-25 11:23:56,228] {basic_session_run_hooks.py:247} INFO - loss = 11.667074, step = 8101 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.744\n",
      "[2019-06-25 11:23:56,446] {basic_session_run_hooks.py:680} INFO - global_step/sec: 449.744\n",
      "INFO:tensorflow:loss = 13.054878, step = 8201 (0.234 sec)\n",
      "[2019-06-25 11:23:56,462] {basic_session_run_hooks.py:247} INFO - loss = 13.054878, step = 8201 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 368.462\n",
      "[2019-06-25 11:23:56,718] {basic_session_run_hooks.py:680} INFO - global_step/sec: 368.462\n",
      "INFO:tensorflow:loss = 9.068862, step = 8301 (0.262 sec)\n",
      "[2019-06-25 11:23:56,724] {basic_session_run_hooks.py:247} INFO - loss = 9.068862, step = 8301 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.815\n",
      "[2019-06-25 11:23:56,972] {basic_session_run_hooks.py:680} INFO - global_step/sec: 392.815\n",
      "INFO:tensorflow:loss = 16.904232, step = 8401 (0.256 sec)\n",
      "[2019-06-25 11:23:56,981] {basic_session_run_hooks.py:247} INFO - loss = 16.904232, step = 8401 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.474\n",
      "[2019-06-25 11:23:57,218] {basic_session_run_hooks.py:680} INFO - global_step/sec: 407.474\n",
      "INFO:tensorflow:loss = 15.353744, step = 8501 (0.244 sec)\n",
      "[2019-06-25 11:23:57,225] {basic_session_run_hooks.py:247} INFO - loss = 15.353744, step = 8501 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 444.123\n",
      "[2019-06-25 11:23:57,443] {basic_session_run_hooks.py:680} INFO - global_step/sec: 444.123\n",
      "INFO:tensorflow:loss = 12.318454, step = 8601 (0.228 sec)\n",
      "[2019-06-25 11:23:57,453] {basic_session_run_hooks.py:247} INFO - loss = 12.318454, step = 8601 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.336\n",
      "[2019-06-25 11:23:57,694] {basic_session_run_hooks.py:680} INFO - global_step/sec: 398.336\n",
      "INFO:tensorflow:loss = 10.813039, step = 8701 (0.248 sec)\n",
      "[2019-06-25 11:23:57,700] {basic_session_run_hooks.py:247} INFO - loss = 10.813039, step = 8701 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.099\n",
      "[2019-06-25 11:23:57,967] {basic_session_run_hooks.py:680} INFO - global_step/sec: 366.099\n",
      "INFO:tensorflow:loss = 12.795684, step = 8801 (0.277 sec)\n",
      "[2019-06-25 11:23:57,977] {basic_session_run_hooks.py:247} INFO - loss = 12.795684, step = 8801 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 383.076\n",
      "[2019-06-25 11:23:58,228] {basic_session_run_hooks.py:680} INFO - global_step/sec: 383.076\n",
      "INFO:tensorflow:loss = 11.916353, step = 8901 (0.258 sec)\n",
      "[2019-06-25 11:23:58,235] {basic_session_run_hooks.py:247} INFO - loss = 11.916353, step = 8901 (0.258 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8991 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:23:58,449] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 8991 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:23:58,689] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 187.15\n",
      "[2019-06-25 11:23:58,763] {basic_session_run_hooks.py:680} INFO - global_step/sec: 187.15\n",
      "INFO:tensorflow:loss = 11.971095, step = 9001 (0.545 sec)\n",
      "[2019-06-25 11:23:58,780] {basic_session_run_hooks.py:247} INFO - loss = 11.971095, step = 9001 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 354.986\n",
      "[2019-06-25 11:23:59,044] {basic_session_run_hooks.py:680} INFO - global_step/sec: 354.986\n",
      "INFO:tensorflow:loss = 14.395503, step = 9101 (0.268 sec)\n",
      "[2019-06-25 11:23:59,047] {basic_session_run_hooks.py:247} INFO - loss = 14.395503, step = 9101 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.769\n",
      "[2019-06-25 11:23:59,426] {basic_session_run_hooks.py:680} INFO - global_step/sec: 261.769\n",
      "INFO:tensorflow:loss = 9.834944, step = 9201 (0.382 sec)\n",
      "[2019-06-25 11:23:59,429] {basic_session_run_hooks.py:247} INFO - loss = 9.834944, step = 9201 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 285.622\n",
      "[2019-06-25 11:23:59,776] {basic_session_run_hooks.py:680} INFO - global_step/sec: 285.622\n",
      "INFO:tensorflow:loss = 15.663172, step = 9301 (0.350 sec)\n",
      "[2019-06-25 11:23:59,779] {basic_session_run_hooks.py:247} INFO - loss = 15.663172, step = 9301 (0.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 285.012\n",
      "[2019-06-25 11:24:00,127] {basic_session_run_hooks.py:680} INFO - global_step/sec: 285.012\n",
      "INFO:tensorflow:loss = 12.719607, step = 9401 (0.353 sec)\n",
      "[2019-06-25 11:24:00,132] {basic_session_run_hooks.py:247} INFO - loss = 12.719607, step = 9401 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.478\n",
      "[2019-06-25 11:24:00,457] {basic_session_run_hooks.py:680} INFO - global_step/sec: 303.478\n",
      "INFO:tensorflow:loss = 14.224852, step = 9501 (0.332 sec)\n",
      "[2019-06-25 11:24:00,463] {basic_session_run_hooks.py:247} INFO - loss = 14.224852, step = 9501 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 321.329\n",
      "[2019-06-25 11:24:00,768] {basic_session_run_hooks.py:680} INFO - global_step/sec: 321.329\n",
      "INFO:tensorflow:loss = 11.57789, step = 9601 (0.312 sec)\n",
      "[2019-06-25 11:24:00,776] {basic_session_run_hooks.py:247} INFO - loss = 11.57789, step = 9601 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.754\n",
      "[2019-06-25 11:24:01,083] {basic_session_run_hooks.py:680} INFO - global_step/sec: 317.754\n",
      "INFO:tensorflow:loss = 11.57715, step = 9701 (0.314 sec)\n",
      "[2019-06-25 11:24:01,089] {basic_session_run_hooks.py:247} INFO - loss = 11.57715, step = 9701 (0.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.398\n",
      "[2019-06-25 11:24:01,399] {basic_session_run_hooks.py:680} INFO - global_step/sec: 316.398\n",
      "INFO:tensorflow:loss = 11.571737, step = 9801 (0.326 sec)\n",
      "[2019-06-25 11:24:01,415] {basic_session_run_hooks.py:247} INFO - loss = 11.571737, step = 9801 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.049\n",
      "[2019-06-25 11:24:01,746] {basic_session_run_hooks.py:680} INFO - global_step/sec: 288.049\n",
      "INFO:tensorflow:loss = 11.501966, step = 9901 (0.338 sec)\n",
      "[2019-06-25 11:24:01,753] {basic_session_run_hooks.py:247} INFO - loss = 11.501966, step = 9901 (0.338 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9990 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:24:02,024] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 9990 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:24:02,195] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 11:24:02,244] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 10000 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 11:24:02,405] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-25 11:24:02,492] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 11:24:04,320] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 11:24:04,349] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 11:24:04,391] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-06-25T11:24:04Z\n",
      "[2019-06-25 11:24:04,423] {evaluation.py:257} INFO - Starting evaluation at 2019-06-25T11:24:04Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2019-06-25 11:24:04,672] {monitored_session.py:222} INFO - Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-25 11:24:04,677] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2019-06-25 11:24:04,795] {session_manager.py:491} INFO - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2019-06-25 11:24:04,837] {session_manager.py:493} INFO - Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [500/5000]\n",
      "[2019-06-25 11:24:07,433] {evaluation.py:169} INFO - Evaluation [500/5000]\n",
      "INFO:tensorflow:Evaluation [1000/5000]\n",
      "[2019-06-25 11:24:09,328] {evaluation.py:169} INFO - Evaluation [1000/5000]\n",
      "INFO:tensorflow:Evaluation [1500/5000]\n",
      "[2019-06-25 11:24:11,052] {evaluation.py:169} INFO - Evaluation [1500/5000]\n",
      "INFO:tensorflow:Evaluation [2000/5000]\n",
      "[2019-06-25 11:24:12,878] {evaluation.py:169} INFO - Evaluation [2000/5000]\n",
      "INFO:tensorflow:Evaluation [2500/5000]\n",
      "[2019-06-25 11:24:14,530] {evaluation.py:169} INFO - Evaluation [2500/5000]\n",
      "INFO:tensorflow:Evaluation [3000/5000]\n",
      "[2019-06-25 11:24:16,012] {evaluation.py:169} INFO - Evaluation [3000/5000]\n",
      "INFO:tensorflow:Evaluation [3500/5000]\n",
      "[2019-06-25 11:24:17,320] {evaluation.py:169} INFO - Evaluation [3500/5000]\n",
      "INFO:tensorflow:Evaluation [4000/5000]\n",
      "[2019-06-25 11:24:19,254] {evaluation.py:169} INFO - Evaluation [4000/5000]\n",
      "INFO:tensorflow:Evaluation [4500/5000]\n",
      "[2019-06-25 11:24:20,531] {evaluation.py:169} INFO - Evaluation [4500/5000]\n",
      "INFO:tensorflow:Evaluation [5000/5000]\n",
      "[2019-06-25 11:24:21,936] {evaluation.py:169} INFO - Evaluation [5000/5000]\n",
      "INFO:tensorflow:Finished evaluation at 2019-06-25-11:24:22\n",
      "[2019-06-25 11:24:22,019] {evaluation.py:277} INFO - Finished evaluation at 2019-06-25-11:24:22\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.793985, accuracy_baseline = 0.76985, auc = 0.9444145, auc_precision_recall = 0.7350879, average_loss = 0.33673945, global_step = 10000, label/mean = 0.23015, loss = 13.469578, precision = 0.73244727, prediction/mean = 0.22648408, recall = 0.16521834\n",
      "[2019-06-25 11:24:22,021] {estimator.py:1979} INFO - Saving dict for global step 10000: accuracy = 0.793985, accuracy_baseline = 0.76985, auc = 0.9444145, auc_precision_recall = 0.7350879, average_loss = 0.33673945, global_step = 10000, label/mean = 0.23015, loss = 13.469578, precision = 0.73244727, prediction/mean = 0.22648408, recall = 0.16521834\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-25 11:24:22,028] {estimator.py:2039} INFO - Saving 'checkpoint_path' summary for global step 10000: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "[2019-06-25 11:24:22,033] {exporter.py:415} INFO - Performing the final export in the end of training.\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:24:22,144] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:24:22,146] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:24:22,150] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-25 11:24:22,161] {estimator.py:1111} INFO - Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 11:24:23,366] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['classification', 'serving_default']\n",
      "[2019-06-25 11:24:23,371] {export.py:587} INFO - Signatures INCLUDED in export for Classify: ['classification', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "[2019-06-25 11:24:23,379] {export.py:587} INFO - Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "[2019-06-25 11:24:23,390] {export.py:587} INFO - Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "[2019-06-25 11:24:23,400] {export.py:587} INFO - Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\n",
      "[2019-06-25 11:24:23,404] {export.py:587} INFO - Signatures INCLUDED in export for Regress: ['regression']\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-25 11:24:23,546] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 11:24:23,634] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561461862'/assets\n",
      "[2019-06-25 11:24:23,641] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561461862'/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561461862'/saved_model.pb\n",
      "[2019-06-25 11:24:23,942] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561461862'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 12.977573.\n",
      "[2019-06-25 11:24:23,997] {estimator.py:359} INFO - Loss for final step: 12.977573.\n",
      "INFO:tensorflow:Training complete.  Model written to /root/taxi/data/simple/trainer/current/serving_model_dir\n",
      "[2019-06-25 11:24:24,010] {executor.py:146} INFO - Training complete.  Model written to /root/taxi/data/simple/trainer/current/serving_model_dir\n",
      "INFO:tensorflow:Exporting eval_savedmodel for TFMA.\n",
      "[2019-06-25 11:24:24,017] {executor.py:149} INFO - Exporting eval_savedmodel for TFMA.\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:24:24,165] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 11:24:24,167] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 11:24:24,195] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-25 11:24:24,270] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 11:24:26,747] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 11:24:26,791] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 11:24:26,830] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "[2019-06-25 11:24:26,838] {export.py:587} INFO - Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "[2019-06-25 11:24:26,845] {export.py:587} INFO - Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n",
      "[2019-06-25 11:24:26,849] {export.py:587} INFO - Signatures INCLUDED in export for Eval: ['eval']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "[2019-06-25 11:24:26,862] {export.py:587} INFO - Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "[2019-06-25 11:24:26,867] {export.py:587} INFO - Signatures INCLUDED in export for Regress: None\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "[2019-06-25 11:24:26,873] {tf_logging.py:161} WARNING - Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-25 11:24:27,071] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 11:24:27,198] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561461864'/assets\n",
      "[2019-06-25 11:24:27,201] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561461864'/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561461864'/saved_model.pb\n",
      "[2019-06-25 11:24:27,693] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561461864'/saved_model.pb\n",
      "INFO:tensorflow:Exported eval_savedmodel to /root/taxi/data/simple/trainer/current/eval_model_dir.\n",
      "[2019-06-25 11:24:27,721] {executor.py:155} INFO - Exported eval_savedmodel to /root/taxi/data/simple/trainer/current/eval_model_dir.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 11:24:27,739] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"model_exports\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelExportPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}]}\n",
      "[2019-06-25 11:24:27,751] {base_executor.py:74} INFO - Inputs for Executor is: {\"examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"model_exports\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelExportPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelEvalPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/eval_output/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelEvalPath\"}}]}\n",
      "[2019-06-25 11:24:27,762] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelEvalPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/eval_output/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelEvalPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"feature_slicing_spec\": \"{\\n  \\\"specs\\\": [\\n    {\\n      \\\"columnForSlicing\\\": [\\n        \\\"trip_start_hour\\\"\\n      ]\\n    }\\n  ]\\n}\"}\n",
      "[2019-06-25 11:24:27,774] {base_executor.py:78} INFO - Execution properties for Executor is: {\"feature_slicing_spec\": \"{\\n  \\\"specs\\\": [\\n    {\\n      \\\"columnForSlicing\\\": [\\n        \\\"trip_start_hour\\\"\\n      ]\\n    }\\n  ]\\n}\"}\n",
      "INFO:tensorflow:Using /root/taxi/data/simple/trainer/current/eval_model_dir/1561461864 for model eval.\n",
      "[2019-06-25 11:24:27,786] {<ipython-input-42-a3725876ff5b>:48} INFO - Using /root/taxi/data/simple/trainer/current/eval_model_dir/1561461864 for model eval.\n",
      "INFO:tensorflow:Evaluating model.\n",
      "[2019-06-25 11:24:27,806] {<ipython-input-42-a3725876ff5b>:59} INFO - Evaluating model.\n",
      "[2019-06-25 11:24:27,812] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/slicer/slicer.py:407: BeamDeprecationWarning: RemoveDuplicates is deprecated since 2.12. Use Distinct instead.\n",
      "  | 'IncrementCounter' >> beam.Map(increment_counter))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:24:29,956] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7fe7a1f2c510> ====================\n",
      "[2019-06-25 11:24:29,959] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7fe7a1f2c620> ====================\n",
      "[2019-06-25 11:24:29,961] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7fe7a1f2c6a8> ====================\n",
      "[2019-06-25 11:24:29,966] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7fe7a1f2c730> ====================\n",
      "[2019-06-25 11:24:29,968] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7fe7a1f2c7b8> ====================\n",
      "[2019-06-25 11:24:29,974] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7fe7a1f2c8c8> ====================\n",
      "[2019-06-25 11:24:29,979] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7fe7a1f2c950> ====================\n",
      "[2019-06-25 11:24:29,988] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7fe7a1f2c9d8> ====================\n",
      "[2019-06-25 11:24:29,992] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7fe7a1f2ca60> ====================\n",
      "[2019-06-25 11:24:29,996] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7fe7a1f2cbf8> ====================\n",
      "[2019-06-25 11:24:30,002] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7fe7a1f2cc80> ====================\n",
      "[2019-06-25 11:24:30,010] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7fe7a1f2cd08> ====================\n",
      "[2019-06-25 11:24:30,022] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/DoOnce/Read_68)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/InitializeWrite_69)+(ref_PCollection_PCollection_34/Write)))+(ref_PCollection_PCollection_33/Write)\n",
      "[2019-06-25 11:24:30,047] {fn_api_runner.py:437} INFO - Running (((((((ref_AppliedPTransform_ReadData/Read_3)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/InputsToExtracts/Map(<lambda at model_eval_lib.py:393>)_6)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/Predict/Batch/ParDo(_GlobalWindowsBatchingDoFn)_10)))+(((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/Predict/Predict_11)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/ExtractSliceKeys/ParDo(_ExtractSliceKeysFn)_13))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/Filter/Map(filter_extracts)_16)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/DoSlicing_19))))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/ExtractSliceKeys_21)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/ToPairs_24)))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Group/Write))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Group/Write)\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/eval_model_dir/1561461864/variables/variables\n",
      "[2019-06-25 11:24:31,469] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/eval_model_dir/1561461864/variables/variables\n",
      "[2019-06-25 11:24:44,972] {fn_api_runner.py:437} INFO - Running (((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Group/Read)+((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Merge)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/ExtractOutputs))+(((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/InterpretOutput_56)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/ParDo(_SeparateMetricsAndPlotsFn)/ParDo(_SeparateMetricsAndPlotsFn)_58))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/SerializeMetricsAndPlots/SerializePlots_61)))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/SerializeMetricsAndPlots/SerializeMetrics_60)))+(((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/Map(<lambda at iobase.py:984>)_86)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/WindowInto(WindowIntoFn)_87))+(ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Write)))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/Map(<lambda at iobase.py:984>)_70)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/WindowInto(WindowIntoFn)_71)+(ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-25 11:24:59,314] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/DoOnce/Read_84)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/InitializeWrite_85)+(ref_PCollection_PCollection_44/Write)))+(ref_PCollection_PCollection_43/Write)\n",
      "[2019-06-25 11:24:59,330] {fn_api_runner.py:437} INFO - Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/WriteBundles_92))+(ref_PCollection_PCollection_50/Write)\n",
      "[2019-06-25 11:24:59,559] {fn_api_runner.py:437} INFO - Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/WriteBundles_76))+(ref_PCollection_PCollection_40/Write)\n",
      "[2019-06-25 11:24:59,579] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_33/Read)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/PreFinalize_77)+(ref_PCollection_PCollection_41/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:24:59,599] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_33/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/FinalizeWrite_78)\n",
      "[2019-06-25 11:24:59,616] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:24:59,720] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:24:59,745] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_43/Read)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/PreFinalize_93)+(ref_PCollection_PCollection_51/Write))\n",
      "[2019-06-25 11:24:59,766] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/FinalizeWrite_94)\n",
      "[2019-06-25 11:24:59,783] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:24:59,889] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:24:59,908] {fn_api_runner.py:437} INFO - Running ((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Group/Read)+((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Merge)+((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/ExtractOutputs)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Distinct_32))))+(((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/KeyWithVoid_35)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))\n",
      "[2019-06-25 11:24:59,956] {fn_api_runner.py:437} INFO - Running (((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/UnKey_43)+(ref_PCollection_PCollection_20/Write))\n",
      "[2019-06-25 11:24:59,982] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/DoOnce/Read_102)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/InitializeWrite_103))+(ref_PCollection_PCollection_54/Write))+(ref_PCollection_PCollection_55/Write)\n",
      "[2019-06-25 11:25:00,010] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/CreateEvalConfig/Read_97)+(((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/Map(<lambda at iobase.py:984>)_104)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/WindowInto(WindowIntoFn)_105))+(ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-25 11:25:00,056] {fn_api_runner.py:437} INFO - Running ((ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/WriteBundles_110))+(ref_PCollection_PCollection_61/Write)\n",
      "[2019-06-25 11:25:00,076] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_54/Read)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/PreFinalize_111)+(ref_PCollection_PCollection_62/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:25:00,100] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_54/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/FinalizeWrite_112)\n",
      "[2019-06-25 11:25:00,114] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:25:00,220] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:25:00,240] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/DoOnce/Read_45)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/InjectDefault_46)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/IncrementCounter_47))\n",
      "INFO:tensorflow:Evaluation complete. Results written to /root/taxi/data/simple/eval_output/.\n",
      "[2019-06-25 11:25:00,265] {<ipython-input-42-a3725876ff5b>:72} INFO - Evaluation complete. Results written to /root/taxi/data/simple/eval_output/.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 11:25:00,272] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"model\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelExportPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}]}\n",
      "[2019-06-25 11:25:00,275] {base_executor.py:74} INFO - Inputs for Executor is: {\"examples\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"model\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelExportPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"results\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelValidationPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/model_validator/results/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelValidationPath\"}}], \"blessing\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelBlessingPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/model_validator/blessed/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelBlessingPath\"}}]}\n",
      "[2019-06-25 11:25:00,285] {base_executor.py:76} INFO - Outputs for Executor is: {\"results\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelValidationPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/model_validator/results/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelValidationPath\"}}], \"blessing\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelBlessingPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/model_validator/blessed/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelBlessingPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"blessed_model_id\": null, \"blessed_model\": null}\n",
      "[2019-06-25 11:25:00,293] {base_executor.py:78} INFO - Execution properties for Executor is: {\"blessed_model_id\": null, \"blessed_model\": null}\n",
      "INFO:tensorflow:Using temp path /root/taxi/data/simple/model_validator/results/.temp for tft.beam\n",
      "[2019-06-25 11:25:00,295] {executor.py:138} INFO - Using temp path /root/taxi/data/simple/model_validator/results/.temp for tft.beam\n",
      "INFO:tensorflow:Using /root/taxi/data/simple/trainer/current/ as current model.\n",
      "[2019-06-25 11:25:00,312] {executor.py:145} INFO - Using /root/taxi/data/simple/trainer/current/ as current model.\n",
      "INFO:tensorflow:Using None as blessed model.\n",
      "[2019-06-25 11:25:00,318] {executor.py:152} INFO - Using None as blessed model.\n",
      "INFO:tensorflow:Validating model.\n",
      "[2019-06-25 11:25:00,323] {executor.py:157} INFO - Validating model.\n",
      "[2019-06-25 11:25:00,334] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "[2019-06-25 11:25:01,128] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7fe7a1f2c510> ====================\n",
      "[2019-06-25 11:25:01,131] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7fe7a1f2c620> ====================\n",
      "[2019-06-25 11:25:01,133] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7fe7a1f2c6a8> ====================\n",
      "[2019-06-25 11:25:01,137] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7fe7a1f2c730> ====================\n",
      "[2019-06-25 11:25:01,141] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7fe7a1f2c7b8> ====================\n",
      "[2019-06-25 11:25:01,148] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7fe7a1f2c8c8> ====================\n",
      "[2019-06-25 11:25:01,151] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7fe7a1f2c950> ====================\n",
      "[2019-06-25 11:25:01,167] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7fe7a1f2c9d8> ====================\n",
      "[2019-06-25 11:25:01,179] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7fe7a1f2ca60> ====================\n",
      "[2019-06-25 11:25:01,182] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7fe7a1f2cbf8> ====================\n",
      "[2019-06-25 11:25:01,186] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7fe7a1f2cc80> ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:25:01,197] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7fe7a1f2cd08> ====================\n",
      "[2019-06-25 11:25:01,230] {fn_api_runner.py:437} INFO - Running (((((((ref_AppliedPTransform_ReadData/Read_3)+((ref_AppliedPTransform_EvalCurrentModel/InputsToExtracts/Map(<lambda at model_eval_lib.py:393>)_6)+(ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/Predict/Batch/ParDo(_GlobalWindowsBatchingDoFn)_10)))+(((ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/Predict/Predict_11)+(ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/ExtractSliceKeys/ParDo(_ExtractSliceKeysFn)_13))+((ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/Filter/Map(filter_extracts)_16)+(ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/DoSlicing_19))))+((ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/ExtractSliceKeys_21)+(ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/ToPairs_24)))+(EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Precombine))+(EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Precombine))+(EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Group/Write))+(EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Group/Write)\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/eval_model_dir/1561461864/variables/variables\n",
      "[2019-06-25 11:25:02,299] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/eval_model_dir/1561461864/variables/variables\n",
      "[2019-06-25 11:25:09,048] {fn_api_runner.py:437} INFO - Running (((EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Group/Read)+((((EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Merge)+(EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/ExtractOutputs))+(((ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/InterpretOutput_56)+(ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/ParDo(_SeparateMetricsAndPlotsFn)/ParDo(_SeparateMetricsAndPlotsFn)_58))+(ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/SerializeMetricsAndPlots/SerializePlots_61)))+(ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/SerializeMetricsAndPlots/SerializeMetrics_60)))+(((ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/plots)/WriteToTFRecord/Write/WriteImpl/Map(<lambda at iobase.py:984>)_86)+(ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/plots)/WriteToTFRecord/Write/WriteImpl/WindowInto(WindowIntoFn)_87))+(EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/plots)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Write)))+((ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/metrics)/WriteToTFRecord/Write/WriteImpl/Map(<lambda at iobase.py:984>)_70)+((ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/metrics)/WriteToTFRecord/Write/WriteImpl/WindowInto(WindowIntoFn)_71)+(EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/metrics)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-25 11:25:09,275] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/DoOnce/Read_102)+(ref_AppliedPTransform_EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/InitializeWrite_103))+(ref_PCollection_PCollection_54/Write))+(ref_PCollection_PCollection_55/Write)\n",
      "[2019-06-25 11:25:09,297] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/CreateEvalConfig/Read_97)+(((ref_AppliedPTransform_EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/Map(<lambda at iobase.py:984>)_104)+(ref_AppliedPTransform_EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/WindowInto(WindowIntoFn)_105))+(EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-25 11:25:09,333] {fn_api_runner.py:437} INFO - Running ((EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/WriteBundles_110))+(ref_PCollection_PCollection_61/Write)\n",
      "[2019-06-25 11:25:09,353] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_54/Read)+((ref_AppliedPTransform_EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/PreFinalize_111)+(ref_PCollection_PCollection_62/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:25:09,370] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_54/Read)+(ref_AppliedPTransform_EvalCurrentModel/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561461864', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/FinalizeWrite_112)\n",
      "[2019-06-25 11:25:09,386] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:25:09,489] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:25:09,498] {fn_api_runner.py:437} INFO - Running ((EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Group/Read)+((EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Merge)+((EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/ExtractOutputs)+(ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Distinct_32))))+(((ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/KeyWithVoid_35)+(EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))\n",
      "[2019-06-25 11:25:09,536] {fn_api_runner.py:437} INFO - Running (((EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/UnKey_43)+(ref_PCollection_PCollection_20/Write))\n",
      "[2019-06-25 11:25:09,562] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/metrics)/WriteToTFRecord/Write/WriteImpl/DoOnce/Read_68)+((ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/metrics)/WriteToTFRecord/Write/WriteImpl/InitializeWrite_69)+(ref_PCollection_PCollection_34/Write)))+(ref_PCollection_PCollection_33/Write)\n",
      "[2019-06-25 11:25:09,588] {fn_api_runner.py:437} INFO - Running ((EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/metrics)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/metrics)/WriteToTFRecord/Write/WriteImpl/WriteBundles_76))+(ref_PCollection_PCollection_40/Write)\n",
      "[2019-06-25 11:25:09,614] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_33/Read)+((ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/metrics)/WriteToTFRecord/Write/WriteImpl/PreFinalize_77)+(ref_PCollection_PCollection_41/Write))\n",
      "[2019-06-25 11:25:09,642] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/plots)/WriteToTFRecord/Write/WriteImpl/DoOnce/Read_84)+((ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/plots)/WriteToTFRecord/Write/WriteImpl/InitializeWrite_85)+(ref_PCollection_PCollection_44/Write)))+(ref_PCollection_PCollection_43/Write)\n",
      "[2019-06-25 11:25:09,666] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_33/Read)+(ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/metrics)/WriteToTFRecord/Write/WriteImpl/FinalizeWrite_78)\n",
      "[2019-06-25 11:25:09,681] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:25:09,787] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:25:09,809] {fn_api_runner.py:437} INFO - Running ((EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/plots)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/plots)/WriteToTFRecord/Write/WriteImpl/WriteBundles_92))+(ref_PCollection_PCollection_50/Write)\n",
      "[2019-06-25 11:25:09,834] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_43/Read)+((ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/plots)/WriteToTFRecord/Write/WriteImpl/PreFinalize_93)+(ref_PCollection_PCollection_51/Write))\n",
      "[2019-06-25 11:25:09,852] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_EvalCurrentModel/WriteResults/WriteTFRecord(/root/taxi/data/simple/model_validator/results/.temp/eval_results/current_model/plots)/WriteToTFRecord/Write/WriteImpl/FinalizeWrite_94)\n",
      "[2019-06-25 11:25:09,874] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 11:25:09,977] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 11:25:10,002] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/DoOnce/Read_45)+((ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/InjectDefault_46)+(ref_AppliedPTransform_EvalCurrentModel/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/IncrementCounter_47))\n",
      "INFO:tensorflow:Current model passes threshold.\n",
      "[2019-06-25 11:25:10,051] {executor.py:97} INFO - Current model passes threshold.\n",
      "INFO:tensorflow:No blessed model yet.\n",
      "[2019-06-25 11:25:10,059] {executor.py:100} INFO - No blessed model yet.\n",
      "INFO:tensorflow:Blessing result True written to /root/taxi/data/simple/model_validator/blessed/.\n",
      "[2019-06-25 11:25:10,078] {executor.py:172} INFO - Blessing result True written to /root/taxi/data/simple/model_validator/blessed/.\n",
      "INFO:tensorflow:Cleaned up temp path /root/taxi/data/simple/model_validator/results/.temp on executor success.\n",
      "[2019-06-25 11:25:10,091] {executor.py:176} INFO - Cleaned up temp path /root/taxi/data/simple/model_validator/results/.temp on executor success.\n",
      "INFO:tensorflow:Starting Executor execution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 11:25:10,114] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"model_export\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelExportPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}], \"model_blessing\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelBlessingPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/model_validator/blessed/\", \"customProperties\": {\"current_model_id\": {\"intValue\": \"0\"}, \"blessed\": {\"intValue\": \"1\"}, \"current_model\": {\"stringValue\": \"/root/taxi/data/simple/trainer/current/\"}}}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelBlessingPath\"}}]}\n",
      "[2019-06-25 11:25:10,117] {base_executor.py:74} INFO - Inputs for Executor is: {\"model_export\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelExportPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}], \"model_blessing\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelBlessingPath\"}, \"split\": {\"stringValue\": \"\"}}, \"uri\": \"/root/taxi/data/simple/model_validator/blessed/\", \"customProperties\": {\"current_model_id\": {\"intValue\": \"0\"}, \"blessed\": {\"intValue\": \"1\"}, \"current_model\": {\"stringValue\": \"/root/taxi/data/simple/trainer/current/\"}}}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelBlessingPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"model_push\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelPushPath\"}, \"split\": {\"stringValue\": \"\"}}}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelPushPath\"}}]}\n",
      "[2019-06-25 11:25:10,120] {base_executor.py:76} INFO - Outputs for Executor is: {\"model_push\": [{\"artifact\": {\"properties\": {\"type_name\": {\"stringValue\": \"ModelPushPath\"}, \"split\": {\"stringValue\": \"\"}}}, \"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"state\": \"STRING\"}, \"name\": \"ModelPushPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"push_destination\": \"{\\n  \\\"filesystem\\\": {\\n    \\\"baseDirectory\\\": \\\"/root/taxi/serving_model/taxi_simple\\\"\\n  }\\n}\", \"custom_config\": {}}\n",
      "[2019-06-25 11:25:10,140] {base_executor.py:78} INFO - Execution properties for Executor is: {\"push_destination\": \"{\\n  \\\"filesystem\\\": {\\n    \\\"baseDirectory\\\": \\\"/root/taxi/serving_model/taxi_simple\\\"\\n  }\\n}\", \"custom_config\": {}}\n",
      "INFO:tensorflow:Model pushing.\n",
      "[2019-06-25 11:25:10,141] {executor.py:64} INFO - Model pushing.\n",
      "INFO:tensorflow:Model version is 1561461862\n",
      "[2019-06-25 11:25:10,143] {executor.py:70} INFO - Model version is 1561461862\n",
      "INFO:tensorflow:Model written to .\n",
      "[2019-06-25 11:25:10,281] {executor.py:72} INFO - Model written to .\n",
      "INFO:tensorflow:Model written to serving path /root/taxi/serving_model/taxi_simple/1561461862.\n",
      "[2019-06-25 11:25:10,289] {executor.py:93} INFO - Model written to serving path /root/taxi/serving_model/taxi_simple/1561461862.\n",
      "INFO:tensorflow:Model pushed to /root/taxi/serving_model/taxi_simple/1561461862.\n",
      "[2019-06-25 11:25:10,294] {executor.py:98} INFO - Model pushed to /root/taxi/serving_model/taxi_simple/1561461862.\n"
     ]
    }
   ],
   "source": [
    "pipeline = DirectRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/taxi/data/simple/:\r\n",
      "total 1.9M\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 csv_example_gen\r\n",
      "1.9M -rw-r--r-- 1 root root 1.9M Jun 25 09:05 data.csv\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:08 eval_output\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 schema_gen\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 statistics_gen\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 25 09:05 trainer\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 transform\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen/eval:\r\n",
      "total 204K\r\n",
      "204K -rw-r--r-- 1 root root 201K Jun 25 09:05 data_tfrecord-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen/train:\r\n",
      "total 408K\r\n",
      "408K -rw-r--r-- 1 root root 405K Jun 25 09:05 data_tfrecord-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/eval_output:\r\n",
      "total 34M\r\n",
      "4.0K -rw-r--r-- 1 root root  506 Jun 25 09:08 eval_config\r\n",
      " 12K -rw-r--r-- 1 root root 8.4K Jun 25 09:08 metrics\r\n",
      " 34M -rw-r--r-- 1 root root  34M Jun 25 09:08 plots\r\n",
      "\r\n",
      "/root/taxi/data/simple/schema_gen:\r\n",
      "total 8.0K\r\n",
      "8.0K -rw-r--r-- 1 root root 4.5K Jun 25 09:05 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen/eval:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 17K Jun 25 09:05 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen/train:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 18K Jun 25 09:05 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:07 current\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 25 09:07 eval_model_dir\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:07 serving_model_dir\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:07 1561453650\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650:\r\n",
      "total 864K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:07 assets\r\n",
      "856K -rw-r--r-- 1 root root 854K Jun 25 09:07 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:07 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 25 09:07 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 25 09:07 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650/variables:\r\n",
      "total 68K\r\n",
      "4.0K -rw-r--r-- 1 root root   8 Jun 25 09:07 variables.data-00000-of-00002\r\n",
      " 60K -rw-r--r-- 1 root root 58K Jun 25 09:07 variables.data-00001-of-00002\r\n",
      "4.0K -rw-r--r-- 1 root root 995 Jun 25 09:07 variables.index\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir:\r\n",
      "total 6.1M\r\n",
      "4.0K -rw-r--r-- 1 root root   89 Jun 25 09:07 checkpoint\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:06 eval_chicago-taxi-eval\r\n",
      "3.6M -rw-r--r-- 1 root root 3.6M Jun 25 09:07 events.out.tfevents.1561453552.38545fafb33e\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 25 09:07 export\r\n",
      "1.6M -rw-r--r-- 1 root root 1.6M Jun 25 09:05 graph.pbtxt\r\n",
      "4.0K -rw-r--r-- 1 root root    8 Jun 25 09:07 model.ckpt-10000.data-00000-of-00002\r\n",
      "124K -rw-r--r-- 1 root root 124K Jun 25 09:07 model.ckpt-10000.data-00001-of-00002\r\n",
      "4.0K -rw-r--r-- 1 root root 2.1K Jun 25 09:07 model.ckpt-10000.index\r\n",
      "824K -rw-r--r-- 1 root root 821K Jun 25 09:07 model.ckpt-10000.meta\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/eval_chicago-taxi-eval:\r\n",
      "total 1.3M\r\n",
      "1.3M -rw-r--r-- 1 root root 1.3M Jun 25 09:07 events.out.tfevents.1561453580.38545fafb33e\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 25 09:07 chicago-taxi\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:07 1561453648\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1561453648:\r\n",
      "total 564K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:07 assets\r\n",
      "556K -rw-r--r-- 1 root root 554K Jun 25 09:07 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:07 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1561453648/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 25 09:07 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 25 09:07 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1561453648/variables:\r\n",
      "total 68K\r\n",
      "4.0K -rw-r--r-- 1 root root   8 Jun 25 09:07 variables.data-00000-of-00002\r\n",
      " 60K -rw-r--r-- 1 root root 58K Jun 25 09:07 variables.data-00001-of-00002\r\n",
      "4.0K -rw-r--r-- 1 root root 995 Jun 25 09:07 variables.index\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 5 root root 4.0K Jun 25 09:05 transform_output\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 transformed_examples\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output:\r\n",
      "total 12K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 metadata\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 transform_fn\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 transformed_metadata\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/metadata:\r\n",
      "total 4.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 916 Jun 25 09:05 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn:\r\n",
      "total 84K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 assets\r\n",
      " 76K -rw-r--r-- 1 root root  76K Jun 25 09:05 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 25 09:05 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 25 09:05 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn/variables:\r\n",
      "total 0\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transformed_metadata:\r\n",
      "total 4.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 2.2K Jun 25 09:05 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples/eval:\r\n",
      "total 176K\r\n",
      "176K -rw-r--r-- 1 root root 173K Jun 25 09:05 transformed_examples-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples/train:\r\n",
      "total 348K\r\n",
      "348K -rw-r--r-- 1 root root 348K Jun 25 09:05 transformed_examples-00000-of-00001.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -Rlhs /root/taxi/data/simple/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
