{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Test Run\n",
    "\n",
    "## Set up\n",
    "\n",
    "TFX requires apache-airflow and docker SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-airflow[gcp] in /usr/local/lib/python3.5/dist-packages (1.10.3)\n",
      "\u001b[33m  WARNING: apache-airflow 1.10.3 does not provide the extra 'gcp'\u001b[0m\n",
      "Requirement already satisfied: docker in /usr/local/lib/python3.5/dist-packages (4.0.2)\n",
      "Requirement already satisfied: tfx in /usr/local/lib/python3.5/dist-packages (0.13.0)\n",
      "Requirement already satisfied: tenacity==4.12.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.12.0)\n",
      "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: lxml>=4.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.3.4)\n",
      "Requirement already satisfied: jinja2<=2.10.0,>=2.7.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.10)\n",
      "Requirement already satisfied: json-merge-patch==0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2)\n",
      "Requirement already satisfied: sqlalchemy<1.3.0,>=1.1.15 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2.19)\n",
      "Requirement already satisfied: croniter<0.4,>=0.3.17 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.3.30)\n",
      "Requirement already satisfied: flask-swagger==0.2.13 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.13)\n",
      "Requirement already satisfied: flask-caching<1.4.0,>=1.3.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.3.3)\n",
      "Requirement already satisfied: tabulate<0.9,>=0.7.5 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.8.3)\n",
      "Requirement already satisfied: future<0.17,>=0.16.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.16.0)\n",
      "Requirement already satisfied: iso8601>=0.1.12 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.1.12)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.22.0)\n",
      "Requirement already satisfied: gitpython>=2.0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.11)\n",
      "Requirement already satisfied: zope.deprecation<5.0,>=4.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.4.0)\n",
      "Requirement already satisfied: python-daemon<2.2,>=2.1.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.2)\n",
      "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2)\n",
      "Requirement already satisfied: flask-appbuilder==1.12.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.12.3)\n",
      "Requirement already satisfied: funcsigs==1.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.0)\n",
      "Requirement already satisfied: tzlocal>=1.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.1)\n",
      "Requirement already satisfied: gunicorn<20.0,>=19.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (19.9.0)\n",
      "Requirement already satisfied: flask-admin==1.5.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.3)\n",
      "Requirement already satisfied: pandas<1.0.0,>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.24.2)\n",
      "Requirement already satisfied: thrift>=0.9.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.11.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.9)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.8.0)\n",
      "Requirement already satisfied: pygments<3.0,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.3.1)\n",
      "Requirement already satisfied: psutil<6.0.0,>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (5.6.3)\n",
      "Requirement already satisfied: pendulum==1.4.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.4.4)\n",
      "Requirement already satisfied: flask<2.0,>=1.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.3)\n",
      "Requirement already satisfied: flask-wtf<0.15,>=0.14.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.2)\n",
      "Requirement already satisfied: configparser<3.6.0,>=3.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (3.5.3)\n",
      "Requirement already satisfied: setproctitle<2,>=1.1.8 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.1.10)\n",
      "Requirement already satisfied: markdown<3.0,>=2.5.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.6.11)\n",
      "Requirement already satisfied: alembic<1.0,>=0.9 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.9.10)\n",
      "Requirement already satisfied: werkzeug<0.15.0,>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: flask-login<0.5,>=0.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.4.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.5/dist-packages (from docker) (1.12.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.5/dist-packages (from docker) (0.56.0)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.12 in /usr/local/lib/python3.5/dist-packages (from tfx) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-data-validation<0.14,>=0.13.1 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.1)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: ml-metadata<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.7 in /usr/local/lib/python3.5/dist-packages (from tfx) (3.7.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.5/dist-packages (from tfx) (1.7.9)\n",
      "Requirement already satisfied: absl-py<1,>=0.1.6 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.7.1)\n",
      "Requirement already satisfied: tensorflow-transform<0.14,>=0.13 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.5/dist-packages (from jinja2<=2.10.0,>=2.7.3->apache-airflow[gcp]) (1.1.1)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.5/dist-packages (from flask-swagger==0.2.13->apache-airflow[gcp]) (3.13)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (3.0.4)\n",
      "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from zope.deprecation<5.0,>=4.0->apache-airflow[gcp]) (41.0.1)\n",
      "Requirement already satisfied: docutils in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.14)\n",
      "Requirement already satisfied: lockfile>=0.10 in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.4.0)\n",
      "Requirement already satisfied: Flask-OpenID<2,>=1.2.5 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (1.2.5)\n",
      "Requirement already satisfied: click<8,>=6.7 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (7.0)\n",
      "Requirement already satisfied: Flask-Babel<1,>=0.11.1 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.4.1)\n",
      "Requirement already satisfied: ordereddict in /usr/local/lib/python3.5/dist-packages (from funcsigs==1.0.0->apache-airflow[gcp]) (1.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.5/dist-packages (from tzlocal>=1.4->apache-airflow[gcp]) (2019.1)\n",
      "Requirement already satisfied: wtforms in /usr/local/lib/python3.5/dist-packages (from flask-admin==1.5.3->apache-airflow[gcp]) (2.2.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas<1.0.0,>=0.17.1->apache-airflow[gcp]) (1.16.3)\n",
      "Requirement already satisfied: pytzdata>=2018.3.0.0 in /usr/local/lib/python3.5/dist-packages (from pendulum==1.4.4->apache-airflow[gcp]) (2019.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.5/dist-packages (from flask<2.0,>=1.0->apache-airflow[gcp]) (1.1.0)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.4)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.12)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.5.7)\n",
      "Requirement already satisfied: httplib2<=0.12.0,>=0.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.12.0)\n",
      "Requirement already satisfied: pyarrow<0.14.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.13.0)\n",
      "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.9.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.20.1)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7)\n",
      "Requirement already satisfied: pydot<1.3,>=1.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.2.4)\n",
      "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.21.24)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.0.0)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.0.0)\n",
      "Requirement already satisfied: google-cloud-core<0.30.0,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.29.1)\n",
      "Requirement already satisfied: google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.39.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.6.1)\n",
      "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.1.1)\n",
      "Requirement already satisfied: google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7.4)\n",
      "Requirement already satisfied: google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.5.28)\n",
      "Requirement already satisfied: google-cloud-bigtable<0.33.0,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.32.2)\n",
      "Requirement already satisfied: joblib<1,>=0.12 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.2)\n",
      "Requirement already satisfied: IPython>=5.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (7.5.0)\n",
      "Requirement already satisfied: scikit-learn<1,>=0.18 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.21.2)\n",
      "Requirement already satisfied: tensorflow-metadata<0.14,>=0.12.1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (7.4.2)\n",
      "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.0.0)\n",
      "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.1.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.3)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.6.3)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.0)\n",
      "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitdb2>=2.0.0->gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: python3-openid>=2.0 in /usr/local/lib/python3.5/dist-packages (from Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (3.1.0)\n",
      "Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.5/dist-packages (from Flask-Babel<1,>=0.11.1->flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.7.0)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.5/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.12->tfx) (0.6.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot<1.3,>=1.2.0->apache-beam[gcp]<3,>=2.12->tfx) (2.4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (0.4.5)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (0.2.5)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.5/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.12->tfx) (5.2.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.0.0 in /usr/local/lib/python3.5/dist-packages (from google-cloud-core<0.30.0,>=0.28.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.13.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.12dev,>=0.11.4 in /usr/local/lib/python3.5/dist-packages (from google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.11.4)\n",
      "Requirement already satisfied: google-resumable-media>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.3.2)\n",
      "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.5/dist-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.15)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (2.0.9)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.7.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.3.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.5/dist-packages (from tensorflow-metadata<0.14,>=0.12.1->tensorflow-data-validation<0.14,>=0.13.1->tfx) (1.6.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.4.2)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.7.8)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.0)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.3)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.5.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.5/dist-packages (from python3-openid>=2.0->Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.6.0)\n",
      "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.5/dist-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.5)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.5/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from jedi>=0.10->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.4.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.2->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.0.1)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.5/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.2)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.5/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.2.4)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.2)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.6.0)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (18.0.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.4.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.3)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.4.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.15.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (19.1.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.5/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.5.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'apache-airflow[gcp]' docker tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use TFX version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tfx\n",
    "tfx.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX requires TensorFlow >= 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX supports Python 3.5 from version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 (default, Nov 12 2018, 13:43:14) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-06-25 09:05:01--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1922668 (1.8M) [text/plain]\n",
      "Saving to: ‘/root/taxi/data/simple/data.csv’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2% 3.59M 0s\n",
      "    50K .......... .......... .......... .......... ..........  5% 6.44M 0s\n",
      "   100K .......... .......... .......... .......... ..........  7% 10.1M 0s\n",
      "   150K .......... .......... .......... .......... .......... 10% 8.41M 0s\n",
      "   200K .......... .......... .......... .......... .......... 13% 19.2M 0s\n",
      "   250K .......... .......... .......... .......... .......... 15% 7.20M 0s\n",
      "   300K .......... .......... .......... .......... .......... 18% 8.96M 0s\n",
      "   350K .......... .......... .......... .......... .......... 21% 11.4M 0s\n",
      "   400K .......... .......... .......... .......... .......... 23% 7.54M 0s\n",
      "   450K .......... .......... .......... .......... .......... 26% 13.9M 0s\n",
      "   500K .......... .......... .......... .......... .......... 29% 16.6M 0s\n",
      "   550K .......... .......... .......... .......... .......... 31% 7.32M 0s\n",
      "   600K .......... .......... .......... .......... .......... 34% 26.5M 0s\n",
      "   650K .......... .......... .......... .......... .......... 37% 11.8M 0s\n",
      "   700K .......... .......... .......... .......... .......... 39% 15.2M 0s\n",
      "   750K .......... .......... .......... .......... .......... 42% 13.7M 0s\n",
      "   800K .......... .......... .......... .......... .......... 45% 20.1M 0s\n",
      "   850K .......... .......... .......... .......... .......... 47% 14.3M 0s\n",
      "   900K .......... .......... .......... .......... .......... 50% 10.8M 0s\n",
      "   950K .......... .......... .......... .......... .......... 53% 56.1M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 55% 10.8M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 58% 12.5M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 61% 13.7M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 63% 8.01M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 66% 17.8M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 69% 13.4M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 71% 6.10M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 74% 4.11M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 77% 3.51M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 79% 3.06M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 82% 5.57M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 85% 4.26M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 87% 20.9M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 90% 8.61M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 93% 9.42M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 95% 11.3M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 98% 23.6M 0s\n",
      "  1850K .......... .......... .......                         100% 41.7M=0.2s\n",
      "\n",
      "2019-06-25 09:05:02 (8.77 MB/s) - ‘/root/taxi/data/simple/data.csv’ saved [1922668/1922668]\n",
      "\n",
      "--2019-06-25 09:05:02--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12084 (12K) [text/plain]\n",
      "Saving to: ‘/root/taxi/taxi_utils.py’\n",
      "\n",
      "     0K .......... .                                          100% 32.9M=0s\n",
      "\n",
      "2019-06-25 09:05:02 (32.9 MB/s) - ‘/root/taxi/taxi_utils.py’ saved [12084/12084]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# This enables you to run this notebook twice.\n",
    "# There should not be train/eval files at ~/taxi/data, since TFX can handle only single file with version 0.13.0\n",
    "if [ -e ~/taxi/data ]; then\n",
    "    rm -rf ~/taxi/data\n",
    "fi\n",
    "\n",
    "# download taxi data\n",
    "mkdir -p ~/taxi/data/simple\n",
    "mkdir -p ~/taxi/serving_model/taxi_simple\n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv -O ~/taxi/data/simple/data.csv\n",
    "\n",
    "# download \n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py -O ~/taxi/taxi_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
      "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.protobuf import json_format\n",
    "\n",
    "from tfx.components.base.base_component import ComponentOutputs\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.orchestration.tfx_runner import TfxRunner\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import csv_input\n",
    "from tfx.utils.channel import Channel\n",
    "from tfx.utils import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This example assumes that the taxi data is stored in ~/taxi/data and the\n",
    "# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n",
    "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
    "_data_root = os.path.join(_taxi_root, 'data/simple')\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')\n",
    "\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(_taxi_root, 'serving_model/taxi_simple')\n",
    "\n",
    "# Directory and data locations.  This example assumes all of the chicago taxi\n",
    "# example code and metadata library is relative to $HOME, but you can store\n",
    "# these files anywhere on your local filesystem.\n",
    "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines')\n",
    "_metadata_db_root = os.path.join(_tfx_root, 'metadata')\n",
    "_log_root = os.path.join(_tfx_root, 'logs')\n",
    "\n",
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "_airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2019, 1, 1),\n",
    "}\n",
    "\n",
    "# Logging overrides\n",
    "logger_overrides = {'log_root': _log_root, 'log_level': logging.INFO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Implements the chicago taxi pipeline with TFX.\"\"\"\n",
    "examples = csv_input(_data_root)\n",
    "\n",
    "# Brings data into the pipeline or otherwise joins/converts training data.\n",
    "train_config = example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2)\n",
    "eval_config = example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        train_config,\n",
    "        eval_config\n",
    "    ]))\n",
    "\n",
    "# Create outputs\n",
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root, 'csv_example_gen/train/')\n",
    "\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root, 'csv_example_gen/eval/')\n",
    "\n",
    "example_outputs = ComponentOutputs({\n",
    "    'examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    ),\n",
    "    'training_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples]\n",
    "    ),\n",
    "    'eval_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[eval_examples]\n",
    "    ),    \n",
    "})\n",
    "\n",
    "example_gen = CsvExampleGen(\n",
    "    input_base=examples, # A Channel of 'ExternalPath' type, it contains path of data source.\n",
    "    output_config=output_config,  # An example_gen_pb2.Output instance, it contains train-eval split ratio.\n",
    "    outputs=example_outputs # dict from name to output channel, it will be stored example_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='train')\n",
    "train_statistics.uri = os.path.join(_data_root, 'statistics_gen/train/')\n",
    "\n",
    "eval_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='eval')\n",
    "eval_statistics.uri = os.path.join(_data_root, 'statistics_gen/eval/')\n",
    "\n",
    "statistics_outputs = ComponentOutputs({\n",
    "    'output': Channel(\n",
    "        type_name='ExampleStatisticsPath',\n",
    "        static_artifact_collection=[train_statistics, eval_statistics]\n",
    "    )\n",
    "})\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    input_data=example_gen.outputs.examples, # A Channel of 'ExamplesPath' type, it is equal to example_outputs\n",
    "    name='Statistics Generator', # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=statistics_outputs # dict from name to output channel, it will be stored statistics_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_schema_path = types.TfxType(type_name='SchemaPath', split='train')\n",
    "train_schema_path.uri = os.path.join(_data_root, 'schema_gen/')\n",
    "\n",
    "# NOTE: SchemaGen.executor can handle JUST ONE SchemaPath.\n",
    "# Two or more SchemaPaths will cause ValueError\n",
    "# such as \"ValueError: expected list length of one but got 2\".\n",
    "schema_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='SchemaPath',\n",
    "        static_artifact_collection=[train_schema_path] \n",
    "    )\n",
    "})\n",
    "\n",
    "infer_schema = SchemaGen(\n",
    "    stats=statistics_gen.outputs.output, # A Channel of 'ExampleStatisticsPath' type, it is equal to statistics_outputs\n",
    "    name='Schema Generator',  # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=schema_outputs # dict from name to output channel, it will be stored schema_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root,\n",
    "                                  'transform/transformed_examples/train/')\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root,\n",
    "                                 'transform/transformed_examples/eval/')\n",
    "transform_output = types.TfxType(type_name='TransformPath')\n",
    "transform_output.uri = os.path.join(_data_root,\n",
    "                                    'transform/transform_output/')\n",
    "\n",
    "transform_outputs = ComponentOutputs({\n",
    "    # Output of 'tf.Transform', which includes an exported \n",
    "    # Tensorflow graph suitable for both training and serving\n",
    "    'transform_output':Channel(\n",
    "        type_name='TransformPath',\n",
    "        static_artifact_collection=[transform_output]\n",
    "    ),\n",
    "    # transformed_examples: Materialized transformed examples, which includes \n",
    "    # both 'train' and 'eval' splits.\n",
    "    'transformed_examples':Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    )\n",
    "})\n",
    "\n",
    "transform = Transform(\n",
    "    input_data=example_gen.outputs.examples,\n",
    "    schema=infer_schema.outputs.output,\n",
    "    module_file=_taxi_module_file,\n",
    "    outputs=transform_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_exports = types.TfxType(type_name='ModelExportPath')\n",
    "model_exports.uri = os.path.join(_data_root, 'trainer/current/')\n",
    "\n",
    "trainer_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='ModelExportPath',\n",
    "        static_artifact_collection=[model_exports]\n",
    "    )\n",
    "})\n",
    "\n",
    "trainer = Trainer(\n",
    "    module_file=_taxi_module_file,\n",
    "    transformed_examples=transform.outputs.transformed_examples,\n",
    "    schema=infer_schema.outputs.output,\n",
    "    transform_output=transform.outputs.transform_output,\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=5000),\n",
    "    outputs=trainer_outputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_output = types.TfxType('ModelEvalPath')\n",
    "eval_output.uri = os.path.join(_data_root, 'eval_output/')\n",
    "\n",
    "model_analyzer_outputs = ComponentOutputs({\n",
    "    'output':\n",
    "    Channel(\n",
    "        type_name='ModelEvalPath',\n",
    "        static_artifact_collection=[eval_output]),\n",
    "})\n",
    "\n",
    "feature_slicing_spec = evaluator_pb2.FeatureSlicingSpec(specs=[\n",
    "    evaluator_pb2.SingleSlicingSpec(\n",
    "        column_for_slicing=['trip_start_hour'])\n",
    "])\n",
    "\n",
    "model_analyzer = Evaluator(\n",
    "    examples=example_gen.outputs.examples,\n",
    "    model_exports=trainer.outputs.output,\n",
    "    feature_slicing_spec=feature_slicing_spec,\n",
    "    outputs=model_analyzer_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _Do(self, input_dict, output_dict, exec_properties):\n",
    "    import apache_beam as beam\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_model_analysis as tfma\n",
    "    from typing import Any, Dict, List, Text\n",
    "    from tfx.components.base import base_executor\n",
    "    from tfx.proto import evaluator_pb2\n",
    "    from tfx.utils import io_utils\n",
    "    from tfx.utils import path_utils\n",
    "    from tfx.utils import types\n",
    "    from google.protobuf import json_format\n",
    "\n",
    "    \"\"\"Runs a batch job to evaluate the eval_model against the given input.\n",
    "    Args:\n",
    "      input_dict: Input dict from input key to a list of Artifacts.\n",
    "        - model_exports: exported model.\n",
    "        - examples: examples for eval the model.\n",
    "      output_dict: Output dict from output key to a list of Artifacts.\n",
    "        - output: model evaluation results.\n",
    "      exec_properties: A dict of execution properties.\n",
    "        - feature_slicing_spec: JSON string of evaluator_pb2.FeatureSlicingSpec\n",
    "          instance, providing the way to slice the data.\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    if 'model_exports' not in input_dict:\n",
    "      raise ValueError('\\'model_exports\\' is missing in input dict.')\n",
    "    if 'examples' not in input_dict:\n",
    "      raise ValueError('\\'examples\\' is missing in input dict.')\n",
    "    if 'output' not in output_dict:\n",
    "      raise ValueError('\\'output\\' is missing in output dict.')\n",
    "\n",
    "    self._log_startup(input_dict, output_dict, exec_properties)\n",
    "\n",
    "    # Extract input artifacts\n",
    "    model_exports_uri = types.get_single_uri(input_dict['model_exports'])\n",
    "\n",
    "    feature_slicing_spec = evaluator_pb2.FeatureSlicingSpec()\n",
    "    json_format.Parse(exec_properties['feature_slicing_spec'],\n",
    "                      feature_slicing_spec)\n",
    "    slice_spec = self._get_slice_spec_from_feature_slicing_spec(\n",
    "        feature_slicing_spec)\n",
    "\n",
    "    output_uri = types.get_single_uri(output_dict['output'])\n",
    "\n",
    "    eval_model_path = path_utils.eval_model_path(model_exports_uri)\n",
    "\n",
    "    tf.logging.info('Using {} for model eval.'.format(eval_model_path))\n",
    "    eval_shared_model = tfma.default_eval_shared_model(\n",
    "        add_metrics_callbacks=[\n",
    "                        # calibration_plot_and_prediction_histogram computes calibration plot and prediction\n",
    "                        # distribution at different thresholds.\n",
    "                        tfma.post_export_metrics.calibration_plot_and_prediction_histogram(),\n",
    "                        # auc_plots enables precision-recall curve and ROC visualization at different thresholds.\n",
    "                        tfma.post_export_metrics.auc_plots()\n",
    "                    ],\n",
    "        eval_saved_model_path=eval_model_path)\n",
    "\n",
    "    tf.logging.info('Evaluating model.')\n",
    "    with beam.Pipeline(argv=self._get_beam_pipeline_args()) as pipeline:\n",
    "      # pylint: disable=expression-not-assigned\n",
    "      (pipeline\n",
    "       | 'ReadData' >> beam.io.ReadFromTFRecord(\n",
    "           file_pattern=io_utils.all_files_pattern(\n",
    "               types.get_split_uri(input_dict['examples'], 'eval')))\n",
    "       |\n",
    "       'ExtractEvaluateAndWriteResults' >> tfma.ExtractEvaluateAndWriteResults(\n",
    "           eval_shared_model=eval_shared_model,\n",
    "           slice_spec=slice_spec,\n",
    "           output_path=output_uri))\n",
    "    tf.logging.info(\n",
    "        'Evaluation complete. Results written to {}.'.format(output_uri))\n",
    "\n",
    "model_analyzer.executor.Do = _Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"TFX Pipeline\",\n",
    "    pipeline_root=_pipeline_root,\n",
    "    components=[example_gen, statistics_gen, infer_schema, transform, trainer, model_analyzer]\n",
    "#     components=[model_analyzer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectRunner(TfxRunner):\n",
    "    \"\"\"Tfx runner on local\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self._config = config or {}\n",
    "    \n",
    "    def run(self, pipeline):\n",
    "        for component in pipeline.components:\n",
    "            self._execute_component(component)\n",
    "            \n",
    "        return pipeline\n",
    "            \n",
    "    def _execute_component(self, component):\n",
    "        input_dict = {key:value.get() for key, value in component.input_dict.items()}\n",
    "        output_dict = {key: value.get() for key, value in component.outputs.get_all().items()}\n",
    "        exec_properties = component.exec_properties\n",
    "        executor = component.executor()\n",
    "        executor.Do(input_dict, output_dict, exec_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 09:05:04,737] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ExternalPath\"}}}, \"artifact_type\": {\"name\": \"ExternalPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "[2019-06-25 09:05:04,743] {base_executor.py:74} INFO - Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ExternalPath\"}}}, \"artifact_type\": {\"name\": \"ExternalPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"training_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"eval_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "[2019-06-25 09:05:04,747] {base_executor.py:76} INFO - Outputs for Executor is: {\"training_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"eval_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"name\\\": \\\"train\\\",\\n        \\\"hashBuckets\\\": 2\\n      },\\n      {\\n        \\\"name\\\": \\\"eval\\\",\\n        \\\"hashBuckets\\\": 1\\n      }\\n    ]\\n  }\\n}\"}\n",
      "[2019-06-25 09:05:04,749] {base_executor.py:78} INFO - Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"name\\\": \\\"train\\\",\\n        \\\"hashBuckets\\\": 2\\n      },\\n      {\\n        \\\"name\\\": \\\"eval\\\",\\n        \\\"hashBuckets\\\": 1\\n      }\\n    ]\\n  }\\n}\"}\n",
      "INFO:tensorflow:Generating examples.\n",
      "[2019-06-25 09:05:04,750] {base_example_gen_executor.py:122} INFO - Generating examples.\n",
      "[2019-06-25 09:05:04,759] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-25 09:05:04,804] {executor.py:70} INFO - Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-25 09:05:05,634] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f8756bbd6a8> ====================\n",
      "[2019-06-25 09:05:05,636] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f8756bbd7b8> ====================\n",
      "[2019-06-25 09:05:05,639] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f8756bbd840> ====================\n",
      "[2019-06-25 09:05:05,642] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f8756bbd8c8> ====================\n",
      "[2019-06-25 09:05:05,645] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f8756bbd950> ====================\n",
      "[2019-06-25 09:05:05,650] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f8756bbda60> ====================\n",
      "[2019-06-25 09:05:05,654] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f8756bbdae8> ====================\n",
      "[2019-06-25 09:05:05,665] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f8756bbdb70> ====================\n",
      "[2019-06-25 09:05:05,668] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f8756bbdbf8> ====================\n",
      "[2019-06-25 09:05:05,671] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f8756bbdd90> ====================\n",
      "[2019-06-25 09:05:05,681] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f8756bbde18> ====================\n",
      "[2019-06-25 09:05:05,685] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f8756bbdea0> ====================\n",
      "[2019-06-25 09:05:05,697] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/DoOnce/Read_66)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/InitializeWrite_67))+(ref_PCollection_PCollection_42/Write))+(ref_PCollection_PCollection_43/Write)\n",
      "[2019-06-25 09:05:05,704] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_InputSourceToExample/ReadFromText/Read_4)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/ParseCSVRecords_6))+((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/KeyWithVoid_8)+((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Precombine)+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Write))))+(ref_PCollection_PCollection_2/Write)\n",
      "[2019-06-25 09:05:06,885] {fn_api_runner.py:437} INFO - Running (InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Read)+(((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Merge)+((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/UnKey_16)))+(ref_PCollection_PCollection_8/Write))\n",
      "[2019-06-25 09:05:06,903] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/DoOnce/Read_18)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/InjectDefault_19))+(ref_PCollection_PCollection_10/Write)\n",
      "[2019-06-25 09:05:06,926] {fn_api_runner.py:437} INFO - Running (((ref_PCollection_PCollection_2/Read)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/CreateInMemoryDict_20))+(ref_AppliedPTransform_InputSourceToExample/ToTFExample_21))+(((ref_AppliedPTransform_SerializeDeterministically_22)+(((((ref_AppliedPTransform_SplitData/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_25)+(ref_AppliedPTransform_ShuffleSpliteval/AddRandomKeys_53))+(ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/Map(reify_timestamps)_55))+(ShuffleSpliteval/ReshufflePerKey/GroupByKey/Write))+((ref_AppliedPTransform_ShuffleSplittrain/AddRandomKeys_27)+(ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/Map(reify_timestamps)_29))))+(ShuffleSplittrain/ReshufflePerKey/GroupByKey/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:12,103] {fn_api_runner.py:437} INFO - Running ((ShuffleSpliteval/ReshufflePerKey/GroupByKey/Read)+((ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/FlatMap(restore_timestamps)_60)+(ref_AppliedPTransform_ShuffleSpliteval/RemoveRandomKeys_61)))+(((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WriteBundles_68)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Pair_69))+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WindowInto(WindowIntoFn)_70)+(OutputSpliteval/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-25 09:05:12,192] {tfrecordio.py:57} WARNING - Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "[2019-06-25 09:05:12,383] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/DoOnce/Read_40)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/InitializeWrite_41)+(ref_PCollection_PCollection_25/Write)))+(ref_PCollection_PCollection_24/Write)\n",
      "[2019-06-25 09:05:12,401] {fn_api_runner.py:437} INFO - Running (((ShuffleSplittrain/ReshufflePerKey/GroupByKey/Read)+((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/FlatMap(restore_timestamps)_34)+(ref_AppliedPTransform_ShuffleSplittrain/RemoveRandomKeys_35)))+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WriteBundles_42))+(((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Pair_43)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WindowInto(WindowIntoFn)_44))+(OutputSplittrain/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-25 09:05:12,860] {fn_api_runner.py:437} INFO - Running ((OutputSpliteval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Extract_75))+(ref_PCollection_PCollection_50/Write)\n",
      "[2019-06-25 09:05:12,873] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/PreFinalize_76))+(ref_PCollection_PCollection_51/Write)\n",
      "[2019-06-25 09:05:12,890] {fn_api_runner.py:437} INFO - Running ((OutputSplittrain/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Extract_49))+(ref_PCollection_PCollection_32/Write)\n",
      "[2019-06-25 09:05:12,909] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/PreFinalize_50)+(ref_PCollection_PCollection_33/Write))\n",
      "[2019-06-25 09:05:12,927] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/FinalizeWrite_51)\n",
      "[2019-06-25 09:05:12,947] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:05:13,054] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "[2019-06-25 09:05:13,074] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/FinalizeWrite_77)\n",
      "[2019-06-25 09:05:13,090] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:05:13,196] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Examples generated.\n",
      "[2019-06-25 09:05:13,216] {base_example_gen_executor.py:145} INFO - Examples generated.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 09:05:13,222] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "[2019-06-25 09:05:13,247] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "[2019-06-25 09:05:13,252] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {}\n",
      "[2019-06-25 09:05:13,256] {base_executor.py:78} INFO - Execution properties for Executor is: {}\n",
      "[2019-06-25 09:05:13,271] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Generating statistics for split train\n",
      "[2019-06-25 09:05:13,283] {executor.py:62} INFO - Generating statistics for split train\n",
      "INFO:tensorflow:Generating statistics for split eval\n",
      "[2019-06-25 09:05:14,031] {executor.py:62} INFO - Generating statistics for split eval\n",
      "INFO:tensorflow:Statistics written to /root/taxi/data/simple/statistics_gen/eval/.\n",
      "[2019-06-25 09:05:14,916] {executor.py:78} INFO - Statistics written to /root/taxi/data/simple/statistics_gen/eval/.\n",
      "[2019-06-25 09:05:17,685] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f8756bbd6a8> ====================\n",
      "[2019-06-25 09:05:17,688] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f8756bbd7b8> ====================\n",
      "[2019-06-25 09:05:17,690] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f8756bbd840> ====================\n",
      "[2019-06-25 09:05:17,702] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f8756bbd8c8> ====================\n",
      "[2019-06-25 09:05:17,706] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f8756bbd950> ====================\n",
      "[2019-06-25 09:05:17,713] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f8756bbda60> ====================\n",
      "[2019-06-25 09:05:17,719] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f8756bbdae8> ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:17,739] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f8756bbdb70> ====================\n",
      "[2019-06-25 09:05:17,746] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f8756bbdbf8> ====================\n",
      "[2019-06-25 09:05:17,748] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f8756bbdd90> ====================\n",
      "[2019-06-25 09:05:17,754] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f8756bbde18> ====================\n",
      "[2019-06-25 09:05:17,758] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f8756bbdea0> ====================\n",
      "[2019-06-25 09:05:17,787] {fn_api_runner.py:437} INFO - Running (((((((ref_AppliedPTransform_ReadData.eval/Read_106)+(ref_AppliedPTransform_DecodeData.eval/ParseTFExamples_108))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/KeyWithVoid_111))+((((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_115)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_116))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write))))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_135))+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_136)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_138))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0)\n",
      "[2019-06-25 09:05:20,687] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+(((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_146))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_156))+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_158)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)))\n",
      "[2019-06-25 09:05:20,720] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_166)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1)))\n",
      "[2019-06-25 09:05:20,738] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadData.train/Read_3)+(ref_AppliedPTransform_DecodeData.train/ParseTFExamples_5))+(((((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/KeyWithVoid_8)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_12)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0))))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_32))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_13))+((((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_33)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_35))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write))))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write))\n",
      "[2019-06-25 09:05:26,448] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_21)))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_22))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:26,707] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write))\n",
      "[2019-06-25 09:05:26,891] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)))\n",
      "[2019-06-25 09:05:27,185] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/DoOnce/Read_94)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/InitializeWrite_95))+(ref_PCollection_PCollection_55/Write))+(ref_PCollection_PCollection_56/Write)\n",
      "[2019-06-25 09:05:27,204] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+((((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_43))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_53)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_55)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write))\n",
      "[2019-06-25 09:05:27,244] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_52)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0))\n",
      "[2019-06-25 09:05:27,263] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_63)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1)))\n",
      "[2019-06-25 09:05:27,283] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_65)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)))\n",
      "[2019-06-25 09:05:27,305] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "[2019-06-25 09:05:27,321] {fn_api_runner.py:437} INFO - Running ((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_74))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_77)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write))\n",
      "[2019-06-25 09:05:27,354] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_85)+(ref_PCollection_PCollection_51/Write)))\n",
      "[2019-06-25 09:05:27,374] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_87)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_88))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_89)+(((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/Map(<lambda at iobase.py:984>)_96)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WindowInto(WindowIntoFn)_97))+(WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-25 09:05:27,418] {fn_api_runner.py:437} INFO - Running (WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WriteBundles_102)+(ref_PCollection_PCollection_62/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:27,434] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/PreFinalize_103))+(ref_PCollection_PCollection_63/Write)\n",
      "[2019-06-25 09:05:27,455] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/FinalizeWrite_104)\n",
      "[2019-06-25 09:05:27,468] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:05:27,572] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 09:05:27,595] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge))+((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_124))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_125))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1))\n",
      "[2019-06-25 09:05:28,011] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "[2019-06-25 09:05:28,151] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0))\n",
      "[2019-06-25 09:05:28,459] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_155)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)))\n",
      "[2019-06-25 09:05:28,482] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_168))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "[2019-06-25 09:05:28,503] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write))\n",
      "[2019-06-25 09:05:28,521] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/DoOnce/Read_197)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/InitializeWrite_198))+(ref_PCollection_PCollection_119/Write))+(ref_PCollection_PCollection_120/Write)\n",
      "[2019-06-25 09:05:28,540] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_177)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_180)))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write))\n",
      "[2019-06-25 09:05:28,567] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_188)+(ref_PCollection_PCollection_115/Write))))\n",
      "[2019-06-25 09:05:28,592] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_190)+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_191)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_192)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/Map(<lambda at iobase.py:984>)_199)))+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WindowInto(WindowIntoFn)_200)))+(WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Write)\n",
      "[2019-06-25 09:05:28,638] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WriteBundles_205))+(ref_PCollection_PCollection_126/Write)\n",
      "[2019-06-25 09:05:28,667] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/PreFinalize_206)+(ref_PCollection_PCollection_127/Write))\n",
      "[2019-06-25 09:05:28,690] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/FinalizeWrite_207)\n",
      "[2019-06-25 09:05:28,711] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:05:28,817] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Infering schema from statistics.\n",
      "[2019-06-25 09:05:28,841] {executor.py:62} INFO - Infering schema from statistics.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:28,844] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:tensorflow:Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
      "[2019-06-25 09:05:28,860] {executor.py:66} INFO - Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 09:05:28,865] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "[2019-06-25 09:05:28,869] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"transformed_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"transform_output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}}, \"artifact_type\": {\"name\": \"TransformPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "[2019-06-25 09:05:28,876] {base_executor.py:76} INFO - Outputs for Executor is: {\"transformed_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"transform_output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}}, \"artifact_type\": {\"name\": \"TransformPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\"}\n",
      "[2019-06-25 09:05:28,880] {base_executor.py:78} INFO - Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\"}\n",
      "INFO:tensorflow:Inputs to executor.Transform function: {'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt', 'preprocessing_fn': '/root/taxi/taxi_utils.py', 'examples_data_format': 'FORMAT_TF_EXAMPLE', 'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*', 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'compute_statistics': False, 'tft_statistics_use_tfdv': True}\n",
      "[2019-06-25 09:05:28,901] {executor.py:567} INFO - Inputs to executor.Transform function: {'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt', 'preprocessing_fn': '/root/taxi/taxi_utils.py', 'examples_data_format': 'FORMAT_TF_EXAMPLE', 'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*', 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'compute_statistics': False, 'tft_statistics_use_tfdv': True}\n",
      "INFO:tensorflow:Outputs to executor.Transform function: {'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples'], 'transform_output_path': '/root/taxi/data/simple/transform/transform_output/', 'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path'}\n",
      "[2019-06-25 09:05:28,908] {executor.py:569} INFO - Outputs to executor.Transform function: {'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples'], 'transform_output_path': '/root/taxi/data/simple/transform/transform_output/', 'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path'}\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_transform/mappers.py:1027: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[2019-06-25 09:05:29,060] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_transform/mappers.py:1027: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
      "[2019-06-25 09:05:29,171] {executor.py:653} INFO - Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
      "INFO:tensorflow:Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
      "[2019-06-25 09:05:29,173] {executor.py:655} INFO - Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
      "INFO:tensorflow:Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:29,177] {executor.py:657} INFO - Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n",
      "INFO:tensorflow:Transform output path: /root/taxi/data/simple/transform/transform_output/\n",
      "[2019-06-25 09:05:29,180] {executor.py:658} INFO - Transform output path: /root/taxi/data/simple/transform/transform_output/\n",
      "[2019-06-25 09:05:29,664] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "[2019-06-25 09:05:30,170] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 09:05:30,177] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "[2019-06-25 09:05:30,181] {builder_impl.py:449} INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/fe9a4f23b2a94e1d9eb115a2d3e1e612/saved_model.pb\n",
      "[2019-06-25 09:05:30,224] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/fe9a4f23b2a94e1d9eb115a2d3e1e612/saved_model.pb\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 09:05:31,910] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "[2019-06-25 09:05:31,914] {builder_impl.py:449} INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/d1a7115a05094077a7af50f9763f82a5/saved_model.pb\n",
      "[2019-06-25 09:05:31,952] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/d1a7115a05094077a7af50f9763f82a5/saved_model.pb\n",
      "[2019-06-25 09:05:36,154] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f8756bbd6a8> ====================\n",
      "[2019-06-25 09:05:36,162] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f8756bbd7b8> ====================\n",
      "[2019-06-25 09:05:36,167] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f8756bbd840> ====================\n",
      "[2019-06-25 09:05:36,181] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f8756bbd8c8> ====================\n",
      "[2019-06-25 09:05:36,186] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f8756bbd950> ====================\n",
      "[2019-06-25 09:05:36,198] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f8756bbda60> ====================\n",
      "[2019-06-25 09:05:36,205] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f8756bbdae8> ====================\n",
      "[2019-06-25 09:05:36,232] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f8756bbdb70> ====================\n",
      "[2019-06-25 09:05:36,236] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f8756bbdbf8> ====================\n",
      "[2019-06-25 09:05:36,240] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f8756bbdd90> ====================\n",
      "[2019-06-25 09:05:36,246] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f8756bbde18> ====================\n",
      "[2019-06-25 09:05:36,249] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f8756bbdea0> ====================\n",
      "[2019-06-25 09:05:36,304] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_143)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_144)+(ref_PCollection_PCollection_87/Write)))+(ref_PCollection_PCollection_86/Write)\n",
      "[2019-06-25 09:05:36,323] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadAnalysisDataset[0]/Read/Read_4)+(ref_AppliedPTransform_ReadAnalysisDataset[0]/AddKey_5))+(ref_AppliedPTransform_ReadAnalysisDataset[0]/ParseExamples_6))+((ref_AppliedPTransform_DecodeAnalysisDataset[0]/ApplyDecodeFn_8)+(FlattenAnalysisDatasets/Write/0))\n",
      "[2019-06-25 09:05:37,899] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CreateSavedModelForAnalyzerInputs[0]/CreateSavedModel/Read_13)+(ref_PCollection_PCollection_6/Write)\n",
      "[2019-06-25 09:05:37,912] {fn_api_runner.py:437} INFO - Running (((FlattenAnalysisDatasets/Read)+((((ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/BatchInputs/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_17)+((((((((((((((((((((ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/ApplySavedModel_18)+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score/mean_and_var]_19))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_1/mean_and_var]_46))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_2/mean_and_var]_73))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary/vocabulary]_100))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_1/vocabulary]_158))+((ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize/quantiles]_216)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/KeyWithVoid_219)))+(((ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_1/quantiles]_248)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/KeyWithVoid_251))+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_2/quantiles]_280))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_3/quantiles]_312))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/KeyWithVoid_49))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FlattenStringsAndMaybeWeightsLabels_102))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/KeyWithVoid_76))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/KeyWithVoid_283)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write)))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CountPerString:PairWithVoid_104)+((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write))))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write)))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/KeyWithVoid_315)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write)))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/KeyWithVoid_22)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write))))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write))+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FlattenStringsAndMaybeWeightsLabels_160)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CountPerString:PairWithVoid_162))+((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:05:38,419] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:05:39,375] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge))+((((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FilterProblematicStrings_112))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Precombine))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Write))\n",
      "[2019-06-25 09:05:39,399] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Read)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Merge))+((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/ExtractOutputs)+((((ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/SwapStringsAndCounts_121)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_125))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0)))\n",
      "[2019-06-25 09:05:39,427] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_127)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1)\n",
      "[2019-06-25 09:05:39,442] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
      "[2019-06-25 09:05:39,456] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_133))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_134)+(ref_PCollection_PCollection_83/Write))\n",
      "[2019-06-25 09:05:39,482] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/Prepare/Read_137)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/OrderElements_138))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_145)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_146)))+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write)\n",
      "[2019-06-25 09:05:39,516] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_151)+(ref_PCollection_PCollection_93/Write))\n",
      "[2019-06-25 09:05:39,535] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_86/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_152))+(ref_PCollection_PCollection_94/Write)\n",
      "[2019-06-25 09:05:39,558] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_86/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_153))+(ref_PCollection_PCollection_95/Write)\n",
      "[2019-06-25 09:05:39,580] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:05:39,684] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 09:05:39,709] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/UnKey_291)+(ref_PCollection_PCollection_181/Write))\n",
      "[2019-06-25 09:05:39,836] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/DoOnce/Read_293)+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/InjectDefault_294)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/KeyWithVoid_297))+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
      "[2019-06-25 09:05:39,914] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/UnKey_259)+(ref_PCollection_PCollection_161/Write))\n",
      "[2019-06-25 09:05:40,047] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/DoOnce/Read_261)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/InjectDefault_262))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/KeyWithVoid_265)+((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n",
      "[2019-06-25 09:05:40,132] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/UnKey_273)+(ref_PCollection_PCollection_169/Write))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:40,265] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/DoOnce/Read_275)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/InjectDefault_276)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_278)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_1/quantiles/Placeholder]_279)))+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/9)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/9)\n",
      "[2019-06-25 09:05:40,294] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/UnKey_323)+(ref_PCollection_PCollection_201/Write))\n",
      "[2019-06-25 09:05:40,431] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/DoOnce/Read_325)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/InjectDefault_326)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/KeyWithVoid_329)))+((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
      "[2019-06-25 09:05:40,517] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/UnKey_337)))+(ref_PCollection_PCollection_209/Write)\n",
      "[2019-06-25 09:05:40,643] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/DoOnce/Read_339)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/InjectDefault_340))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_342))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_3/quantiles/Placeholder]_343)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/11)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/11)))\n",
      "[2019-06-25 09:05:40,678] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/UnKey_84)))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_87)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
      "[2019-06-25 09:05:40,701] {fn_api_runner.py:437} INFO - Running ((((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/UnKey_95))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_97)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder_1]_99))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder]_98)))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/5)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/5)))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/4)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/4))\n",
      "[2019-06-25 09:05:40,740] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/UnKey_57)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_60)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n",
      "[2019-06-25 09:05:40,764] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge)+(((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/UnKey_68))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_70))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder_1]_72)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/3)))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder]_71))))+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/2))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/2))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/3)\n",
      "[2019-06-25 09:05:40,806] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_201)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_202))+(ref_PCollection_PCollection_122/Write))+(ref_PCollection_PCollection_123/Write)\n",
      "[2019-06-25 09:05:40,838] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_185)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:40,862] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs)))+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FilterProblematicStrings_170)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Precombine))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Write))\n",
      "[2019-06-25 09:05:40,889] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Read)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Merge))+((((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/SwapStringsAndCounts_179))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_183)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0)))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0))\n",
      "[2019-06-25 09:05:40,921] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
      "[2019-06-25 09:05:40,939] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_191))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_192))+(ref_PCollection_PCollection_119/Write)\n",
      "[2019-06-25 09:05:40,963] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/Prepare/Read_195)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/OrderElements_196)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_203)))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_204)+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-25 09:05:41,006] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_209))+(ref_PCollection_PCollection_129/Write)\n",
      "[2019-06-25 09:05:41,023] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_122/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_210))+(ref_PCollection_PCollection_130/Write)\n",
      "[2019-06-25 09:05:41,055] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_122/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_211)+(ref_PCollection_PCollection_131/Write))\n",
      "[2019-06-25 09:05:41,082] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:05:41,186] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 09:05:41,209] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/CreatePath/Read_213)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WaitForVocabularyFile_214))+(((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1/vocabulary/Placeholder]_215)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/7))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/7))\n",
      "[2019-06-25 09:05:41,240] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/CreatePath/Read_155)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WaitForVocabularyFile_156))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary/vocabulary/Placeholder]_157)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/6)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/6)))\n",
      "[2019-06-25 09:05:41,264] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/UnKey_227)+(ref_PCollection_PCollection_141/Write))\n",
      "[2019-06-25 09:05:41,398] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/DoOnce/Read_229)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/InjectDefault_230)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/KeyWithVoid_233)))+((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
      "[2019-06-25 09:05:41,496] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/UnKey_241)+(ref_PCollection_PCollection_149/Write)))\n",
      "[2019-06-25 09:05:41,657] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/DoOnce/Read_243)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/InjectDefault_244)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_246))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize/quantiles/Placeholder]_247)))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/8)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/8))\n",
      "[2019-06-25 09:05:41,694] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+((((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/UnKey_30)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_33)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine))))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:41,728] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+(((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/UnKey_41)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_43)))+(((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder_1]_45)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/1))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/1))))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder]_44)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/0)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/0)\n",
      "[2019-06-25 09:05:41,771] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/UnKey_305)+(ref_PCollection_PCollection_189/Write)))\n",
      "[2019-06-25 09:05:41,974] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/DoOnce/Read_307)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/InjectDefault_308))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_310)+(((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_2/quantiles/Placeholder]_311)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/10))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/10)))\n",
      "[2019-06-25 09:05:42,026] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CreateSavedModel/Flatten/Read)+(ref_PCollection_PCollection_216/Write)\n",
      "[2019-06-25 09:05:42,052] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/CreateSavedModel/Read_346)+(((((ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/BindTensors_348)+((ref_AppliedPTransform_AnalyzeDataset/ComputeDeferredMetadata_349)+(ref_AppliedPTransform_WriteTransformFn/WriteMetadata/WriteMetadata_360)))+((ref_AppliedPTransform_AnalyzeDataset/MakeCheapBarrier_350)+(ref_PCollection_PCollection_219/Write)))+(ref_PCollection_PCollection_217/Write))+(ref_PCollection_PCollection_224/Write))\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:05:42,377] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 09:05:42,422] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/2afbbd15e2da409daefc148e58f3c4a9/assets\n",
      "[2019-06-25 09:05:42,427] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/2afbbd15e2da409daefc148e58f3c4a9/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/2afbbd15e2da409daefc148e58f3c4a9/saved_model.pb\n",
      "[2019-06-25 09:05:42,473] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/2afbbd15e2da409daefc148e58f3c4a9/saved_model.pb\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:05:42,588] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:05:42,590] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:05:42,593] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:05:42,763] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/DoOnce/Read_405)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/InitializeWrite_406))+(ref_PCollection_PCollection_250/Write))+(ref_PCollection_PCollection_251/Write)\n",
      "[2019-06-25 09:05:42,787] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_ReadTransformDataset[0]/Read/Read_365)+(ref_AppliedPTransform_ReadTransformDataset[0]/AddKey_366))+(ref_AppliedPTransform_ReadTransformDataset[0]/ParseExamples_367))+(((ref_AppliedPTransform_DecodeTransformDataset[0]/ApplyDecodeFn_369)+((ref_AppliedPTransform_TransformDataset[0]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_373)+(ref_AppliedPTransform_TransformDataset[0]/Transform_374)))+(((ref_AppliedPTransform_TransformDataset[0]/ConvertAndUnbatch_375)+((ref_AppliedPTransform_TransformDataset[0]/MakeCheapBarrier_376)+(ref_PCollection_PCollection_234/Write)))+((ref_AppliedPTransform_EncodeTransformedDataset[0]_380)+((ref_AppliedPTransform_Materialize[0]/DropNoneKeys_400)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WriteBundles_407))))))+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Pair_408)+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_409)+(Materialize[0]/Write/Write/WriteImpl/GroupByKey/Write)))\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:05:42,939] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:05:42,944] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:05:42,949] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:05:46,812] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[0]/PrepareToClearSharedKeepAlives/Read_378)+(ref_AppliedPTransform_TransformDataset[0]/WaitAndClearSharedKeepAlives_379)\n",
      "[2019-06-25 09:05:46,830] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/DoOnce/Read_423)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/InitializeWrite_424)+(ref_PCollection_PCollection_263/Write)))+(ref_PCollection_PCollection_262/Write)\n",
      "[2019-06-25 09:05:46,854] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_ReadTransformDataset[1]/Read/Read_383)+((((((((ref_AppliedPTransform_ReadTransformDataset[1]/AddKey_384)+(ref_AppliedPTransform_ReadTransformDataset[1]/ParseExamples_385))+(ref_AppliedPTransform_DecodeTransformDataset[1]/ApplyDecodeFn_387))+(ref_AppliedPTransform_TransformDataset[1]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_391))+(ref_AppliedPTransform_TransformDataset[1]/Transform_392))+((((ref_AppliedPTransform_TransformDataset[1]/ConvertAndUnbatch_393)+(ref_AppliedPTransform_TransformDataset[1]/MakeCheapBarrier_394))+(ref_AppliedPTransform_EncodeTransformedDataset[1]_398))+(ref_PCollection_PCollection_245/Write)))+(ref_AppliedPTransform_Materialize[1]/DropNoneKeys_418))+(((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WriteBundles_425)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Pair_426))+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_427)+(Materialize[1]/Write/Write/WriteImpl/GroupByKey/Write))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:05:47,003] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:05:47,007] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:05:47,012] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:05:49,275] {fn_api_runner.py:437} INFO - Running (Materialize[1]/Write/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Extract_432)+(ref_PCollection_PCollection_270/Write))\n",
      "[2019-06-25 09:05:49,291] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_262/Read)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/PreFinalize_433)+(ref_PCollection_PCollection_271/Write))\n",
      "[2019-06-25 09:05:49,315] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_262/Read)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/FinalizeWrite_434)\n",
      "[2019-06-25 09:05:49,329] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:05:49,434] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 09:05:49,458] {fn_api_runner.py:437} INFO - Running (Materialize[0]/Write/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Extract_414)+(ref_PCollection_PCollection_258/Write))\n",
      "[2019-06-25 09:05:49,476] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/PreFinalize_415))+(ref_PCollection_PCollection_259/Write)\n",
      "[2019-06-25 09:05:49,498] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/FinalizeWrite_416)\n",
      "[2019-06-25 09:05:49,516] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:05:49,639] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "[2019-06-25 09:05:49,675] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_217/Read)+((ref_AppliedPTransform_WriteTransformFn/WriteTransformFn_361)+(ref_AppliedPTransform_WriteTransformFn/WaitOnWriteMetadataDone_362))\n",
      "[2019-06-25 09:05:49,699] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_WriteMetadata/Create/Read_356)+(ref_AppliedPTransform_WriteMetadata/WriteMetadata_357)\n",
      "[2019-06-25 09:05:49,725] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[1]/PrepareToClearSharedKeepAlives/Read_396)+(ref_AppliedPTransform_TransformDataset[1]/WaitAndClearSharedKeepAlives_397)\n",
      "[2019-06-25 09:05:49,745] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/PrepareToClearSharedKeepAlives/Read_352)+(ref_AppliedPTransform_AnalyzeDataset/WaitAndClearSharedKeepAlives_353)\n",
      "INFO:tensorflow:Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n",
      "[2019-06-25 09:05:49,769] {executor.py:248} INFO - Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 09:05:49,778] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"transformed_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"transform_output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}}, \"artifact_type\": {\"name\": \"TransformPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "[2019-06-25 09:05:49,784] {base_executor.py:74} INFO - Inputs for Executor is: {\"transformed_examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"transform_output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}}, \"artifact_type\": {\"name\": \"TransformPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/trainer/current/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}}, \"artifact_type\": {\"name\": \"ModelExportPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "[2019-06-25 09:05:49,790] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/trainer/current/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}}, \"artifact_type\": {\"name\": \"ModelExportPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\", \"train_args\": \"{\\n  \\\"numSteps\\\": 10000\\n}\", \"eval_args\": \"{\\n  \\\"numSteps\\\": 5000\\n}\", \"custom_config\": null}\n",
      "[2019-06-25 09:05:49,795] {base_executor.py:78} INFO - Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\", \"train_args\": \"{\\n  \\\"numSteps\\\": 10000\\n}\", \"eval_args\": \"{\\n  \\\"numSteps\\\": 5000\\n}\", \"custom_config\": null}\n",
      "INFO:tensorflow:Using config: {'_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_device_fn': None, '_protocol': None, '_num_worker_replicas': 1, '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_save_checkpoints_secs': None, '_model_dir': '/root/taxi/data/simple/trainer/current/serving_model_dir', '_task_id': 0, '_train_distribute': None, '_task_type': 'worker', '_experimental_distribute': None, '_keep_checkpoint_max': 1, '_service': None, '_save_summary_steps': 100, '_global_id_in_cluster': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f873c71ea90>, '_eval_distribute': None, '_is_chief': True, '_save_checkpoints_steps': 999, '_master': '', '_evaluation_master': ''}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:05:49,812] {estimator.py:201} INFO - Using config: {'_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_device_fn': None, '_protocol': None, '_num_worker_replicas': 1, '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_save_checkpoints_secs': None, '_model_dir': '/root/taxi/data/simple/trainer/current/serving_model_dir', '_task_id': 0, '_train_distribute': None, '_task_type': 'worker', '_experimental_distribute': None, '_keep_checkpoint_max': 1, '_service': None, '_save_summary_steps': 100, '_global_id_in_cluster': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f873c71ea90>, '_eval_distribute': None, '_is_chief': True, '_save_checkpoints_steps': 999, '_master': '', '_evaluation_master': ''}\n",
      "INFO:tensorflow:Training model.\n",
      "[2019-06-25 09:05:49,816] {executor.py:141} INFO - Training model.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "[2019-06-25 09:05:49,821] {estimator_training.py:185} INFO - Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "[2019-06-25 09:05:49,828] {training.py:610} INFO - Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 999 or save_checkpoints_secs None.\n",
      "[2019-06-25 09:05:49,832] {training.py:698} INFO - Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 999 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[2019-06-25 09:05:49,848] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-25 09:05:49,954] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[2019-06-25 09:05:49,971] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 09:05:51,909] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "[2019-06-25 09:05:51,920] {basic_session_run_hooks.py:527} INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2019-06-25 09:05:52,528] {monitored_session.py:222} INFO - Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2019-06-25 09:05:52,707] {session_manager.py:491} INFO - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2019-06-25 09:05:52,748] {session_manager.py:493} INFO - Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:05:53,831] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 0 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:loss = 27.762661, step = 1\n",
      "[2019-06-25 09:05:55,140] {basic_session_run_hooks.py:249} INFO - loss = 27.762661, step = 1\n",
      "INFO:tensorflow:global_step/sec: 141.452\n",
      "[2019-06-25 09:05:55,843] {basic_session_run_hooks.py:680} INFO - global_step/sec: 141.452\n",
      "INFO:tensorflow:loss = 22.144955, step = 101 (0.714 sec)\n",
      "[2019-06-25 09:05:55,854] {basic_session_run_hooks.py:247} INFO - loss = 22.144955, step = 101 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.941\n",
      "[2019-06-25 09:05:56,235] {basic_session_run_hooks.py:680} INFO - global_step/sec: 254.941\n",
      "INFO:tensorflow:loss = 20.124681, step = 201 (0.387 sec)\n",
      "[2019-06-25 09:05:56,241] {basic_session_run_hooks.py:247} INFO - loss = 20.124681, step = 201 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.045\n",
      "[2019-06-25 09:05:56,633] {basic_session_run_hooks.py:680} INFO - global_step/sec: 251.045\n",
      "INFO:tensorflow:loss = 19.784353, step = 301 (0.397 sec)\n",
      "[2019-06-25 09:05:56,638] {basic_session_run_hooks.py:247} INFO - loss = 19.784353, step = 301 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.902\n",
      "[2019-06-25 09:05:56,957] {basic_session_run_hooks.py:680} INFO - global_step/sec: 308.902\n",
      "INFO:tensorflow:loss = 20.57004, step = 401 (0.333 sec)\n",
      "[2019-06-25 09:05:56,972] {basic_session_run_hooks.py:247} INFO - loss = 20.57004, step = 401 (0.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.208\n",
      "[2019-06-25 09:05:57,350] {basic_session_run_hooks.py:680} INFO - global_step/sec: 254.208\n",
      "INFO:tensorflow:loss = 18.416433, step = 501 (0.396 sec)\n",
      "[2019-06-25 09:05:57,368] {basic_session_run_hooks.py:247} INFO - loss = 18.416433, step = 501 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.556\n",
      "[2019-06-25 09:05:57,740] {basic_session_run_hooks.py:680} INFO - global_step/sec: 256.556\n",
      "INFO:tensorflow:loss = 19.116257, step = 601 (0.389 sec)\n",
      "[2019-06-25 09:05:57,756] {basic_session_run_hooks.py:247} INFO - loss = 19.116257, step = 601 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.143\n",
      "[2019-06-25 09:05:58,117] {basic_session_run_hooks.py:680} INFO - global_step/sec: 265.143\n",
      "INFO:tensorflow:loss = 18.052769, step = 701 (0.373 sec)\n",
      "[2019-06-25 09:05:58,129] {basic_session_run_hooks.py:247} INFO - loss = 18.052769, step = 701 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 324.979\n",
      "[2019-06-25 09:05:58,425] {basic_session_run_hooks.py:680} INFO - global_step/sec: 324.979\n",
      "INFO:tensorflow:loss = 21.859451, step = 801 (0.310 sec)\n",
      "[2019-06-25 09:05:58,439] {basic_session_run_hooks.py:247} INFO - loss = 21.859451, step = 801 (0.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.678\n",
      "[2019-06-25 09:05:58,837] {basic_session_run_hooks.py:680} INFO - global_step/sec: 242.678\n",
      "INFO:tensorflow:loss = 13.551321, step = 901 (0.407 sec)\n",
      "[2019-06-25 09:05:58,847] {basic_session_run_hooks.py:247} INFO - loss = 13.551321, step = 901 (0.407 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 999 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:05:59,138] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 999 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[2019-06-25 09:05:59,149] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-25 09:05:59,387] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "[2019-06-25 09:06:00,627] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 09:06:00,866] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 09:06:00,892] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 09:06:00,922] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-06-25T09:06:00Z\n",
      "[2019-06-25 09:06:00,954] {evaluation.py:257} INFO - Starting evaluation at 2019-06-25T09:06:00Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2019-06-25 09:06:01,183] {monitored_session.py:222} INFO - Graph was finalized.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "[2019-06-25 09:06:01,185] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "[2019-06-25 09:06:01,194] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2019-06-25 09:06:01,315] {session_manager.py:491} INFO - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2019-06-25 09:06:01,362] {session_manager.py:493} INFO - Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [500/5000]\n",
      "[2019-06-25 09:06:04,183] {evaluation.py:169} INFO - Evaluation [500/5000]\n",
      "INFO:tensorflow:Evaluation [1000/5000]\n",
      "[2019-06-25 09:06:05,732] {evaluation.py:169} INFO - Evaluation [1000/5000]\n",
      "INFO:tensorflow:Evaluation [1500/5000]\n",
      "[2019-06-25 09:06:07,306] {evaluation.py:169} INFO - Evaluation [1500/5000]\n",
      "INFO:tensorflow:Evaluation [2000/5000]\n",
      "[2019-06-25 09:06:09,033] {evaluation.py:169} INFO - Evaluation [2000/5000]\n",
      "INFO:tensorflow:Evaluation [2500/5000]\n",
      "[2019-06-25 09:06:10,536] {evaluation.py:169} INFO - Evaluation [2500/5000]\n",
      "INFO:tensorflow:Evaluation [3000/5000]\n",
      "[2019-06-25 09:06:12,481] {evaluation.py:169} INFO - Evaluation [3000/5000]\n",
      "INFO:tensorflow:Evaluation [3500/5000]\n",
      "[2019-06-25 09:06:13,937] {evaluation.py:169} INFO - Evaluation [3500/5000]\n",
      "INFO:tensorflow:Evaluation [4000/5000]\n",
      "[2019-06-25 09:06:15,805] {evaluation.py:169} INFO - Evaluation [4000/5000]\n",
      "INFO:tensorflow:Evaluation [4500/5000]\n",
      "[2019-06-25 09:06:17,351] {evaluation.py:169} INFO - Evaluation [4500/5000]\n",
      "INFO:tensorflow:Evaluation [5000/5000]\n",
      "[2019-06-25 09:06:20,296] {evaluation.py:169} INFO - Evaluation [5000/5000]\n",
      "INFO:tensorflow:Finished evaluation at 2019-06-25-09:06:20\n",
      "[2019-06-25 09:06:20,402] {evaluation.py:277} INFO - Finished evaluation at 2019-06-25-09:06:20\n",
      "INFO:tensorflow:Saving dict for global step 999: accuracy = 0.769845, accuracy_baseline = 0.769845, auc = 0.9066664, auc_precision_recall = 0.64749587, average_loss = 0.45117003, global_step = 999, label/mean = 0.230155, loss = 18.0468, precision = 0.0, prediction/mean = 0.22743182, recall = 0.0\n",
      "[2019-06-25 09:06:20,404] {estimator.py:1979} INFO - Saving dict for global step 999: accuracy = 0.769845, accuracy_baseline = 0.769845, auc = 0.9066664, auc_precision_recall = 0.64749587, average_loss = 0.45117003, global_step = 999, label/mean = 0.230155, loss = 18.0468, precision = 0.0, prediction/mean = 0.22743182, recall = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 999: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "[2019-06-25 09:06:20,728] {estimator.py:2039} INFO - Saving 'checkpoint_path' summary for global step 999: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "INFO:tensorflow:global_step/sec: 4.56472\n",
      "[2019-06-25 09:06:20,744] {basic_session_run_hooks.py:680} INFO - global_step/sec: 4.56472\n",
      "INFO:tensorflow:loss = 20.965916, step = 1001 (21.903 sec)\n",
      "[2019-06-25 09:06:20,749] {basic_session_run_hooks.py:247} INFO - loss = 20.965916, step = 1001 (21.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.271\n",
      "[2019-06-25 09:06:21,136] {basic_session_run_hooks.py:680} INFO - global_step/sec: 255.271\n",
      "INFO:tensorflow:loss = 21.477005, step = 1101 (0.399 sec)\n",
      "[2019-06-25 09:06:21,148] {basic_session_run_hooks.py:247} INFO - loss = 21.477005, step = 1101 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.266\n",
      "[2019-06-25 09:06:21,463] {basic_session_run_hooks.py:680} INFO - global_step/sec: 306.266\n",
      "INFO:tensorflow:loss = 21.60163, step = 1201 (0.319 sec)\n",
      "[2019-06-25 09:06:21,467] {basic_session_run_hooks.py:247} INFO - loss = 21.60163, step = 1201 (0.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.922\n",
      "[2019-06-25 09:06:21,932] {basic_session_run_hooks.py:680} INFO - global_step/sec: 212.922\n",
      "INFO:tensorflow:loss = 15.628035, step = 1301 (0.474 sec)\n",
      "[2019-06-25 09:06:21,940] {basic_session_run_hooks.py:247} INFO - loss = 15.628035, step = 1301 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.04\n",
      "[2019-06-25 09:06:22,494] {basic_session_run_hooks.py:680} INFO - global_step/sec: 178.04\n",
      "INFO:tensorflow:loss = 14.740206, step = 1401 (0.556 sec)\n",
      "[2019-06-25 09:06:22,497] {basic_session_run_hooks.py:247} INFO - loss = 14.740206, step = 1401 (0.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.683\n",
      "[2019-06-25 09:06:23,047] {basic_session_run_hooks.py:680} INFO - global_step/sec: 180.683\n",
      "INFO:tensorflow:loss = 15.070787, step = 1501 (0.562 sec)\n",
      "[2019-06-25 09:06:23,059] {basic_session_run_hooks.py:247} INFO - loss = 15.070787, step = 1501 (0.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.626\n",
      "[2019-06-25 09:06:23,470] {basic_session_run_hooks.py:680} INFO - global_step/sec: 236.626\n",
      "INFO:tensorflow:loss = 21.175642, step = 1601 (0.422 sec)\n",
      "[2019-06-25 09:06:23,480] {basic_session_run_hooks.py:247} INFO - loss = 21.175642, step = 1601 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.104\n",
      "[2019-06-25 09:06:23,944] {basic_session_run_hooks.py:680} INFO - global_step/sec: 211.104\n",
      "INFO:tensorflow:loss = 17.710472, step = 1701 (0.470 sec)\n",
      "[2019-06-25 09:06:23,951] {basic_session_run_hooks.py:247} INFO - loss = 17.710472, step = 1701 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.048\n",
      "[2019-06-25 09:06:24,417] {basic_session_run_hooks.py:680} INFO - global_step/sec: 211.048\n",
      "INFO:tensorflow:loss = 13.9506645, step = 1801 (0.476 sec)\n",
      "[2019-06-25 09:06:24,427] {basic_session_run_hooks.py:247} INFO - loss = 13.9506645, step = 1801 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.182\n",
      "[2019-06-25 09:06:24,685] {basic_session_run_hooks.py:680} INFO - global_step/sec: 373.182\n",
      "INFO:tensorflow:loss = 24.545925, step = 1901 (0.264 sec)\n",
      "[2019-06-25 09:06:24,691] {basic_session_run_hooks.py:247} INFO - loss = 24.545925, step = 1901 (0.264 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1998 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:06:25,174] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 1998 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:06:25,447] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 122.035\n",
      "[2019-06-25 09:06:25,505] {basic_session_run_hooks.py:680} INFO - global_step/sec: 122.035\n",
      "INFO:tensorflow:loss = 19.81829, step = 2001 (0.824 sec)\n",
      "[2019-06-25 09:06:25,515] {basic_session_run_hooks.py:247} INFO - loss = 19.81829, step = 2001 (0.824 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:06:26,001] {basic_session_run_hooks.py:680} INFO - global_step/sec: 201.527\n",
      "INFO:tensorflow:loss = 19.746746, step = 2101 (0.505 sec)\n",
      "[2019-06-25 09:06:26,021] {basic_session_run_hooks.py:247} INFO - loss = 19.746746, step = 2101 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.646\n",
      "[2019-06-25 09:06:26,405] {basic_session_run_hooks.py:680} INFO - global_step/sec: 247.646\n",
      "INFO:tensorflow:loss = 17.767605, step = 2201 (0.390 sec)\n",
      "[2019-06-25 09:06:26,411] {basic_session_run_hooks.py:247} INFO - loss = 17.767605, step = 2201 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.692\n",
      "[2019-06-25 09:06:26,815] {basic_session_run_hooks.py:680} INFO - global_step/sec: 243.692\n",
      "INFO:tensorflow:loss = 18.158344, step = 2301 (0.422 sec)\n",
      "[2019-06-25 09:06:26,833] {basic_session_run_hooks.py:247} INFO - loss = 18.158344, step = 2301 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.366\n",
      "[2019-06-25 09:06:27,186] {basic_session_run_hooks.py:680} INFO - global_step/sec: 269.366\n",
      "INFO:tensorflow:loss = 16.973997, step = 2401 (0.357 sec)\n",
      "[2019-06-25 09:06:27,190] {basic_session_run_hooks.py:247} INFO - loss = 16.973997, step = 2401 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.302\n",
      "[2019-06-25 09:06:27,571] {basic_session_run_hooks.py:680} INFO - global_step/sec: 260.302\n",
      "INFO:tensorflow:loss = 16.600355, step = 2501 (0.391 sec)\n",
      "[2019-06-25 09:06:27,581] {basic_session_run_hooks.py:247} INFO - loss = 16.600355, step = 2501 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.655\n",
      "[2019-06-25 09:06:27,906] {basic_session_run_hooks.py:680} INFO - global_step/sec: 297.655\n",
      "INFO:tensorflow:loss = 15.830002, step = 2601 (0.332 sec)\n",
      "[2019-06-25 09:06:27,913] {basic_session_run_hooks.py:247} INFO - loss = 15.830002, step = 2601 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.628\n",
      "[2019-06-25 09:06:28,223] {basic_session_run_hooks.py:680} INFO - global_step/sec: 315.628\n",
      "INFO:tensorflow:loss = 14.6598625, step = 2701 (0.319 sec)\n",
      "[2019-06-25 09:06:28,232] {basic_session_run_hooks.py:247} INFO - loss = 14.6598625, step = 2701 (0.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.852\n",
      "[2019-06-25 09:06:28,716] {basic_session_run_hooks.py:680} INFO - global_step/sec: 202.852\n",
      "INFO:tensorflow:loss = 16.281574, step = 2801 (0.497 sec)\n",
      "[2019-06-25 09:06:28,729] {basic_session_run_hooks.py:247} INFO - loss = 16.281574, step = 2801 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.611\n",
      "[2019-06-25 09:06:29,296] {basic_session_run_hooks.py:680} INFO - global_step/sec: 172.611\n",
      "INFO:tensorflow:loss = 18.891071, step = 2901 (0.577 sec)\n",
      "[2019-06-25 09:06:29,306] {basic_session_run_hooks.py:247} INFO - loss = 18.891071, step = 2901 (0.577 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2997 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:06:29,732] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 2997 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:06:29,961] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 140.491\n",
      "[2019-06-25 09:06:30,007] {basic_session_run_hooks.py:680} INFO - global_step/sec: 140.491\n",
      "INFO:tensorflow:loss = 15.321395, step = 3001 (0.706 sec)\n",
      "[2019-06-25 09:06:30,013] {basic_session_run_hooks.py:247} INFO - loss = 15.321395, step = 3001 (0.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.661\n",
      "[2019-06-25 09:06:30,441] {basic_session_run_hooks.py:680} INFO - global_step/sec: 230.661\n",
      "INFO:tensorflow:loss = 12.695845, step = 3101 (0.437 sec)\n",
      "[2019-06-25 09:06:30,449] {basic_session_run_hooks.py:247} INFO - loss = 12.695845, step = 3101 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.732\n",
      "[2019-06-25 09:06:30,894] {basic_session_run_hooks.py:680} INFO - global_step/sec: 220.732\n",
      "INFO:tensorflow:loss = 15.277426, step = 3201 (0.449 sec)\n",
      "[2019-06-25 09:06:30,899] {basic_session_run_hooks.py:247} INFO - loss = 15.277426, step = 3201 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.523\n",
      "[2019-06-25 09:06:31,321] {basic_session_run_hooks.py:680} INFO - global_step/sec: 234.523\n",
      "INFO:tensorflow:loss = 16.945133, step = 3301 (0.430 sec)\n",
      "[2019-06-25 09:06:31,329] {basic_session_run_hooks.py:247} INFO - loss = 16.945133, step = 3301 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.723\n",
      "[2019-06-25 09:06:31,678] {basic_session_run_hooks.py:680} INFO - global_step/sec: 279.723\n",
      "INFO:tensorflow:loss = 13.902373, step = 3401 (0.357 sec)\n",
      "[2019-06-25 09:06:31,686] {basic_session_run_hooks.py:247} INFO - loss = 13.902373, step = 3401 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.254\n",
      "[2019-06-25 09:06:32,130] {basic_session_run_hooks.py:680} INFO - global_step/sec: 221.254\n",
      "INFO:tensorflow:loss = 17.202377, step = 3501 (0.469 sec)\n",
      "[2019-06-25 09:06:32,155] {basic_session_run_hooks.py:247} INFO - loss = 17.202377, step = 3501 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.34\n",
      "[2019-06-25 09:06:32,559] {basic_session_run_hooks.py:680} INFO - global_step/sec: 233.34\n",
      "INFO:tensorflow:loss = 14.010731, step = 3601 (0.422 sec)\n",
      "[2019-06-25 09:06:32,577] {basic_session_run_hooks.py:247} INFO - loss = 14.010731, step = 3601 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.67\n",
      "[2019-06-25 09:06:32,967] {basic_session_run_hooks.py:680} INFO - global_step/sec: 244.67\n",
      "INFO:tensorflow:loss = 20.942154, step = 3701 (0.402 sec)\n",
      "[2019-06-25 09:06:32,978] {basic_session_run_hooks.py:247} INFO - loss = 20.942154, step = 3701 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.794\n",
      "[2019-06-25 09:06:33,458] {basic_session_run_hooks.py:680} INFO - global_step/sec: 203.794\n",
      "INFO:tensorflow:loss = 19.120577, step = 3801 (0.489 sec)\n",
      "[2019-06-25 09:06:33,467] {basic_session_run_hooks.py:247} INFO - loss = 19.120577, step = 3801 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.451\n",
      "[2019-06-25 09:06:33,873] {basic_session_run_hooks.py:680} INFO - global_step/sec: 241.451\n",
      "INFO:tensorflow:loss = 11.231459, step = 3901 (0.415 sec)\n",
      "[2019-06-25 09:06:33,882] {basic_session_run_hooks.py:247} INFO - loss = 11.231459, step = 3901 (0.415 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3996 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:06:34,286] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 3996 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:06:34,508] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 145.927\n",
      "[2019-06-25 09:06:34,557] {basic_session_run_hooks.py:680} INFO - global_step/sec: 145.927\n",
      "INFO:tensorflow:loss = 12.3595295, step = 4001 (0.684 sec)\n",
      "[2019-06-25 09:06:34,567] {basic_session_run_hooks.py:247} INFO - loss = 12.3595295, step = 4001 (0.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.095\n",
      "[2019-06-25 09:06:34,918] {basic_session_run_hooks.py:680} INFO - global_step/sec: 277.095\n",
      "INFO:tensorflow:loss = 19.943155, step = 4101 (0.376 sec)\n",
      "[2019-06-25 09:06:34,944] {basic_session_run_hooks.py:247} INFO - loss = 19.943155, step = 4101 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.984\n",
      "[2019-06-25 09:06:35,307] {basic_session_run_hooks.py:680} INFO - global_step/sec: 256.984\n",
      "INFO:tensorflow:loss = 14.529532, step = 4201 (0.368 sec)\n",
      "[2019-06-25 09:06:35,311] {basic_session_run_hooks.py:247} INFO - loss = 14.529532, step = 4201 (0.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.792\n",
      "[2019-06-25 09:06:35,623] {basic_session_run_hooks.py:680} INFO - global_step/sec: 316.792\n",
      "INFO:tensorflow:loss = 11.127319, step = 4301 (0.317 sec)\n",
      "[2019-06-25 09:06:35,627] {basic_session_run_hooks.py:247} INFO - loss = 11.127319, step = 4301 (0.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.022\n",
      "[2019-06-25 09:06:35,950] {basic_session_run_hooks.py:680} INFO - global_step/sec: 306.022\n",
      "INFO:tensorflow:loss = 16.765217, step = 4401 (0.326 sec)\n",
      "[2019-06-25 09:06:35,954] {basic_session_run_hooks.py:247} INFO - loss = 16.765217, step = 4401 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.218\n",
      "[2019-06-25 09:06:36,289] {basic_session_run_hooks.py:680} INFO - global_step/sec: 295.218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.972572, step = 4501 (0.347 sec)\n",
      "[2019-06-25 09:06:36,301] {basic_session_run_hooks.py:247} INFO - loss = 17.972572, step = 4501 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.925\n",
      "[2019-06-25 09:06:36,684] {basic_session_run_hooks.py:680} INFO - global_step/sec: 252.925\n",
      "INFO:tensorflow:loss = 10.739395, step = 4601 (0.391 sec)\n",
      "[2019-06-25 09:06:36,692] {basic_session_run_hooks.py:247} INFO - loss = 10.739395, step = 4601 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.434\n",
      "[2019-06-25 09:06:37,051] {basic_session_run_hooks.py:680} INFO - global_step/sec: 272.434\n",
      "INFO:tensorflow:loss = 12.545689, step = 4701 (0.365 sec)\n",
      "[2019-06-25 09:06:37,057] {basic_session_run_hooks.py:247} INFO - loss = 12.545689, step = 4701 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.183\n",
      "[2019-06-25 09:06:37,409] {basic_session_run_hooks.py:680} INFO - global_step/sec: 279.183\n",
      "INFO:tensorflow:loss = 14.608418, step = 4801 (0.360 sec)\n",
      "[2019-06-25 09:06:37,417] {basic_session_run_hooks.py:247} INFO - loss = 14.608418, step = 4801 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.515\n",
      "[2019-06-25 09:06:37,772] {basic_session_run_hooks.py:680} INFO - global_step/sec: 275.515\n",
      "INFO:tensorflow:loss = 11.129419, step = 4901 (0.362 sec)\n",
      "[2019-06-25 09:06:37,779] {basic_session_run_hooks.py:247} INFO - loss = 11.129419, step = 4901 (0.362 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4995 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:06:38,058] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 4995 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:06:38,275] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 178.554\n",
      "[2019-06-25 09:06:38,332] {basic_session_run_hooks.py:680} INFO - global_step/sec: 178.554\n",
      "INFO:tensorflow:loss = 15.273818, step = 5001 (0.560 sec)\n",
      "[2019-06-25 09:06:38,340] {basic_session_run_hooks.py:247} INFO - loss = 15.273818, step = 5001 (0.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.054\n",
      "[2019-06-25 09:06:38,650] {basic_session_run_hooks.py:680} INFO - global_step/sec: 314.054\n",
      "INFO:tensorflow:loss = 11.00018, step = 5101 (0.317 sec)\n",
      "[2019-06-25 09:06:38,656] {basic_session_run_hooks.py:247} INFO - loss = 11.00018, step = 5101 (0.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.747\n",
      "[2019-06-25 09:06:39,084] {basic_session_run_hooks.py:680} INFO - global_step/sec: 230.747\n",
      "INFO:tensorflow:loss = 16.065216, step = 5201 (0.430 sec)\n",
      "[2019-06-25 09:06:39,086] {basic_session_run_hooks.py:247} INFO - loss = 16.065216, step = 5201 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.118\n",
      "[2019-06-25 09:06:39,432] {basic_session_run_hooks.py:680} INFO - global_step/sec: 287.118\n",
      "INFO:tensorflow:loss = 17.068724, step = 5301 (0.354 sec)\n",
      "[2019-06-25 09:06:39,440] {basic_session_run_hooks.py:247} INFO - loss = 17.068724, step = 5301 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.672\n",
      "[2019-06-25 09:06:39,745] {basic_session_run_hooks.py:680} INFO - global_step/sec: 319.672\n",
      "INFO:tensorflow:loss = 16.431286, step = 5401 (0.313 sec)\n",
      "[2019-06-25 09:06:39,753] {basic_session_run_hooks.py:247} INFO - loss = 16.431286, step = 5401 (0.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 302.152\n",
      "[2019-06-25 09:06:40,076] {basic_session_run_hooks.py:680} INFO - global_step/sec: 302.152\n",
      "INFO:tensorflow:loss = 11.8839655, step = 5501 (0.340 sec)\n",
      "[2019-06-25 09:06:40,093] {basic_session_run_hooks.py:247} INFO - loss = 11.8839655, step = 5501 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.909\n",
      "[2019-06-25 09:06:40,385] {basic_session_run_hooks.py:680} INFO - global_step/sec: 323.909\n",
      "INFO:tensorflow:loss = 15.441727, step = 5601 (0.299 sec)\n",
      "[2019-06-25 09:06:40,393] {basic_session_run_hooks.py:247} INFO - loss = 15.441727, step = 5601 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.212\n",
      "[2019-06-25 09:06:40,755] {basic_session_run_hooks.py:680} INFO - global_step/sec: 270.212\n",
      "INFO:tensorflow:loss = 13.933983, step = 5701 (0.367 sec)\n",
      "[2019-06-25 09:06:40,760] {basic_session_run_hooks.py:247} INFO - loss = 13.933983, step = 5701 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.205\n",
      "[2019-06-25 09:06:41,154] {basic_session_run_hooks.py:680} INFO - global_step/sec: 250.205\n",
      "INFO:tensorflow:loss = 11.88072, step = 5801 (0.402 sec)\n",
      "[2019-06-25 09:06:41,162] {basic_session_run_hooks.py:247} INFO - loss = 11.88072, step = 5801 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.22\n",
      "[2019-06-25 09:06:41,424] {basic_session_run_hooks.py:680} INFO - global_step/sec: 371.22\n",
      "INFO:tensorflow:loss = 12.57922, step = 5901 (0.278 sec)\n",
      "[2019-06-25 09:06:41,440] {basic_session_run_hooks.py:247} INFO - loss = 12.57922, step = 5901 (0.278 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5994 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:06:41,794] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 5994 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:06:42,027] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 149.297\n",
      "[2019-06-25 09:06:42,094] {basic_session_run_hooks.py:680} INFO - global_step/sec: 149.297\n",
      "INFO:tensorflow:loss = 20.165943, step = 6001 (0.668 sec)\n",
      "[2019-06-25 09:06:42,109] {basic_session_run_hooks.py:247} INFO - loss = 20.165943, step = 6001 (0.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.63\n",
      "[2019-06-25 09:06:42,356] {basic_session_run_hooks.py:680} INFO - global_step/sec: 381.63\n",
      "INFO:tensorflow:loss = 16.026855, step = 6101 (0.254 sec)\n",
      "[2019-06-25 09:06:42,363] {basic_session_run_hooks.py:247} INFO - loss = 16.026855, step = 6101 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 338.811\n",
      "[2019-06-25 09:06:42,651] {basic_session_run_hooks.py:680} INFO - global_step/sec: 338.811\n",
      "INFO:tensorflow:loss = 16.551985, step = 6201 (0.293 sec)\n",
      "[2019-06-25 09:06:42,655] {basic_session_run_hooks.py:247} INFO - loss = 16.551985, step = 6201 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.618\n",
      "[2019-06-25 09:06:42,989] {basic_session_run_hooks.py:680} INFO - global_step/sec: 295.618\n",
      "INFO:tensorflow:loss = 14.5703535, step = 6301 (0.344 sec)\n",
      "[2019-06-25 09:06:43,000] {basic_session_run_hooks.py:247} INFO - loss = 14.5703535, step = 6301 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.428\n",
      "[2019-06-25 09:06:43,333] {basic_session_run_hooks.py:680} INFO - global_step/sec: 290.428\n",
      "INFO:tensorflow:loss = 14.16337, step = 6401 (0.337 sec)\n",
      "[2019-06-25 09:06:43,337] {basic_session_run_hooks.py:247} INFO - loss = 14.16337, step = 6401 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.909\n",
      "[2019-06-25 09:06:43,679] {basic_session_run_hooks.py:680} INFO - global_step/sec: 288.909\n",
      "INFO:tensorflow:loss = 20.07727, step = 6501 (0.347 sec)\n",
      "[2019-06-25 09:06:43,684] {basic_session_run_hooks.py:247} INFO - loss = 20.07727, step = 6501 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.83\n",
      "[2019-06-25 09:06:43,921] {basic_session_run_hooks.py:680} INFO - global_step/sec: 413.83\n",
      "INFO:tensorflow:loss = 11.744825, step = 6601 (0.242 sec)\n",
      "[2019-06-25 09:06:43,926] {basic_session_run_hooks.py:247} INFO - loss = 11.744825, step = 6601 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.741\n",
      "[2019-06-25 09:06:44,259] {basic_session_run_hooks.py:680} INFO - global_step/sec: 295.741\n",
      "INFO:tensorflow:loss = 11.143435, step = 6701 (0.340 sec)\n",
      "[2019-06-25 09:06:44,265] {basic_session_run_hooks.py:247} INFO - loss = 11.143435, step = 6701 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.258\n",
      "[2019-06-25 09:06:44,535] {basic_session_run_hooks.py:680} INFO - global_step/sec: 362.258\n",
      "INFO:tensorflow:loss = 16.18207, step = 6801 (0.275 sec)\n",
      "[2019-06-25 09:06:44,541] {basic_session_run_hooks.py:247} INFO - loss = 16.18207, step = 6801 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 361.586\n",
      "[2019-06-25 09:06:44,812] {basic_session_run_hooks.py:680} INFO - global_step/sec: 361.586\n",
      "INFO:tensorflow:loss = 16.753784, step = 6901 (0.278 sec)\n",
      "[2019-06-25 09:06:44,818] {basic_session_run_hooks.py:247} INFO - loss = 16.753784, step = 6901 (0.278 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 6993 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:06:45,168] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 6993 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:06:45,976] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 81.4123\n",
      "[2019-06-25 09:06:46,040] {basic_session_run_hooks.py:680} INFO - global_step/sec: 81.4123\n",
      "INFO:tensorflow:loss = 10.752756, step = 7001 (1.230 sec)\n",
      "[2019-06-25 09:06:46,048] {basic_session_run_hooks.py:247} INFO - loss = 10.752756, step = 7001 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.38\n",
      "[2019-06-25 09:06:46,668] {basic_session_run_hooks.py:680} INFO - global_step/sec: 159.38\n",
      "INFO:tensorflow:loss = 17.880833, step = 7101 (0.627 sec)\n",
      "[2019-06-25 09:06:46,675] {basic_session_run_hooks.py:247} INFO - loss = 17.880833, step = 7101 (0.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 281.353\n",
      "[2019-06-25 09:06:47,023] {basic_session_run_hooks.py:680} INFO - global_step/sec: 281.353\n",
      "INFO:tensorflow:loss = 12.681112, step = 7201 (0.351 sec)\n",
      "[2019-06-25 09:06:47,027] {basic_session_run_hooks.py:247} INFO - loss = 12.681112, step = 7201 (0.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.461\n",
      "[2019-06-25 09:06:47,416] {basic_session_run_hooks.py:680} INFO - global_step/sec: 254.461\n",
      "INFO:tensorflow:loss = 14.149761, step = 7301 (0.399 sec)\n",
      "[2019-06-25 09:06:47,426] {basic_session_run_hooks.py:247} INFO - loss = 14.149761, step = 7301 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.811\n",
      "[2019-06-25 09:06:47,714] {basic_session_run_hooks.py:680} INFO - global_step/sec: 335.811\n",
      "INFO:tensorflow:loss = 17.475931, step = 7401 (0.299 sec)\n",
      "[2019-06-25 09:06:47,725] {basic_session_run_hooks.py:247} INFO - loss = 17.475931, step = 7401 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 326.084\n",
      "[2019-06-25 09:06:48,020] {basic_session_run_hooks.py:680} INFO - global_step/sec: 326.084\n",
      "INFO:tensorflow:loss = 11.644756, step = 7501 (0.311 sec)\n",
      "[2019-06-25 09:06:48,036] {basic_session_run_hooks.py:247} INFO - loss = 11.644756, step = 7501 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.293\n",
      "[2019-06-25 09:06:48,312] {basic_session_run_hooks.py:680} INFO - global_step/sec: 343.293\n",
      "INFO:tensorflow:loss = 13.81364, step = 7601 (0.284 sec)\n",
      "[2019-06-25 09:06:48,320] {basic_session_run_hooks.py:247} INFO - loss = 13.81364, step = 7601 (0.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.4053\n",
      "[2019-06-25 09:06:49,693] {basic_session_run_hooks.py:680} INFO - global_step/sec: 72.4053\n",
      "INFO:tensorflow:loss = 12.075325, step = 7701 (1.377 sec)\n",
      "[2019-06-25 09:06:49,697] {basic_session_run_hooks.py:247} INFO - loss = 12.075325, step = 7701 (1.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.339\n",
      "[2019-06-25 09:06:49,995] {basic_session_run_hooks.py:680} INFO - global_step/sec: 331.339\n",
      "INFO:tensorflow:loss = 16.867493, step = 7801 (0.401 sec)\n",
      "[2019-06-25 09:06:50,098] {basic_session_run_hooks.py:247} INFO - loss = 16.867493, step = 7801 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.6773\n",
      "[2019-06-25 09:06:51,074] {basic_session_run_hooks.py:680} INFO - global_step/sec: 92.6773\n",
      "INFO:tensorflow:loss = 16.055082, step = 7901 (1.010 sec)\n",
      "[2019-06-25 09:06:51,109] {basic_session_run_hooks.py:247} INFO - loss = 16.055082, step = 7901 (1.010 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7992 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:06:51,851] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 7992 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:06:52,126] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 89.1118\n",
      "[2019-06-25 09:06:52,196] {basic_session_run_hooks.py:680} INFO - global_step/sec: 89.1118\n",
      "INFO:tensorflow:loss = 11.846949, step = 8001 (1.102 sec)\n",
      "[2019-06-25 09:06:52,210] {basic_session_run_hooks.py:247} INFO - loss = 11.846949, step = 8001 (1.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.861\n",
      "[2019-06-25 09:06:52,655] {basic_session_run_hooks.py:680} INFO - global_step/sec: 217.861\n",
      "INFO:tensorflow:loss = 12.24174, step = 8101 (0.451 sec)\n",
      "[2019-06-25 09:06:52,661] {basic_session_run_hooks.py:247} INFO - loss = 12.24174, step = 8101 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.153\n",
      "[2019-06-25 09:06:52,990] {basic_session_run_hooks.py:680} INFO - global_step/sec: 298.153\n",
      "INFO:tensorflow:loss = 11.243181, step = 8201 (0.335 sec)\n",
      "[2019-06-25 09:06:52,996] {basic_session_run_hooks.py:247} INFO - loss = 11.243181, step = 8201 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.302\n",
      "[2019-06-25 09:06:53,360] {basic_session_run_hooks.py:680} INFO - global_step/sec: 270.302\n",
      "INFO:tensorflow:loss = 13.471416, step = 8301 (0.381 sec)\n",
      "[2019-06-25 09:06:53,377] {basic_session_run_hooks.py:247} INFO - loss = 13.471416, step = 8301 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.558\n",
      "[2019-06-25 09:06:53,744] {basic_session_run_hooks.py:680} INFO - global_step/sec: 260.558\n",
      "INFO:tensorflow:loss = 12.081127, step = 8401 (0.372 sec)\n",
      "[2019-06-25 09:06:53,749] {basic_session_run_hooks.py:247} INFO - loss = 12.081127, step = 8401 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 359.311\n",
      "[2019-06-25 09:06:54,023] {basic_session_run_hooks.py:680} INFO - global_step/sec: 359.311\n",
      "INFO:tensorflow:loss = 11.768514, step = 8501 (0.281 sec)\n",
      "[2019-06-25 09:06:54,030] {basic_session_run_hooks.py:247} INFO - loss = 11.768514, step = 8501 (0.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.807\n",
      "[2019-06-25 09:06:54,399] {basic_session_run_hooks.py:680} INFO - global_step/sec: 265.807\n",
      "INFO:tensorflow:loss = 14.901488, step = 8601 (0.384 sec)\n",
      "[2019-06-25 09:06:54,414] {basic_session_run_hooks.py:247} INFO - loss = 14.901488, step = 8601 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.257\n",
      "[2019-06-25 09:06:54,859] {basic_session_run_hooks.py:680} INFO - global_step/sec: 217.257\n",
      "INFO:tensorflow:loss = 16.449093, step = 8701 (0.456 sec)\n",
      "[2019-06-25 09:06:54,870] {basic_session_run_hooks.py:247} INFO - loss = 16.449093, step = 8701 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.473\n",
      "[2019-06-25 09:06:55,255] {basic_session_run_hooks.py:680} INFO - global_step/sec: 252.473\n",
      "INFO:tensorflow:loss = 16.76904, step = 8801 (0.410 sec)\n",
      "[2019-06-25 09:06:55,280] {basic_session_run_hooks.py:247} INFO - loss = 16.76904, step = 8801 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 274.555\n",
      "[2019-06-25 09:06:55,619] {basic_session_run_hooks.py:680} INFO - global_step/sec: 274.555\n",
      "INFO:tensorflow:loss = 13.205505, step = 8901 (0.349 sec)\n",
      "[2019-06-25 09:06:55,629] {basic_session_run_hooks.py:247} INFO - loss = 13.205505, step = 8901 (0.349 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8991 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:06:56,428] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 8991 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:06:56,954] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 69.1814\n",
      "[2019-06-25 09:06:57,065] {basic_session_run_hooks.py:680} INFO - global_step/sec: 69.1814\n",
      "INFO:tensorflow:loss = 13.69826, step = 9001 (1.474 sec)\n",
      "[2019-06-25 09:06:57,102] {basic_session_run_hooks.py:247} INFO - loss = 13.69826, step = 9001 (1.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.155\n",
      "[2019-06-25 09:06:57,505] {basic_session_run_hooks.py:680} INFO - global_step/sec: 227.155\n",
      "INFO:tensorflow:loss = 13.143774, step = 9101 (0.412 sec)\n",
      "[2019-06-25 09:06:57,514] {basic_session_run_hooks.py:247} INFO - loss = 13.143774, step = 9101 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.251\n",
      "[2019-06-25 09:06:58,396] {basic_session_run_hooks.py:680} INFO - global_step/sec: 112.251\n",
      "INFO:tensorflow:loss = 11.228243, step = 9201 (0.909 sec)\n",
      "[2019-06-25 09:06:58,424] {basic_session_run_hooks.py:247} INFO - loss = 11.228243, step = 9201 (0.909 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 138.873\n",
      "[2019-06-25 09:06:59,116] {basic_session_run_hooks.py:680} INFO - global_step/sec: 138.873\n",
      "INFO:tensorflow:loss = 17.7883, step = 9301 (0.719 sec)\n",
      "[2019-06-25 09:06:59,143] {basic_session_run_hooks.py:247} INFO - loss = 17.7883, step = 9301 (0.719 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.159\n",
      "[2019-06-25 09:06:59,760] {basic_session_run_hooks.py:680} INFO - global_step/sec: 155.159\n",
      "INFO:tensorflow:loss = 13.4597645, step = 9401 (0.652 sec)\n",
      "[2019-06-25 09:06:59,794] {basic_session_run_hooks.py:247} INFO - loss = 13.4597645, step = 9401 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.046\n",
      "[2019-06-25 09:07:00,284] {basic_session_run_hooks.py:680} INFO - global_step/sec: 191.046\n",
      "INFO:tensorflow:loss = 13.234793, step = 9501 (0.506 sec)\n",
      "[2019-06-25 09:07:00,300] {basic_session_run_hooks.py:247} INFO - loss = 13.234793, step = 9501 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.197\n",
      "[2019-06-25 09:07:00,674] {basic_session_run_hooks.py:680} INFO - global_step/sec: 256.197\n",
      "INFO:tensorflow:loss = 15.71859, step = 9601 (0.386 sec)\n",
      "[2019-06-25 09:07:00,686] {basic_session_run_hooks.py:247} INFO - loss = 15.71859, step = 9601 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.055\n",
      "[2019-06-25 09:07:01,094] {basic_session_run_hooks.py:680} INFO - global_step/sec: 238.055\n",
      "INFO:tensorflow:loss = 10.992365, step = 9701 (0.415 sec)\n",
      "[2019-06-25 09:07:01,101] {basic_session_run_hooks.py:247} INFO - loss = 10.992365, step = 9701 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.245\n",
      "[2019-06-25 09:07:01,542] {basic_session_run_hooks.py:680} INFO - global_step/sec: 223.245\n",
      "INFO:tensorflow:loss = 15.409775, step = 9801 (0.450 sec)\n",
      "[2019-06-25 09:07:01,551] {basic_session_run_hooks.py:247} INFO - loss = 15.409775, step = 9801 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.765\n",
      "[2019-06-25 09:07:01,911] {basic_session_run_hooks.py:680} INFO - global_step/sec: 270.765\n",
      "INFO:tensorflow:loss = 13.7661495, step = 9901 (0.363 sec)\n",
      "[2019-06-25 09:07:01,914] {basic_session_run_hooks.py:247} INFO - loss = 13.7661495, step = 9901 (0.363 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9990 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:07:02,201] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 9990 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:07:02,432] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-25 09:07:02,530] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 10000 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-25 09:07:03,086] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-25 09:07:03,294] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 09:07:05,936] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 09:07:05,998] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 09:07:06,042] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-06-25T09:07:06Z\n",
      "[2019-06-25 09:07:06,078] {evaluation.py:257} INFO - Starting evaluation at 2019-06-25T09:07:06Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2019-06-25 09:07:06,833] {monitored_session.py:222} INFO - Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-25 09:07:06,840] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2019-06-25 09:07:06,990] {session_manager.py:491} INFO - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2019-06-25 09:07:07,057] {session_manager.py:493} INFO - Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [500/5000]\n",
      "[2019-06-25 09:07:10,067] {evaluation.py:169} INFO - Evaluation [500/5000]\n",
      "INFO:tensorflow:Evaluation [1000/5000]\n",
      "[2019-06-25 09:07:11,957] {evaluation.py:169} INFO - Evaluation [1000/5000]\n",
      "INFO:tensorflow:Evaluation [1500/5000]\n",
      "[2019-06-25 09:07:14,423] {evaluation.py:169} INFO - Evaluation [1500/5000]\n",
      "INFO:tensorflow:Evaluation [2000/5000]\n",
      "[2019-06-25 09:07:16,779] {evaluation.py:169} INFO - Evaluation [2000/5000]\n",
      "INFO:tensorflow:Evaluation [2500/5000]\n",
      "[2019-06-25 09:07:18,903] {evaluation.py:169} INFO - Evaluation [2500/5000]\n",
      "INFO:tensorflow:Evaluation [3000/5000]\n",
      "[2019-06-25 09:07:20,538] {evaluation.py:169} INFO - Evaluation [3000/5000]\n",
      "INFO:tensorflow:Evaluation [3500/5000]\n",
      "[2019-06-25 09:07:22,826] {evaluation.py:169} INFO - Evaluation [3500/5000]\n",
      "INFO:tensorflow:Evaluation [4000/5000]\n",
      "[2019-06-25 09:07:24,780] {evaluation.py:169} INFO - Evaluation [4000/5000]\n",
      "INFO:tensorflow:Evaluation [4500/5000]\n",
      "[2019-06-25 09:07:26,334] {evaluation.py:169} INFO - Evaluation [4500/5000]\n",
      "INFO:tensorflow:Evaluation [5000/5000]\n",
      "[2019-06-25 09:07:28,009] {evaluation.py:169} INFO - Evaluation [5000/5000]\n",
      "INFO:tensorflow:Finished evaluation at 2019-06-25-09:07:28\n",
      "[2019-06-25 09:07:28,087] {evaluation.py:277} INFO - Finished evaluation at 2019-06-25-09:07:28\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.796995, accuracy_baseline = 0.769785, auc = 0.94612956, auc_precision_recall = 0.74298644, average_loss = 0.3347936, global_step = 10000, label/mean = 0.230215, loss = 13.391744, precision = 0.7259591, prediction/mean = 0.22746885, recall = 0.18986599\n",
      "[2019-06-25 09:07:28,098] {estimator.py:1979} INFO - Saving dict for global step 10000: accuracy = 0.796995, accuracy_baseline = 0.769785, auc = 0.94612956, auc_precision_recall = 0.74298644, average_loss = 0.3347936, global_step = 10000, label/mean = 0.230215, loss = 13.391744, precision = 0.7259591, prediction/mean = 0.22746885, recall = 0.18986599\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-25 09:07:28,107] {estimator.py:2039} INFO - Saving 'checkpoint_path' summary for global step 10000: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "[2019-06-25 09:07:28,120] {exporter.py:415} INFO - Performing the final export in the end of training.\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:07:28,238] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:07:28,245] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:07:28,251] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:07:28,261] {estimator.py:1111} INFO - Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 09:07:29,863] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\n",
      "[2019-06-25 09:07:29,869] {export.py:587} INFO - Signatures INCLUDED in export for Regress: ['regression']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "[2019-06-25 09:07:29,876] {export.py:587} INFO - Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "[2019-06-25 09:07:29,883] {export.py:587} INFO - Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "[2019-06-25 09:07:29,896] {export.py:587} INFO - Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
      "[2019-06-25 09:07:29,908] {export.py:587} INFO - Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-25 09:07:30,109] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 09:07:30,195] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561453648'/assets\n",
      "[2019-06-25 09:07:30,200] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561453648'/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561453648'/saved_model.pb\n",
      "[2019-06-25 09:07:30,438] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561453648'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 11.684275.\n",
      "[2019-06-25 09:07:30,483] {estimator.py:359} INFO - Loss for final step: 11.684275.\n",
      "INFO:tensorflow:Training complete.  Model written to /root/taxi/data/simple/trainer/current/serving_model_dir\n",
      "[2019-06-25 09:07:30,488] {executor.py:146} INFO - Training complete.  Model written to /root/taxi/data/simple/trainer/current/serving_model_dir\n",
      "INFO:tensorflow:Exporting eval_savedmodel for TFMA.\n",
      "[2019-06-25 09:07:30,492] {executor.py:149} INFO - Exporting eval_savedmodel for TFMA.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/export.py:476: export_all_saved_models (from tensorflow_estimator.contrib.estimator.python.estimator.export) is deprecated and will be removed after 2018-12-03.\n",
      "Instructions for updating:\n",
      "Use estimator.experimental_export_all_saved_models\n",
      "[2019-06-25 09:07:30,498] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/export.py:476: export_all_saved_models (from tensorflow_estimator.contrib.estimator.python.estimator.export) is deprecated and will be removed after 2018-12-03.\n",
      "Instructions for updating:\n",
      "Use estimator.experimental_export_all_saved_models\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:07:30,609] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-25 09:07:30,610] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-25 09:07:30,614] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-25 09:07:30,683] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 09:07:32,726] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-25 09:07:32,754] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-25 09:07:32,789] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "[2019-06-25 09:07:32,794] {export.py:587} INFO - Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "[2019-06-25 09:07:32,798] {export.py:587} INFO - Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n",
      "[2019-06-25 09:07:32,802] {export.py:587} INFO - Signatures INCLUDED in export for Eval: ['eval']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "[2019-06-25 09:07:32,812] {export.py:587} INFO - Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "[2019-06-25 09:07:32,817] {export.py:587} INFO - Signatures INCLUDED in export for Classify: None\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "[2019-06-25 09:07:32,827] {tf_logging.py:161} WARNING - Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-25 09:07:32,984] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-25 09:07:33,093] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561453650'/assets\n",
      "[2019-06-25 09:07:33,098] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561453650'/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561453650'/saved_model.pb\n",
      "[2019-06-25 09:07:33,525] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561453650'/saved_model.pb\n",
      "INFO:tensorflow:Exported eval_savedmodel to /root/taxi/data/simple/trainer/current/eval_model_dir.\n",
      "[2019-06-25 09:07:33,558] {executor.py:155} INFO - Exported eval_savedmodel to /root/taxi/data/simple/trainer/current/eval_model_dir.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-25 09:07:33,570] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"model_exports\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/trainer/current/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}}, \"artifact_type\": {\"name\": \"ModelExportPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:07:33,577] {base_executor.py:74} INFO - Inputs for Executor is: {\"model_exports\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/trainer/current/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}}, \"artifact_type\": {\"name\": \"ModelExportPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}], \"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelEvalPath\"}}}, \"artifact_type\": {\"name\": \"ModelEvalPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "[2019-06-25 09:07:33,583] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval_output/\", \"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelEvalPath\"}}}, \"artifact_type\": {\"name\": \"ModelEvalPath\", \"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"type_name\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"feature_slicing_spec\": \"{\\n  \\\"specs\\\": [\\n    {\\n      \\\"columnForSlicing\\\": [\\n        \\\"trip_start_hour\\\"\\n      ]\\n    }\\n  ]\\n}\"}\n",
      "[2019-06-25 09:07:33,589] {base_executor.py:78} INFO - Execution properties for Executor is: {\"feature_slicing_spec\": \"{\\n  \\\"specs\\\": [\\n    {\\n      \\\"columnForSlicing\\\": [\\n        \\\"trip_start_hour\\\"\\n      ]\\n    }\\n  ]\\n}\"}\n",
      "INFO:tensorflow:Using /root/taxi/data/simple/trainer/current/eval_model_dir/1561453650 for model eval.\n",
      "[2019-06-25 09:07:33,600] {<ipython-input-14-a3725876ff5b>:48} INFO - Using /root/taxi/data/simple/trainer/current/eval_model_dir/1561453650 for model eval.\n",
      "INFO:tensorflow:Evaluating model.\n",
      "[2019-06-25 09:07:33,623] {<ipython-input-14-a3725876ff5b>:59} INFO - Evaluating model.\n",
      "[2019-06-25 09:07:33,636] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/slicer/slicer.py:407: BeamDeprecationWarning: RemoveDuplicates is deprecated since 2.12. Use Distinct instead.\n",
      "  | 'IncrementCounter' >> beam.Map(increment_counter))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:07:35,528] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f8756bbd6a8> ====================\n",
      "[2019-06-25 09:07:35,530] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f8756bbd7b8> ====================\n",
      "[2019-06-25 09:07:35,533] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f8756bbd840> ====================\n",
      "[2019-06-25 09:07:35,537] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f8756bbd8c8> ====================\n",
      "[2019-06-25 09:07:35,541] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f8756bbd950> ====================\n",
      "[2019-06-25 09:07:35,552] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f8756bbda60> ====================\n",
      "[2019-06-25 09:07:35,554] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f8756bbdae8> ====================\n",
      "[2019-06-25 09:07:35,566] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f8756bbdb70> ====================\n",
      "[2019-06-25 09:07:35,569] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f8756bbdbf8> ====================\n",
      "[2019-06-25 09:07:35,571] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f8756bbdd90> ====================\n",
      "[2019-06-25 09:07:35,578] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f8756bbde18> ====================\n",
      "[2019-06-25 09:07:35,581] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f8756bbdea0> ====================\n",
      "[2019-06-25 09:07:35,601] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/DoOnce/Read_102)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/InitializeWrite_103)+(ref_PCollection_PCollection_55/Write)))+(ref_PCollection_PCollection_54/Write)\n",
      "[2019-06-25 09:07:35,619] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/DoOnce/Read_84)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/InitializeWrite_85)+(ref_PCollection_PCollection_44/Write)))+(ref_PCollection_PCollection_43/Write)\n",
      "[2019-06-25 09:07:35,637] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_ReadData/Read_3)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/InputsToExtracts/Map(<lambda at model_eval_lib.py:393>)_6))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/Predict/Batch/ParDo(_GlobalWindowsBatchingDoFn)_10)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/Predict/Predict_11)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/ExtractSliceKeys/ParDo(_ExtractSliceKeysFn)_13))))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/Filter/Map(filter_extracts)_16))+(((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/DoSlicing_19)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/ExtractSliceKeys_21))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Group/Write))+(((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/ToPairs_24)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Group/Write)))\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/load.py:150: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "[2019-06-25 09:07:35,770] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/load.py:150: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/eval_model_dir/1561453650/variables/variables\n",
      "[2019-06-25 09:07:36,895] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/eval_model_dir/1561453650/variables/variables\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/graph_ref.py:189: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.get_tensor_from_tensor_info or tf.compat.v1.saved_model.get_tensor_from_tensor_info.\n",
      "[2019-06-25 09:07:37,042] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/graph_ref.py:189: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.get_tensor_from_tensor_info or tf.compat.v1.saved_model.get_tensor_from_tensor_info.\n",
      "[2019-06-25 09:07:48,363] {fn_api_runner.py:437} INFO - Running ((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Group/Read)+((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Merge)+((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/ExtractOutputs)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/InterpretOutput_56))))+(((((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/ParDo(_SeparateMetricsAndPlotsFn)/ParDo(_SeparateMetricsAndPlotsFn)_58)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/SerializeMetricsAndPlots/SerializeMetrics_60))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/Map(<lambda at iobase.py:984>)_70))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/WindowInto(WindowIntoFn)_71))+(ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Write))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/SerializeMetricsAndPlots/SerializePlots_61)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/Map(<lambda at iobase.py:984>)_86)))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/WindowInto(WindowIntoFn)_87)+(ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Write)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:08:02,913] {fn_api_runner.py:437} INFO - Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/WriteBundles_92))+(ref_PCollection_PCollection_50/Write)\n",
      "[2019-06-25 09:08:03,150] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/CreateEvalConfig/Read_97)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/Map(<lambda at iobase.py:984>)_104))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/WindowInto(WindowIntoFn)_105)+(ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-25 09:08:03,178] {fn_api_runner.py:437} INFO - Running (ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/WriteBundles_110)+(ref_PCollection_PCollection_61/Write))\n",
      "[2019-06-25 09:08:03,195] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_54/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/PreFinalize_111))+(ref_PCollection_PCollection_62/Write)\n",
      "[2019-06-25 09:08:03,220] {fn_api_runner.py:437} INFO - Running (ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Group/Read)+((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Merge)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/ExtractOutputs))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Distinct_32)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/KeyWithVoid_35)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))\n",
      "[2019-06-25 09:08:03,251] {fn_api_runner.py:437} INFO - Running ((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/UnKey_43))+(ref_PCollection_PCollection_20/Write))\n",
      "[2019-06-25 09:08:03,274] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/DoOnce/Read_45)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/InjectDefault_46)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/IncrementCounter_47))\n",
      "[2019-06-25 09:08:03,307] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_43/Read)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/PreFinalize_93)+(ref_PCollection_PCollection_51/Write))\n",
      "[2019-06-25 09:08:03,329] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/DoOnce/Read_68)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/InitializeWrite_69))+(ref_PCollection_PCollection_33/Write))+(ref_PCollection_PCollection_34/Write)\n",
      "[2019-06-25 09:08:03,353] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_54/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/FinalizeWrite_112)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-25 09:08:03,370] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:08:03,475] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 09:08:03,498] {fn_api_runner.py:437} INFO - Running (ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/WriteBundles_76)+(ref_PCollection_PCollection_40/Write))\n",
      "[2019-06-25 09:08:03,524] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_33/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/PreFinalize_77))+(ref_PCollection_PCollection_41/Write)\n",
      "[2019-06-25 09:08:03,543] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_33/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/FinalizeWrite_78)\n",
      "[2019-06-25 09:08:03,563] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:08:03,667] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-25 09:08:03,692] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/FinalizeWrite_94)\n",
      "[2019-06-25 09:08:03,734] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-25 09:08:03,840] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "INFO:tensorflow:Evaluation complete. Results written to /root/taxi/data/simple/eval_output/.\n",
      "[2019-06-25 09:08:03,851] {<ipython-input-14-a3725876ff5b>:72} INFO - Evaluation complete. Results written to /root/taxi/data/simple/eval_output/.\n"
     ]
    }
   ],
   "source": [
    "pipeline = DirectRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/taxi/data/simple/:\r\n",
      "total 1.9M\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 csv_example_gen\r\n",
      "1.9M -rw-r--r-- 1 root root 1.9M Jun 25 09:05 data.csv\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:08 eval_output\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 schema_gen\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 statistics_gen\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 25 09:05 trainer\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 transform\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen/eval:\r\n",
      "total 204K\r\n",
      "204K -rw-r--r-- 1 root root 201K Jun 25 09:05 data_tfrecord-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen/train:\r\n",
      "total 408K\r\n",
      "408K -rw-r--r-- 1 root root 405K Jun 25 09:05 data_tfrecord-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/eval_output:\r\n",
      "total 34M\r\n",
      "4.0K -rw-r--r-- 1 root root  506 Jun 25 09:08 eval_config\r\n",
      " 12K -rw-r--r-- 1 root root 8.4K Jun 25 09:08 metrics\r\n",
      " 34M -rw-r--r-- 1 root root  34M Jun 25 09:08 plots\r\n",
      "\r\n",
      "/root/taxi/data/simple/schema_gen:\r\n",
      "total 8.0K\r\n",
      "8.0K -rw-r--r-- 1 root root 4.5K Jun 25 09:05 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen/eval:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 17K Jun 25 09:05 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen/train:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 18K Jun 25 09:05 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:07 current\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 25 09:07 eval_model_dir\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:07 serving_model_dir\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:07 1561453650\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650:\r\n",
      "total 864K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:07 assets\r\n",
      "856K -rw-r--r-- 1 root root 854K Jun 25 09:07 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:07 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 25 09:07 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 25 09:07 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir/1561453650/variables:\r\n",
      "total 68K\r\n",
      "4.0K -rw-r--r-- 1 root root   8 Jun 25 09:07 variables.data-00000-of-00002\r\n",
      " 60K -rw-r--r-- 1 root root 58K Jun 25 09:07 variables.data-00001-of-00002\r\n",
      "4.0K -rw-r--r-- 1 root root 995 Jun 25 09:07 variables.index\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir:\r\n",
      "total 6.1M\r\n",
      "4.0K -rw-r--r-- 1 root root   89 Jun 25 09:07 checkpoint\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:06 eval_chicago-taxi-eval\r\n",
      "3.6M -rw-r--r-- 1 root root 3.6M Jun 25 09:07 events.out.tfevents.1561453552.38545fafb33e\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 25 09:07 export\r\n",
      "1.6M -rw-r--r-- 1 root root 1.6M Jun 25 09:05 graph.pbtxt\r\n",
      "4.0K -rw-r--r-- 1 root root    8 Jun 25 09:07 model.ckpt-10000.data-00000-of-00002\r\n",
      "124K -rw-r--r-- 1 root root 124K Jun 25 09:07 model.ckpt-10000.data-00001-of-00002\r\n",
      "4.0K -rw-r--r-- 1 root root 2.1K Jun 25 09:07 model.ckpt-10000.index\r\n",
      "824K -rw-r--r-- 1 root root 821K Jun 25 09:07 model.ckpt-10000.meta\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/eval_chicago-taxi-eval:\r\n",
      "total 1.3M\r\n",
      "1.3M -rw-r--r-- 1 root root 1.3M Jun 25 09:07 events.out.tfevents.1561453580.38545fafb33e\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 25 09:07 chicago-taxi\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:07 1561453648\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1561453648:\r\n",
      "total 564K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:07 assets\r\n",
      "556K -rw-r--r-- 1 root root 554K Jun 25 09:07 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:07 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1561453648/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 25 09:07 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 25 09:07 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1561453648/variables:\r\n",
      "total 68K\r\n",
      "4.0K -rw-r--r-- 1 root root   8 Jun 25 09:07 variables.data-00000-of-00002\r\n",
      " 60K -rw-r--r-- 1 root root 58K Jun 25 09:07 variables.data-00001-of-00002\r\n",
      "4.0K -rw-r--r-- 1 root root 995 Jun 25 09:07 variables.index\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 5 root root 4.0K Jun 25 09:05 transform_output\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 transformed_examples\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output:\r\n",
      "total 12K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 metadata\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 25 09:05 transform_fn\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 transformed_metadata\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/metadata:\r\n",
      "total 4.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 916 Jun 25 09:05 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn:\r\n",
      "total 84K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 assets\r\n",
      " 76K -rw-r--r-- 1 root root  76K Jun 25 09:05 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 25 09:05 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 25 09:05 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn/variables:\r\n",
      "total 0\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transformed_metadata:\r\n",
      "total 4.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 2.2K Jun 25 09:05 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 25 09:05 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples/eval:\r\n",
      "total 176K\r\n",
      "176K -rw-r--r-- 1 root root 173K Jun 25 09:05 transformed_examples-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples/train:\r\n",
      "total 348K\r\n",
      "348K -rw-r--r-- 1 root root 348K Jun 25 09:05 transformed_examples-00000-of-00001.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -Rlhs /root/taxi/data/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_dir(model_analyzer):\n",
    "    artifact = model_analyzer.outputs.output.get()\n",
    "    return types.get_single_uri(artifact)\n",
    "    \n",
    "eval_dir = get_eval_dir(model_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "result = tfma.load_eval_result(eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfc54c62ba94b478647c00a4f2908ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'post_export_metrics/example_count'}, data=[{'metrics':…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfma.view.render_slicing_metrics(result, slicing_column='trip_start_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89988c6678fa4f3e888cc1cbc8b68787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PlotViewer(config={'metricKeys': {'calibrationPlot': {'dataSeries': 'buckets', 'metricName': 'calibrationHisto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "tfma.view.render_plot(result, tfma.slicer.SingleSliceSpec(features=[('trip_start_hour', 10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
