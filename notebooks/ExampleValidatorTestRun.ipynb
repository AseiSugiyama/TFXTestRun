{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExampleValidator Test Run\n",
    "\n",
    "## Set up\n",
    "\n",
    "TFX requires apache-airflow and docker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'apache-airflow[gcp]' docker tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use TFX version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfx\n",
    "tfx.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX requires TensorFlow >= 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX supports Python 3.5 from version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 (default, Nov 12 2018, 13:43:14) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-06-13 10:32:45--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1922668 (1.8M) [text/plain]\n",
      "Saving to: ‘/root/taxi/data/simple/data.csv’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2% 4.47M 0s\n",
      "    50K .......... .......... .......... .......... ..........  5% 8.02M 0s\n",
      "   100K .......... .......... .......... .......... ..........  7% 19.4M 0s\n",
      "   150K .......... .......... .......... .......... .......... 10% 15.7M 0s\n",
      "   200K .......... .......... .......... .......... .......... 13% 13.4M 0s\n",
      "   250K .......... .......... .......... .......... .......... 15% 18.9M 0s\n",
      "   300K .......... .......... .......... .......... .......... 18% 17.3M 0s\n",
      "   350K .......... .......... .......... .......... .......... 21% 13.1M 0s\n",
      "   400K .......... .......... .......... .......... .......... 23% 13.1M 0s\n",
      "   450K .......... .......... .......... .......... .......... 26% 21.3M 0s\n",
      "   500K .......... .......... .......... .......... .......... 29% 16.6M 0s\n",
      "   550K .......... .......... .......... .......... .......... 31% 14.6M 0s\n",
      "   600K .......... .......... .......... .......... .......... 34% 29.3M 0s\n",
      "   650K .......... .......... .......... .......... .......... 37% 4.52M 0s\n",
      "   700K .......... .......... .......... .......... .......... 39% 17.5M 0s\n",
      "   750K .......... .......... .......... .......... .......... 42% 72.1M 0s\n",
      "   800K .......... .......... .......... .......... .......... 45% 95.8M 0s\n",
      "   850K .......... .......... .......... .......... .......... 47% 45.9M 0s\n",
      "   900K .......... .......... .......... .......... .......... 50% 19.2M 0s\n",
      "   950K .......... .......... .......... .......... .......... 53% 12.4M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 55% 22.0M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 58% 40.4M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 61% 19.7M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 63% 25.3M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 66% 22.8M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 69% 31.6M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 71% 10.3M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 74% 35.4M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 77% 16.5M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 79% 20.1M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 82% 33.7M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 85% 18.3M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 87% 31.4M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 90% 11.2M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 93% 33.6M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 95% 19.9M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 98% 13.3M 0s\n",
      "  1850K .......... .......... .......                         100% 35.2M=0.1s\n",
      "\n",
      "2019-06-13 10:32:46 (16.2 MB/s) - ‘/root/taxi/data/simple/data.csv’ saved [1922668/1922668]\n",
      "\n",
      "--2019-06-13 10:32:46--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12084 (12K) [text/plain]\n",
      "Saving to: ‘/root/taxi/taxi_utils.py’\n",
      "\n",
      "     0K .......... .                                          100% 9.31M=0.001s\n",
      "\n",
      "2019-06-13 10:32:46 (9.31 MB/s) - ‘/root/taxi/taxi_utils.py’ saved [12084/12084]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# This enables you to run this notebook twice.\n",
    "# There should not be train/eval files at ~/taxi/data, since TFX can handle only single file with version 0.13.0\n",
    "if [ -e ~/taxi/data ]; then\n",
    "    rm -rf ~/taxi/data\n",
    "fi\n",
    "\n",
    "# download taxi data\n",
    "mkdir -p ~/taxi/data/simple\n",
    "mkdir -p ~/taxi/serving_model/taxi_simple\n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv -O ~/taxi/data/simple/data.csv\n",
    "\n",
    "# download \n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py -O ~/taxi/taxi_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
      "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.protobuf import json_format\n",
    "\n",
    "from tfx.components.base.base_component import ComponentOutputs\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.orchestration.tfx_runner import TfxRunner\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import csv_input\n",
    "from tfx.utils.channel import Channel\n",
    "from tfx.utils import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example assumes that the taxi data is stored in ~/taxi/data and the\n",
    "# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n",
    "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
    "_data_root = os.path.join(_taxi_root, 'data/simple')\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(_taxi_root, 'serving_model/taxi_simple')\n",
    "\n",
    "# Directory and data locations.  This example assumes all of the chicago taxi\n",
    "# example code and metadata library is relative to $HOME, but you can store\n",
    "# these files anywhere on your local filesystem.\n",
    "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines')\n",
    "_metadata_db_root = os.path.join(_tfx_root, 'metadata')\n",
    "_log_root = os.path.join(_tfx_root, 'logs')\n",
    "\n",
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "_airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2019, 1, 1),\n",
    "}\n",
    "\n",
    "# Logging overrides\n",
    "logger_overrides = {'log_root': _log_root, 'log_level': logging.INFO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implements the chicago taxi pipeline with TFX.\"\"\"\n",
    "examples = csv_input(_data_root)\n",
    "\n",
    "# Brings data into the pipeline or otherwise joins/converts training data.\n",
    "train_config = example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2)\n",
    "eval_config = example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        train_config,\n",
    "        eval_config\n",
    "    ]))\n",
    "\n",
    "# Create outputs\n",
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root, 'train/')\n",
    "\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root, 'eval/')\n",
    "\n",
    "output_dict = {'examples': Channel(\n",
    "    type_name='ExamplesPath',\n",
    "    static_artifact_collection=[train_examples, eval_examples])}\n",
    "\n",
    "example_outputs = ComponentOutputs(output_dict)\n",
    "\n",
    "example_gen = CsvExampleGen(\n",
    "    input_base=examples, # A Channel of 'ExternalPath' type, it contains path of data source.\n",
    "    output_config=output_config,  # An example_gen_pb2.Output instance, it contains train-eval split ratio.\n",
    "    outputs=example_outputs # dict from name to output channel, it will be stored example_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='train')\n",
    "train_statistics.uri = os.path.join(_data_root, 'train/stats/')\n",
    "\n",
    "eval_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='eval')\n",
    "eval_statistics.uri = os.path.join(_data_root, 'eval/stats/')\n",
    "\n",
    "output_dict = {'output': Channel(\n",
    "    type_name='ExampleStatisticsPath',\n",
    "    static_artifact_collection=[train_statistics, eval_statistics])}\n",
    "\n",
    "statistics_outputs = ComponentOutputs(output_dict)\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    input_data=example_gen.outputs.examples, # A Channel of 'ExamplesPath' type, it is equal to example_outputs\n",
    "    name='Statistics Generator', # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=statistics_outputs # dict from name to output channel, it will be stored statistics_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_schema_path = types.TfxType(type_name='SchemaPath', split='train')\n",
    "train_schema_path.uri = os.path.join(_data_root, 'train/schema/')\n",
    "\n",
    "# NOTE: SchemaGen.executor can handle JUST ONE SchemaPath.\n",
    "# Two or more SchemaPaths will cause ValueError\n",
    "# such as \"ValueError: expected list length of one but got 2\".\n",
    "schema_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='SchemaPath',\n",
    "        static_artifact_collection=[train_schema_path] \n",
    "    )\n",
    "})\n",
    "\n",
    "schema_gen = SchemaGen(\n",
    "    stats=statistics_gen.outputs.output, # A Channel of 'ExampleStatisticsPath' type, it is equal to statistics_outputs\n",
    "    name='Schema Generator',  # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=schema_outputs # dict from name to output channel, it will be stored schema_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_validator_path = types.TfxType('ExampleValidationPath', split='eval')\n",
    "eval_validator_path.uri = os.path.join(_data_root, 'eval/validation/')\n",
    "\n",
    "# NOTE: ExampleValidator.executor can handle JUST ONE ExampleValidationPath\n",
    "# since ExampleValidator should check only 'eval' data.\n",
    "# Two or more ExampleValidationPath will cause ValueError\n",
    "# such as \"ValueError: expected list length of one but got 2\".\n",
    "validator_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='ExampleValidationPath',\n",
    "        static_artifact_collection=[eval_validator_path]\n",
    "    )\n",
    "})\n",
    "\n",
    "example_validator = ExampleValidator(\n",
    "    stats=statistics_gen.outputs.output,\n",
    "    schema=schema_gen.outputs.output,\n",
    "    name='Example Validator',\n",
    "    outputs=validator_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"TFX Pipeline\",\n",
    "    pipeline_root=_pipeline_root,\n",
    "    components=[example_gen, statistics_gen, schema_gen, example_validator]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectRunner(TfxRunner):\n",
    "    \"\"\"Tfx runner on local\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self._config = config or {}\n",
    "    \n",
    "    def run(self, pipeline):\n",
    "        for component in pipeline.components:\n",
    "            self._execute_component(component)\n",
    "            \n",
    "        return pipeline\n",
    "            \n",
    "    def _execute_component(self, component):\n",
    "        input_dict = {key:value.get() for key, value in component.input_dict.items()}\n",
    "        output_dict = {key: value.get() for key, value in component.outputs.get_all().items()}\n",
    "        exec_properties = component.exec_properties\n",
    "        executor = component.executor()\n",
    "        executor.Do(input_dict, output_dict, exec_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-13 10:32:50,003] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"type_name\": {\"stringValue\": \"ExternalPath\"}, \"split\": {\"stringValue\": \"\"}}}, \"artifact_type\": {\"name\": \"ExternalPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "[2019-06-13 10:32:50,007] {base_executor.py:74} INFO - Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"type_name\": {\"stringValue\": \"ExternalPath\"}, \"split\": {\"stringValue\": \"\"}}}, \"artifact_type\": {\"name\": \"ExternalPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "[2019-06-13 10:32:50,021] {base_executor.py:76} INFO - Outputs for Executor is: {\"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"name\\\": \\\"train\\\",\\n        \\\"hashBuckets\\\": 2\\n      },\\n      {\\n        \\\"name\\\": \\\"eval\\\",\\n        \\\"hashBuckets\\\": 1\\n      }\\n    ]\\n  }\\n}\"}\n",
      "[2019-06-13 10:32:50,033] {base_executor.py:78} INFO - Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"name\\\": \\\"train\\\",\\n        \\\"hashBuckets\\\": 2\\n      },\\n      {\\n        \\\"name\\\": \\\"eval\\\",\\n        \\\"hashBuckets\\\": 1\\n      }\\n    ]\\n  }\\n}\"}\n",
      "INFO:tensorflow:Generating examples.\n",
      "[2019-06-13 10:32:50,045] {base_example_gen_executor.py:122} INFO - Generating examples.\n",
      "[2019-06-13 10:32:50,053] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-13 10:32:50,068] {executor.py:70} INFO - Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-13 10:32:50,861] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f792abe5840> ====================\n",
      "[2019-06-13 10:32:50,863] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f792abe5950> ====================\n",
      "[2019-06-13 10:32:50,866] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f792abe59d8> ====================\n",
      "[2019-06-13 10:32:50,869] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f792abe5a60> ====================\n",
      "[2019-06-13 10:32:50,870] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f792abe5ae8> ====================\n",
      "[2019-06-13 10:32:50,874] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f792abe5bf8> ====================\n",
      "[2019-06-13 10:32:50,876] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f792abe5c80> ====================\n",
      "[2019-06-13 10:32:50,885] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f792abe5d08> ====================\n",
      "[2019-06-13 10:32:50,887] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f792abe5d90> ====================\n",
      "[2019-06-13 10:32:50,889] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f792abe5f28> ====================\n",
      "[2019-06-13 10:32:50,891] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f792abe6048> ====================\n",
      "[2019-06-13 10:32:50,893] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f792abe60d0> ====================\n",
      "[2019-06-13 10:32:50,903] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/DoOnce/Read_40)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/InitializeWrite_41))+(ref_PCollection_PCollection_24/Write))+(ref_PCollection_PCollection_25/Write)\n",
      "[2019-06-13 10:32:50,921] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_InputSourceToExample/ReadFromText/Read_4)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/ParseCSVRecords_6))+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/KeyWithVoid_8))+(ref_PCollection_PCollection_2/Write))+((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Precombine)+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Write))\n",
      "[2019-06-13 10:32:52,088] {fn_api_runner.py:437} INFO - Running ((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Read)+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Merge))+(((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/UnKey_16))+(ref_PCollection_PCollection_8/Write))\n",
      "[2019-06-13 10:32:52,103] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/DoOnce/Read_18)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/InjectDefault_19))+(ref_PCollection_PCollection_10/Write)\n",
      "[2019-06-13 10:32:52,122] {fn_api_runner.py:437} INFO - Running ((((ref_PCollection_PCollection_2/Read)+(((ref_AppliedPTransform_InputSourceToExample/ParseCSV/CreateInMemoryDict_20)+(ref_AppliedPTransform_InputSourceToExample/ToTFExample_21))+(ref_AppliedPTransform_SerializeDeterministically_22)))+(((ref_AppliedPTransform_SplitData/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_25)+(ref_AppliedPTransform_ShuffleSpliteval/AddRandomKeys_53))+((ref_AppliedPTransform_ShuffleSplittrain/AddRandomKeys_27)+(ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/Map(reify_timestamps)_29))))+((ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/Map(reify_timestamps)_55)+(ShuffleSpliteval/ReshufflePerKey/GroupByKey/Write)))+(ShuffleSplittrain/ReshufflePerKey/GroupByKey/Write)\n",
      "[2019-06-13 10:32:57,307] {fn_api_runner.py:437} INFO - Running (((ShuffleSplittrain/ReshufflePerKey/GroupByKey/Read)+((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/FlatMap(restore_timestamps)_34)+((ref_AppliedPTransform_ShuffleSplittrain/RemoveRandomKeys_35)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WriteBundles_42))))+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Pair_43)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WindowInto(WindowIntoFn)_44)))+(OutputSplittrain/Write/WriteImpl/GroupByKey/Write)\n",
      "[2019-06-13 10:32:57,465] {tfrecordio.py:57} WARNING - Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "[2019-06-13 10:32:57,785] {fn_api_runner.py:437} INFO - Running (OutputSplittrain/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Extract_49)+(ref_PCollection_PCollection_32/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-13 10:32:57,800] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/PreFinalize_50))+(ref_PCollection_PCollection_33/Write)\n",
      "[2019-06-13 10:32:57,822] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/FinalizeWrite_51)\n",
      "[2019-06-13 10:32:57,835] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-13 10:32:57,941] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-13 10:32:57,964] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/DoOnce/Read_66)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/InitializeWrite_67))+(ref_PCollection_PCollection_42/Write))+(ref_PCollection_PCollection_43/Write)\n",
      "[2019-06-13 10:32:57,979] {fn_api_runner.py:437} INFO - Running ((ShuffleSpliteval/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/FlatMap(restore_timestamps)_60))+((ref_AppliedPTransform_ShuffleSpliteval/RemoveRandomKeys_61)+(((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WriteBundles_68)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Pair_69))+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WindowInto(WindowIntoFn)_70)+(OutputSpliteval/Write/WriteImpl/GroupByKey/Write))))\n",
      "[2019-06-13 10:32:58,253] {fn_api_runner.py:437} INFO - Running ((OutputSpliteval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Extract_75))+(ref_PCollection_PCollection_50/Write)\n",
      "[2019-06-13 10:32:58,265] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/PreFinalize_76))+(ref_PCollection_PCollection_51/Write)\n",
      "[2019-06-13 10:32:58,282] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/FinalizeWrite_77)\n",
      "[2019-06-13 10:32:58,296] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-13 10:32:58,401] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Examples generated.\n",
      "[2019-06-13 10:32:58,423] {base_example_gen_executor.py:145} INFO - Examples generated.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-13 10:32:58,428] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "[2019-06-13 10:32:58,433] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "[2019-06-13 10:32:58,440] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {}\n",
      "[2019-06-13 10:32:58,445] {base_executor.py:78} INFO - Execution properties for Executor is: {}\n",
      "[2019-06-13 10:32:58,453] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Generating statistics for split eval\n",
      "[2019-06-13 10:32:58,467] {executor.py:62} INFO - Generating statistics for split eval\n",
      "INFO:tensorflow:Generating statistics for split train\n",
      "[2019-06-13 10:32:59,200] {executor.py:62} INFO - Generating statistics for split train\n",
      "INFO:tensorflow:Statistics written to /root/taxi/data/simple/train/stats/.\n",
      "[2019-06-13 10:33:00,093] {executor.py:78} INFO - Statistics written to /root/taxi/data/simple/train/stats/.\n",
      "[2019-06-13 10:33:02,896] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f792abe5840> ====================\n",
      "[2019-06-13 10:33:02,900] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f792abe5950> ====================\n",
      "[2019-06-13 10:33:02,903] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f792abe59d8> ====================\n",
      "[2019-06-13 10:33:02,911] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f792abe5a60> ====================\n",
      "[2019-06-13 10:33:02,915] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f792abe5ae8> ====================\n",
      "[2019-06-13 10:33:02,924] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f792abe5bf8> ====================\n",
      "[2019-06-13 10:33:02,928] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f792abe5c80> ====================\n",
      "[2019-06-13 10:33:02,941] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f792abe5d08> ====================\n",
      "[2019-06-13 10:33:02,944] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f792abe5d90> ====================\n",
      "[2019-06-13 10:33:02,948] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f792abe5f28> ====================\n",
      "[2019-06-13 10:33:02,951] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f792abe6048> ====================\n",
      "[2019-06-13 10:33:02,954] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f792abe60d0> ====================\n",
      "[2019-06-13 10:33:02,984] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/DoOnce/Read_197)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/InitializeWrite_198)+(ref_PCollection_PCollection_120/Write)))+(ref_PCollection_PCollection_119/Write)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-13 10:33:03,005] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_ReadData.train/Read_106)+(ref_AppliedPTransform_DecodeData.train/ParseTFExamples_108))+(((((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/KeyWithVoid_111)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_115)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0)))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_135))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_116)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0)))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_136))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_138)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write))\n",
      "[2019-06-13 10:33:08,715] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_146)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_156))))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_158)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write))\n",
      "[2019-06-13 10:33:08,753] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_155))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0))\n",
      "[2019-06-13 10:33:08,775] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_166)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1)))\n",
      "[2019-06-13 10:33:08,797] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_168))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1))\n",
      "[2019-06-13 10:33:08,819] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_124)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_125)))))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1)\n",
      "[2019-06-13 10:33:09,088] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write))\n",
      "[2019-06-13 10:33:09,249] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)))\n",
      "[2019-06-13 10:33:09,583] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-13 10:33:09,597] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_177))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_180))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "[2019-06-13 10:33:09,636] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_188)+(ref_PCollection_PCollection_115/Write)))\n",
      "[2019-06-13 10:33:09,656] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_190)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_191)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_192)))+(((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/Map(<lambda at iobase.py:984>)_199)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WindowInto(WindowIntoFn)_200))+(WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-13 10:33:09,697] {fn_api_runner.py:437} INFO - Running (WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WriteBundles_205)+(ref_PCollection_PCollection_126/Write))\n",
      "[2019-06-13 10:33:09,718] {fn_api_runner.py:437} INFO - Running (((((ref_AppliedPTransform_ReadData.eval/Read_3)+(ref_AppliedPTransform_DecodeData.eval/ParseTFExamples_5))+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/KeyWithVoid_8)+((((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_12)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_13))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_32)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_33))))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_35)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0)\n",
      "[2019-06-13 10:33:12,905] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge))+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_21)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_22)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1))\n",
      "[2019-06-13 10:33:13,167] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "[2019-06-13 10:33:13,327] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge))+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0))\n",
      "[2019-06-13 10:33:13,626] {fn_api_runner.py:437} INFO - Running (((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_43)))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_53))+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_55)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-13 10:33:13,656] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_63))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1))\n",
      "[2019-06-13 10:33:13,680] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_52)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)\n",
      "[2019-06-13 10:33:13,709] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_65)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1))\n",
      "[2019-06-13 10:33:13,731] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write))\n",
      "[2019-06-13 10:33:13,744] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_74)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_77)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine)))))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "[2019-06-13 10:33:13,787] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_85))+(ref_PCollection_PCollection_51/Write))\n",
      "[2019-06-13 10:33:13,808] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_87)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_88)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_89)))+(((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/Map(<lambda at iobase.py:984>)_96)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WindowInto(WindowIntoFn)_97))+(WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-13 10:33:13,845] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/DoOnce/Read_94)+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/InitializeWrite_95)+(ref_PCollection_PCollection_56/Write)))+(ref_PCollection_PCollection_55/Write)\n",
      "[2019-06-13 10:33:13,869] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/PreFinalize_206)+(ref_PCollection_PCollection_127/Write))\n",
      "[2019-06-13 10:33:13,891] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/FinalizeWrite_207)\n",
      "[2019-06-13 10:33:13,911] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-13 10:33:14,019] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-13 10:33:14,039] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WriteBundles_102))+(ref_PCollection_PCollection_62/Write)\n",
      "[2019-06-13 10:33:14,061] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/PreFinalize_103))+(ref_PCollection_PCollection_63/Write)\n",
      "[2019-06-13 10:33:14,081] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/FinalizeWrite_104)\n",
      "[2019-06-13 10:33:14,094] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-13 10:33:14,201] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "INFO:tensorflow:Infering schema from statistics.\n",
      "[2019-06-13 10:33:14,226] {executor.py:62} INFO - Infering schema from statistics.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "[2019-06-13 10:33:14,229] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:tensorflow:Schema written to /root/taxi/data/simple/train/schema/schema.pbtxt.\n",
      "[2019-06-13 10:33:14,246] {executor.py:66} INFO - Schema written to /root/taxi/data/simple/train/schema/schema.pbtxt.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-13 10:33:14,248] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/schema/\", \"properties\": {\"type_name\": {\"stringValue\": \"SchemaPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}], \"stats\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-13 10:33:14,255] {base_executor.py:74} INFO - Inputs for Executor is: {\"schema\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/schema/\", \"properties\": {\"type_name\": {\"stringValue\": \"SchemaPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"SchemaPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}], \"stats\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/validation/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleValidationPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExampleValidationPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "[2019-06-13 10:33:14,257] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/validation/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleValidationPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExampleValidationPath\", \"properties\": {\"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"state\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {}\n",
      "[2019-06-13 10:33:14,259] {base_executor.py:78} INFO - Execution properties for Executor is: {}\n",
      "INFO:tensorflow:Validating schema against the computed statistics.\n",
      "[2019-06-13 10:33:14,262] {executor.py:58} INFO - Validating schema against the computed statistics.\n",
      "INFO:tensorflow:Validation complete. Anomalies written to /root/taxi/data/simple/eval/validation/.\n",
      "[2019-06-13 10:33:14,324] {executor.py:70} INFO - Validation complete. Anomalies written to /root/taxi/data/simple/eval/validation/.\n"
     ]
    }
   ],
   "source": [
    "pipeline = DirectRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/taxi/data/simple/:\r\n",
      "total 1.9M\r\n",
      "1.9M -rw-r--r-- 1 root root 1.9M Jun 13 10:32 data.csv\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 13 10:33 eval\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 13 10:33 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/eval:\r\n",
      "total 212K\r\n",
      "204K -rw-r--r-- 1 root root 201K Jun 13 10:32 data_tfrecord-00000-of-00001.gz\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 13 10:33 stats\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 13 10:33 validation\r\n",
      "\r\n",
      "/root/taxi/data/simple/eval/stats:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 17K Jun 13 10:33 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/eval/validation:\r\n",
      "total 8.0K\r\n",
      "8.0K -rw-r--r-- 1 root root 6.1K Jun 13 10:33 anomalies.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/train:\r\n",
      "total 416K\r\n",
      "408K -rw-r--r-- 1 root root 406K Jun 13 10:32 data_tfrecord-00000-of-00001.gz\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 13 10:33 schema\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 13 10:33 stats\r\n",
      "\r\n",
      "/root/taxi/data/simple/train/schema:\r\n",
      "total 8.0K\r\n",
      "8.0K -rw-r--r-- 1 root root 4.5K Jun 13 10:33 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/train/stats:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 18K Jun 13 10:33 stats_tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!ls -Rlhs /root/taxi/data/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline {\r\n",
      "  feature {\r\n",
      "    name: \"trip_seconds\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"pickup_latitude\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"pickup_longitude\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"trip_start_hour\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: INT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"company\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: BYTES\r\n",
      "    domain: \"company\"\r\n",
      "    presence {\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"pickup_community_area\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: INT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"dropoff_latitude\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"payment_type\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: BYTES\r\n",
      "    domain: \"payment_type\"\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"fare\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"trip_miles\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"dropoff_longitude\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"trip_start_month\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: INT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"dropoff_community_area\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"trip_start_day\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: INT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"dropoff_census_tract\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"tips\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: FLOAT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"pickup_census_tract\"\r\n",
      "    type: BYTES\r\n",
      "    presence {\r\n",
      "      min_count: 0\r\n",
      "    }\r\n",
      "  }\r\n",
      "  feature {\r\n",
      "    name: \"trip_start_timestamp\"\r\n",
      "    value_count {\r\n",
      "      min: 1\r\n",
      "      max: 1\r\n",
      "    }\r\n",
      "    type: INT\r\n",
      "    presence {\r\n",
      "      min_fraction: 1.0\r\n",
      "      min_count: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "  string_domain {\r\n",
      "    name: \"company\"\r\n",
      "    value: \"0118 - 42111 Godfrey S.Awir\"\r\n",
      "    value: \"0694 - 59280 Chinesco Trans Inc\"\r\n",
      "    value: \"2092 - 61288 Sbeih company\"\r\n",
      "    value: \"2192 - 73487 Zeymane Corp\"\r\n",
      "    value: \"2733 - 74600 Benny Jona\"\r\n",
      "    value: \"2823 - 73307 Seung Lee\"\r\n",
      "    value: \"3011 - 66308 JBL Cab Inc.\"\r\n",
      "    value: \"3094 - 24059 G.L.B. Cab Co\"\r\n",
      "    value: \"3152 - 97284 Crystal Abernathy\"\r\n",
      "    value: \"3201 - C&D Cab Co Inc\"\r\n",
      "    value: \"3201 - CID Cab Co Inc\"\r\n",
      "    value: \"3253 - 91138 Gaither Cab Co.\"\r\n",
      "    value: \"3319 - CD Cab Co\"\r\n",
      "    value: \"3385 - 23210 Eman Cab\"\r\n",
      "    value: \"3385 - Eman Cab\"\r\n",
      "    value: \"3623 - 72222 Arrington Enterprises\"\r\n",
      "    value: \"3897 - Ilie Malec\"\r\n",
      "    value: \"4053 - 40193 Adwar H. Nikola\"\r\n",
      "    value: \"4053 - Adwar H. Nikola\"\r\n",
      "    value: \"4197 - 41842 Royal Star\"\r\n",
      "    value: \"4197 - Royal Star\"\r\n",
      "    value: \"4615 - 83503 Tyrone Henderson\"\r\n",
      "    value: \"4623 - Jay Kim\"\r\n",
      "    value: \"5006 - 39261 Salifu Bawa\"\r\n",
      "    value: \"5006 - Salifu Bawa\"\r\n",
      "    value: \"5074 - 54002 Ahzmi Inc\"\r\n",
      "    value: \"5074 - Ahzmi Inc\"\r\n",
      "    value: \"5129 - 87128\"\r\n",
      "    value: \"5129 - 98755 Mengisti Taxi\"\r\n",
      "    value: \"5129 - Mengisti Taxi\"\r\n",
      "    value: \"5724 - KYVI Cab Inc\"\r\n",
      "    value: \"585 - 88805 Valley Cab Co\"\r\n",
      "    value: \"5864 - 73614 Thomas Owusu\"\r\n",
      "    value: \"5864 - Thomas Owusu\"\r\n",
      "    value: \"5874 - 73628 Sergey Cab Corp.\"\r\n",
      "    value: \"5997 - 65283 AW Services Inc.\"\r\n",
      "    value: \"6488 - 83287 Zuha Taxi\"\r\n",
      "    value: \"6742 - 83735 Tasha ride inc\"\r\n",
      "    value: \"Blue Ribbon Taxi Association Inc.\"\r\n",
      "    value: \"C & D Cab Co Inc\"\r\n",
      "    value: \"Chicago Elite Cab Corp.\"\r\n",
      "    value: \"Chicago Elite Cab Corp. (Chicago Carriag\"\r\n",
      "    value: \"Chicago Medallion Leasing INC\"\r\n",
      "    value: \"Chicago Medallion Management\"\r\n",
      "    value: \"Choice Taxi Association\"\r\n",
      "    value: \"Dispatch Taxi Affiliation\"\r\n",
      "    value: \"KOAM Taxi Association\"\r\n",
      "    value: \"Northwest Management LLC\"\r\n",
      "    value: \"Taxi Affiliation Services\"\r\n",
      "    value: \"Top Cab Affiliation\"\r\n",
      "  }\r\n",
      "  string_domain {\r\n",
      "    name: \"payment_type\"\r\n",
      "    value: \"Cash\"\r\n",
      "    value: \"Credit Card\"\r\n",
      "    value: \"Dispute\"\r\n",
      "    value: \"No Charge\"\r\n",
      "    value: \"Pcard\"\r\n",
      "    value: \"Prcard\"\r\n",
      "    value: \"Unknown\"\r\n",
      "  }\r\n",
      "}\r\n",
      "anomaly_info {\r\n",
      "  key: \"company\"\r\n",
      "  value {\r\n",
      "    description: \"Examples contain values missing from the schema: 1085 - 72312 N and W Cab Co (<1%), 2192 - Zeymane Corp (<1%), 2809 - 95474 C & D Cab Co Inc. (<1%), 3897 - 57856 Ilie Malec (<1%), 4615 - Tyrone Henderson (<1%), 585 - Valley Cab Co (<1%), 5874 - Sergey Cab Corp. (<1%), 5997 - AW Services Inc. (<1%), 6057 - 24657 Richard Addo (<1%), 6574 - Babylon Express Inc. (<1%), 6743 - Luhak Corp (<1%). \"\r\n",
      "    severity: ERROR\r\n",
      "    short_description: \"Unexpected string values\"\r\n",
      "    reason {\r\n",
      "      type: ENUM_TYPE_UNEXPECTED_STRING_VALUES\r\n",
      "      short_description: \"Unexpected string values\"\r\n",
      "      description: \"Examples contain values missing from the schema: 1085 - 72312 N and W Cab Co (<1%), 2192 - Zeymane Corp (<1%), 2809 - 95474 C & D Cab Co Inc. (<1%), 3897 - 57856 Ilie Malec (<1%), 4615 - Tyrone Henderson (<1%), 585 - Valley Cab Co (<1%), 5874 - Sergey Cab Corp. (<1%), 5997 - AW Services Inc. (<1%), 6057 - 24657 Richard Addo (<1%), 6574 - Babylon Express Inc. (<1%), 6743 - Luhak Corp (<1%). \"\r\n",
      "    }\r\n",
      "    path {\r\n",
      "      step: \"company\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "anomaly_name_format: SERIALIZED_PATH\r\n"
     ]
    }
   ],
   "source": [
    "!cat /root/taxi/data/simple/eval/validation/anomalies.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`anomalies.pbtxt` is only simple text file. To visualize it, we are going to;\n",
    "\n",
    "1. get the path of `anomalies.pbtxt`from `example_validator`\n",
    "2. parse `anomalies.pbtxt` into anomalies (protobuf)\n",
    "3. visualize schema with [tfdv](https://www.tensorflow.org/tfx/data_validation/get_started)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the path of `schema.pbtxt`\n",
    "def get_validation_directory(example_validator):\n",
    "    output_dict = {key: value.get() for key, value in example_validator.outputs.get_all().items()}\n",
    "    directory = types.get_split_uri(output_dict['output'], 'eval')\n",
    "    return directory\n",
    "\n",
    "validation_directory = get_validation_directory(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. parse schema.pbtxt\n",
    "\n",
    "from pathlib import Path\n",
    "from google.protobuf.text_format import Parse\n",
    "from google.protobuf.message import Message\n",
    "from google.protobuf import text_format\n",
    "from tensorflow_metadata.proto.v0 import anomalies_pb2\n",
    "\n",
    "def parse_anomalies_proto_string(validation_directory):\n",
    "    path = Path(validation_directory)\n",
    "    for filepath in path.glob('*'):\n",
    "        with open(str(filepath), 'r') as file: # since we are using python 3.5, not 3.6+\n",
    "            anomalies_proto_string = file.read()\n",
    "            anomalies = anomalies_pb2.Anomalies()\n",
    "            text_format.Parse(anomalies_proto_string, anomalies)\n",
    "            return anomalies\n",
    "\n",
    "anomalies = parse_anomalies_proto_string(validation_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'company'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: 1085 - 72312 N and W Cab Co (&lt;1%), 2192 - Zeymane Corp (&lt;1%), 2809 - 95474 C &amp; D Cab Co Inc. (&lt;1%), 3897 - 57856 Ilie Malec (&lt;1%), 4615 - Tyrone Henderson (&lt;1%), 585 - Valley Cab Co (&lt;1%), 5874 - Sergey Cab Corp. (&lt;1%), 5997 - AW Services Inc. (&lt;1%), 6057 - 24657 Richard Addo (&lt;1%), 6574 - Babylon Express Inc. (&lt;1%), 6743 - Luhak Corp (&lt;1%).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Anomaly short description  \\\n",
       "Feature name                             \n",
       "'company'     Unexpected string values   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                               Anomaly long description  \n",
       "Feature name                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "'company'     Examples contain values missing from the schema: 1085 - 72312 N and W Cab Co (<1%), 2192 - Zeymane Corp (<1%), 2809 - 95474 C & D Cab Co Inc. (<1%), 3897 - 57856 Ilie Malec (<1%), 4615 - Tyrone Henderson (<1%), 585 - Valley Cab Co (<1%), 5874 - Sergey Cab Corp. (<1%), 5997 - AW Services Inc. (<1%), 6057 - 24657 Richard Addo (<1%), 6574 - Babylon Express Inc. (<1%), 6743 - Luhak Corp (<1%).   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. visualize anomalies with tfdv\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
