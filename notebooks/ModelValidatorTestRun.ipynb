{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Test Run\n",
    "\n",
    "## Set up\n",
    "\n",
    "TFX requires apache-airflow and docker SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-airflow[gcp] in /usr/local/lib/python3.5/dist-packages (1.10.3)\n",
      "\u001b[33m  WARNING: apache-airflow 1.10.3 does not provide the extra 'gcp'\u001b[0m\n",
      "Requirement already satisfied: docker in /usr/local/lib/python3.5/dist-packages (4.0.1)\n",
      "Requirement already satisfied: tfx in /usr/local/lib/python3.5/dist-packages (0.13.0)\n",
      "Requirement already satisfied: flask-appbuilder==1.12.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.12.3)\n",
      "Requirement already satisfied: tenacity==4.12.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.12.0)\n",
      "Requirement already satisfied: funcsigs==1.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.0)\n",
      "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.8.0)\n",
      "Requirement already satisfied: tabulate<0.9,>=0.7.5 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.8.3)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.22.0)\n",
      "Requirement already satisfied: jinja2<=2.10.0,>=2.7.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.10)\n",
      "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: flask-login<0.5,>=0.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.4.1)\n",
      "Requirement already satisfied: setproctitle<2,>=1.1.8 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.1.10)\n",
      "Requirement already satisfied: werkzeug<0.15.0,>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: flask-wtf<0.15,>=0.14.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.2)\n",
      "Requirement already satisfied: python-daemon<2.2,>=2.1.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.2)\n",
      "Requirement already satisfied: zope.deprecation<5.0,>=4.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.4.0)\n",
      "Requirement already satisfied: pendulum==1.4.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.4.4)\n",
      "Requirement already satisfied: sqlalchemy<1.3.0,>=1.1.15 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2.19)\n",
      "Requirement already satisfied: configparser<3.6.0,>=3.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (3.5.3)\n",
      "Requirement already satisfied: pygments<3.0,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.3.1)\n",
      "Requirement already satisfied: croniter<0.4,>=0.3.17 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.3.30)\n",
      "Requirement already satisfied: iso8601>=0.1.12 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.1.12)\n",
      "Requirement already satisfied: lxml>=4.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.3.4)\n",
      "Requirement already satisfied: thrift>=0.9.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.11.0)\n",
      "Requirement already satisfied: alembic<1.0,>=0.9 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.9.10)\n",
      "Requirement already satisfied: dill<0.3,>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.9)\n",
      "Requirement already satisfied: future<0.17,>=0.16.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.16.0)\n",
      "Requirement already satisfied: gunicorn<20.0,>=19.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (19.9.0)\n",
      "Requirement already satisfied: tzlocal>=1.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.1)\n",
      "Requirement already satisfied: pandas<1.0.0,>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.24.2)\n",
      "Requirement already satisfied: json-merge-patch==0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2)\n",
      "Requirement already satisfied: psutil<6.0.0,>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (5.6.3)\n",
      "Requirement already satisfied: flask<2.0,>=1.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.3)\n",
      "Requirement already satisfied: flask-swagger==0.2.13 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.13)\n",
      "Requirement already satisfied: flask-admin==1.5.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.3)\n",
      "Requirement already satisfied: gitpython>=2.0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.11)\n",
      "Requirement already satisfied: flask-caching<1.4.0,>=1.3.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.3.3)\n",
      "Requirement already satisfied: markdown<3.0,>=2.5.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.6.11)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.5/dist-packages (from docker) (1.12.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.5/dist-packages (from docker) (0.56.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.7 in /usr/local/lib/python3.5/dist-packages (from tfx) (3.7.1)\n",
      "Requirement already satisfied: ml-metadata<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: absl-py<1,>=0.1.6 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.7.1)\n",
      "Requirement already satisfied: tensorflow-transform<0.14,>=0.13 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.0)\n",
      "Requirement already satisfied: tensorflow-data-validation<0.14,>=0.13.1 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.1)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.5/dist-packages (from tfx) (1.7.9)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.12 in /usr/local/lib/python3.5/dist-packages (from tfx) (2.13.0)\n",
      "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.4.0)\n",
      "Requirement already satisfied: Flask-OpenID<2,>=1.2.5 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (1.2.5)\n",
      "Requirement already satisfied: click<8,>=6.7 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (7.0)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.4.1)\n",
      "Requirement already satisfied: Flask-Babel<1,>=0.11.1 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: ordereddict in /usr/local/lib/python3.5/dist-packages (from funcsigs==1.0.0->apache-airflow[gcp]) (1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (1.25.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.5/dist-packages (from jinja2<=2.10.0,>=2.7.3->apache-airflow[gcp]) (1.1.1)\n",
      "Requirement already satisfied: WTForms in /usr/local/lib/python3.5/dist-packages (from flask-wtf<0.15,>=0.14.2->apache-airflow[gcp]) (2.2.1)\n",
      "Requirement already satisfied: lockfile>=0.10 in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: docutils in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.14)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (41.0.1)\n",
      "Requirement already satisfied: pytzdata>=2018.3.0.0 in /usr/local/lib/python3.5/dist-packages (from pendulum==1.4.4->apache-airflow[gcp]) (2019.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.4)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz in /usr/local/lib/python3.5/dist-packages (from tzlocal>=1.4->apache-airflow[gcp]) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas<1.0.0,>=0.17.1->apache-airflow[gcp]) (1.16.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.5/dist-packages (from flask<2.0,>=1.0->apache-airflow[gcp]) (1.1.0)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.5/dist-packages (from flask-swagger==0.2.13->apache-airflow[gcp]) (3.13)\n",
      "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: pydot<1.3,>=1.2.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-transform<0.14,>=0.13->tfx) (1.2.4)\n",
      "Requirement already satisfied: tensorflow-metadata<0.14,>=0.12.1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-transform<0.14,>=0.13->tfx) (0.13.0)\n",
      "Requirement already satisfied: joblib<1,>=0.12 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.2)\n",
      "Requirement already satisfied: IPython>=5.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (7.5.0)\n",
      "Requirement already satisfied: scikit-learn<1,>=0.18 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.21.2)\n",
      "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.0.0)\n",
      "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.1.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (7.4.2)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.6.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.12.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.20.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.5.4)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.0.0)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.0.0)\n",
      "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.21.24)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7)\n",
      "Requirement already satisfied: pyarrow<0.14.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.13.0)\n",
      "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.9.0)\n",
      "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.1.1)\n",
      "Requirement already satisfied: google-cloud-core<0.30.0,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.29.1)\n",
      "Requirement already satisfied: google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7.4)\n",
      "Requirement already satisfied: google-cloud-bigtable<0.33.0,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.32.2)\n",
      "Requirement already satisfied: google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.6.1)\n",
      "Requirement already satisfied: google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.39.1)\n",
      "Requirement already satisfied: google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.5.28)\n",
      "Requirement already satisfied: python3-openid>=2.0 in /usr/local/lib/python3.5/dist-packages (from Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (3.1.0)\n",
      "Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.5/dist-packages (from Flask-Babel<1,>=0.11.1->flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.7.0)\n",
      "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitdb2>=2.0.0->gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot<1.3,>=1.2.0->tensorflow-transform<0.14,>=0.13->tfx) (2.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.5/dist-packages (from tensorflow-metadata<0.14,>=0.12.1->tensorflow-transform<0.14,>=0.13->tfx) (1.6.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.4.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.3.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (2.0.9)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.7.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.0)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.0)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.3)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.1.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.5.0)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.7.8)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.4.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (0.2.5)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (4.0)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.5/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.12->tfx) (0.6.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (0.4.5)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.5/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.12->tfx) (5.2.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.0.0 in /usr/local/lib/python3.5/dist-packages (from google-cloud-core<0.30.0,>=0.28.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.11.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.12dev,>=0.11.4 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigtable<0.33.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.11.4)\n",
      "Requirement already satisfied: google-resumable-media>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.3.2)\n",
      "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.5/dist-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.5/dist-packages (from python3-openid>=2.0->Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from jedi>=0.10->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.4.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.2->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.5/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.6.0)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.5/dist-packages (from jupyter-console->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.2.4)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.5/dist-packages (from qtconsole->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.5/dist-packages (from ipykernel->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.4.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.3)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.4)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.4.2)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (18.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.2)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.6.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.0.1)\n",
      "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.5/dist-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.5)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.5/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.5.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.15.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (19.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'apache-airflow[gcp]' docker tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use TFX version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tfx\n",
    "tfx.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX requires TensorFlow >= 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX supports Python 3.5 from version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 (default, Nov 12 2018, 13:43:14) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-06-20 08:31:20--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1922668 (1.8M) [text/plain]\n",
      "Saving to: ‘/root/taxi/data/simple/data.csv’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2% 2.19M 1s\n",
      "    50K .......... .......... .......... .......... ..........  5% 5.52M 1s\n",
      "   100K .......... .......... .......... .......... ..........  7% 8.17M 0s\n",
      "   150K .......... .......... .......... .......... .......... 10% 5.29M 0s\n",
      "   200K .......... .......... .......... .......... .......... 13% 7.05M 0s\n",
      "   250K .......... .......... .......... .......... .......... 15% 3.62M 0s\n",
      "   300K .......... .......... .......... .......... .......... 18% 5.18M 0s\n",
      "   350K .......... .......... .......... .......... .......... 21% 5.94M 0s\n",
      "   400K .......... .......... .......... .......... .......... 23% 14.2M 0s\n",
      "   450K .......... .......... .......... .......... .......... 26% 10.7M 0s\n",
      "   500K .......... .......... .......... .......... .......... 29% 13.3M 0s\n",
      "   550K .......... .......... .......... .......... .......... 31% 4.98M 0s\n",
      "   600K .......... .......... .......... .......... .......... 34% 10.1M 0s\n",
      "   650K .......... .......... .......... .......... .......... 37% 6.20M 0s\n",
      "   700K .......... .......... .......... .......... .......... 39% 5.01M 0s\n",
      "   750K .......... .......... .......... .......... .......... 42% 6.63M 0s\n",
      "   800K .......... .......... .......... .......... .......... 45% 11.2M 0s\n",
      "   850K .......... .......... .......... .......... .......... 47% 20.2M 0s\n",
      "   900K .......... .......... .......... .......... .......... 50% 27.1M 0s\n",
      "   950K .......... .......... .......... .......... .......... 53% 17.5M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 55% 14.0M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 58% 14.2M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 61% 3.82M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 63% 6.56M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 66% 11.9M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 69% 12.5M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 71% 15.5M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 74% 21.1M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 77% 30.7M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 79% 30.2M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 82% 22.0M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 85% 30.2M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 87% 26.0M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 90% 46.5M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 93% 34.4M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 95% 21.8M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 98% 60.4M 0s\n",
      "  1850K .......... .......... .......                         100% 25.8M=0.2s\n",
      "\n",
      "2019-06-20 08:31:21 (9.03 MB/s) - ‘/root/taxi/data/simple/data.csv’ saved [1922668/1922668]\n",
      "\n",
      "--2019-06-20 08:31:21--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12084 (12K) [text/plain]\n",
      "Saving to: ‘/root/taxi/taxi_utils.py’\n",
      "\n",
      "     0K .......... .                                          100% 2.04M=0.006s\n",
      "\n",
      "2019-06-20 08:31:22 (2.04 MB/s) - ‘/root/taxi/taxi_utils.py’ saved [12084/12084]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# This enables you to run this notebook twice.\n",
    "# There should not be train/eval files at ~/taxi/data, since TFX can handle only single file with version 0.13.0\n",
    "if [ -e ~/taxi/data ]; then\n",
    "    rm -rf ~/taxi/data\n",
    "fi\n",
    "\n",
    "# download taxi data\n",
    "mkdir -p ~/taxi/data/simple\n",
    "mkdir -p ~/taxi/serving_model/taxi_simple\n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv -O ~/taxi/data/simple/data.csv\n",
    "\n",
    "# download \n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py -O ~/taxi/taxi_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
      "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.protobuf import json_format\n",
    "\n",
    "from tfx.components.base.base_component import ComponentOutputs\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.orchestration.tfx_runner import TfxRunner\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import csv_input\n",
    "from tfx.utils.channel import Channel\n",
    "from tfx.utils import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This example assumes that the taxi data is stored in ~/taxi/data and the\n",
    "# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n",
    "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
    "_data_root = os.path.join(_taxi_root, 'data/simple')\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')\n",
    "\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(_taxi_root, 'serving_model/taxi_simple')\n",
    "\n",
    "# Directory and data locations.  This example assumes all of the chicago taxi\n",
    "# example code and metadata library is relative to $HOME, but you can store\n",
    "# these files anywhere on your local filesystem.\n",
    "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines')\n",
    "_metadata_db_root = os.path.join(_tfx_root, 'metadata')\n",
    "_log_root = os.path.join(_tfx_root, 'logs')\n",
    "\n",
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "_airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2019, 1, 1),\n",
    "}\n",
    "\n",
    "# Logging overrides\n",
    "logger_overrides = {'log_root': _log_root, 'log_level': logging.INFO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Implements the chicago taxi pipeline with TFX.\"\"\"\n",
    "examples = csv_input(_data_root)\n",
    "\n",
    "# Brings data into the pipeline or otherwise joins/converts training data.\n",
    "train_config = example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2)\n",
    "eval_config = example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        train_config,\n",
    "        eval_config\n",
    "    ]))\n",
    "\n",
    "# Create outputs\n",
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root, 'csv_example_gen/train/')\n",
    "\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root, 'csv_example_gen/eval/')\n",
    "\n",
    "example_outputs = ComponentOutputs({\n",
    "    'examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    ),\n",
    "    'training_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples]\n",
    "    ),\n",
    "    'eval_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[eval_examples]\n",
    "    ),    \n",
    "})\n",
    "\n",
    "example_gen = CsvExampleGen(\n",
    "    input_base=examples, # A Channel of 'ExternalPath' type, it contains path of data source.\n",
    "    output_config=output_config,  # An example_gen_pb2.Output instance, it contains train-eval split ratio.\n",
    "    outputs=example_outputs # dict from name to output channel, it will be stored example_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='train')\n",
    "train_statistics.uri = os.path.join(_data_root, 'statistics_gen/train/')\n",
    "\n",
    "eval_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='eval')\n",
    "eval_statistics.uri = os.path.join(_data_root, 'statistics_gen/eval/')\n",
    "\n",
    "statistics_outputs = ComponentOutputs({\n",
    "    'output': Channel(\n",
    "        type_name='ExampleStatisticsPath',\n",
    "        static_artifact_collection=[train_statistics, eval_statistics]\n",
    "    )\n",
    "})\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    input_data=example_gen.outputs.examples, # A Channel of 'ExamplesPath' type, it is equal to example_outputs\n",
    "    name='Statistics Generator', # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=statistics_outputs # dict from name to output channel, it will be stored statistics_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_schema_path = types.TfxType(type_name='SchemaPath', split='train')\n",
    "train_schema_path.uri = os.path.join(_data_root, 'schema_gen/')\n",
    "\n",
    "# NOTE: SchemaGen.executor can handle JUST ONE SchemaPath.\n",
    "# Two or more SchemaPaths will cause ValueError\n",
    "# such as \"ValueError: expected list length of one but got 2\".\n",
    "schema_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='SchemaPath',\n",
    "        static_artifact_collection=[train_schema_path] \n",
    "    )\n",
    "})\n",
    "\n",
    "infer_schema = SchemaGen(\n",
    "    stats=statistics_gen.outputs.output, # A Channel of 'ExampleStatisticsPath' type, it is equal to statistics_outputs\n",
    "    name='Schema Generator',  # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=schema_outputs # dict from name to output channel, it will be stored schema_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root,\n",
    "                                  'transform/transformed_examples/train/')\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root,\n",
    "                                 'transform/transformed_examples/eval/')\n",
    "transform_output = types.TfxType(type_name='TransformPath')\n",
    "transform_output.uri = os.path.join(_data_root,\n",
    "                                    'transform/transform_output/')\n",
    "\n",
    "transform_outputs = ComponentOutputs({\n",
    "    # Output of 'tf.Transform', which includes an exported \n",
    "    # Tensorflow graph suitable for both training and serving\n",
    "    'transform_output':Channel(\n",
    "        type_name='TransformPath',\n",
    "        static_artifact_collection=[transform_output]\n",
    "    ),\n",
    "    # transformed_examples: Materialized transformed examples, which includes \n",
    "    # both 'train' and 'eval' splits.\n",
    "    'transformed_examples':Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    )\n",
    "})\n",
    "\n",
    "transform = Transform(\n",
    "    input_data=example_gen.outputs.examples,\n",
    "    schema=infer_schema.outputs.output,\n",
    "    module_file=_taxi_module_file,\n",
    "    outputs=transform_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_exports = types.TfxType(type_name='ModelExportPath')\n",
    "model_exports.uri = os.path.join(_data_root, 'trainer/current/')\n",
    "\n",
    "trainer_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='ModelExportPath',\n",
    "        static_artifact_collection=[model_exports]\n",
    "    )\n",
    "})\n",
    "\n",
    "trainer = Trainer(\n",
    "    module_file=_taxi_module_file,\n",
    "    transformed_examples=transform.outputs.transformed_examples,\n",
    "    schema=infer_schema.outputs.output,\n",
    "    transform_output=transform.outputs.transform_output,\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=5000),\n",
    "    outputs=trainer_outputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_output = types.TfxType('ModelEvalPath')\n",
    "eval_output.uri = os.path.join(_data_root, 'eval_output/')\n",
    "\n",
    "model_analyzer_outputs = ComponentOutputs({\n",
    "    'output':\n",
    "    Channel(\n",
    "        type_name='ModelEvalPath',\n",
    "        static_artifact_collection=[eval_output]),\n",
    "})\n",
    "\n",
    "feature_slicing_spec = evaluator_pb2.FeatureSlicingSpec(specs=[\n",
    "    evaluator_pb2.SingleSlicingSpec(\n",
    "        column_for_slicing=['trip_start_hour'])\n",
    "])\n",
    "\n",
    "model_analyzer = Evaluator(\n",
    "    examples=example_gen.outputs.examples,\n",
    "    model_exports=trainer.outputs.output,\n",
    "    feature_slicing_spec=feature_slicing_spec,\n",
    "    outputs=model_analyzer_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blessing = types.TfxType(type_name='ModelBlessingPath')\n",
    "blessing.uri = os.path.join(_data_root, 'model_validator/blessed/')\n",
    "\n",
    "results = types.TfxType(type_name='ModelValidationPath')\n",
    "results.uri = os.path.join(_data_root, 'model_validator/results/')\n",
    "\n",
    "model_validator_outputs = ComponentOutputs({\n",
    "    'blessing':\n",
    "    Channel(\n",
    "        type_name='ModelBlessingPath',\n",
    "        static_artifact_collection=[blessing]),\n",
    "    'results':\n",
    "    Channel(\n",
    "        type_name='ModelValidationPath',\n",
    "        static_artifact_collection=[results]),\n",
    "})\n",
    "\n",
    "model_validator = ModelValidator(\n",
    "      examples=example_gen.outputs.examples, \n",
    "    model=trainer.outputs.output,\n",
    "    outputs=model_validator_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"TFX Pipeline\",\n",
    "    pipeline_root=_pipeline_root,\n",
    "    components=[example_gen, statistics_gen, infer_schema, transform, trainer, model_analyzer]\n",
    "#     components=[model_analyzer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectRunner(TfxRunner):\n",
    "    \"\"\"Tfx runner on local\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self._config = config or {}\n",
    "    \n",
    "    def run(self, pipeline):\n",
    "        for component in pipeline.components:\n",
    "            self._execute_component(component)\n",
    "            \n",
    "        return pipeline\n",
    "            \n",
    "    def _execute_component(self, component):\n",
    "        input_dict = {key:value.get() for key, value in component.input_dict.items()}\n",
    "        output_dict = {key: value.get() for key, value in component.outputs.get_all().items()}\n",
    "        exec_properties = component.exec_properties\n",
    "        executor = component.executor()\n",
    "        executor.Do(input_dict, output_dict, exec_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-20 08:31:24,167] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ExternalPath\"}}, \"uri\": \"/root/taxi/data/simple\"}, \"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\"}, \"name\": \"ExternalPath\"}}]}\n",
      "[2019-06-20 08:31:24,175] {base_executor.py:74} INFO - Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ExternalPath\"}}, \"uri\": \"/root/taxi/data/simple\"}, \"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\"}, \"name\": \"ExternalPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"eval_examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"training_examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "[2019-06-20 08:31:24,178] {base_executor.py:76} INFO - Outputs for Executor is: {\"eval_examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"training_examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hashBuckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hashBuckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"}\n",
      "[2019-06-20 08:31:24,179] {base_executor.py:78} INFO - Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hashBuckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hashBuckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"}\n",
      "INFO:tensorflow:Generating examples.\n",
      "[2019-06-20 08:31:24,184] {base_example_gen_executor.py:122} INFO - Generating examples.\n",
      "[2019-06-20 08:31:24,190] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-20 08:31:24,213] {executor.py:70} INFO - Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-20 08:31:25,034] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f09a246fae8> ====================\n",
      "[2019-06-20 08:31:25,037] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f09a246fbf8> ====================\n",
      "[2019-06-20 08:31:25,040] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f09a246fc80> ====================\n",
      "[2019-06-20 08:31:25,046] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f09a246fd08> ====================\n",
      "[2019-06-20 08:31:25,067] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f09a246fd90> ====================\n",
      "[2019-06-20 08:31:25,072] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f09a246fea0> ====================\n",
      "[2019-06-20 08:31:25,078] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f09a246ff28> ====================\n",
      "[2019-06-20 08:31:25,087] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f09a2471048> ====================\n",
      "[2019-06-20 08:31:25,089] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f09a24710d0> ====================\n",
      "[2019-06-20 08:31:25,092] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f09a2471268> ====================\n",
      "[2019-06-20 08:31:25,097] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f09a24712f0> ====================\n",
      "[2019-06-20 08:31:25,100] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f09a2471378> ====================\n",
      "[2019-06-20 08:31:25,114] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/DoOnce/Read_40)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/InitializeWrite_41))+(ref_PCollection_PCollection_24/Write))+(ref_PCollection_PCollection_25/Write)\n",
      "[2019-06-20 08:31:25,134] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_InputSourceToExample/ReadFromText/Read_4)+((((ref_AppliedPTransform_InputSourceToExample/ParseCSV/ParseCSVRecords_6)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/KeyWithVoid_8))+(ref_PCollection_PCollection_2/Write))+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Precombine)))+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Write)\n",
      "[2019-06-20 08:31:26,504] {fn_api_runner.py:437} INFO - Running ((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Read)+((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Merge)+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/ExtractOutputs)))+((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/UnKey_16)+(ref_PCollection_PCollection_8/Write))\n",
      "[2019-06-20 08:31:26,522] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/DoOnce/Read_18)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/InjectDefault_19))+(ref_PCollection_PCollection_10/Write)\n",
      "[2019-06-20 08:31:26,542] {fn_api_runner.py:437} INFO - Running (((ref_PCollection_PCollection_2/Read)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/CreateInMemoryDict_20))+((ref_AppliedPTransform_InputSourceToExample/ToTFExample_21)+(ref_AppliedPTransform_SerializeDeterministically_22)))+(((((ref_AppliedPTransform_SplitData/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_25)+(ref_AppliedPTransform_ShuffleSplittrain/AddRandomKeys_27))+((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/Map(reify_timestamps)_29)+(ShuffleSplittrain/ReshufflePerKey/GroupByKey/Write)))+((ref_AppliedPTransform_ShuffleSpliteval/AddRandomKeys_53)+(ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/Map(reify_timestamps)_55)))+(ShuffleSpliteval/ReshufflePerKey/GroupByKey/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:31:31,980] {fn_api_runner.py:437} INFO - Running ((ShuffleSplittrain/ReshufflePerKey/GroupByKey/Read)+((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/FlatMap(restore_timestamps)_34)+(ref_AppliedPTransform_ShuffleSplittrain/RemoveRandomKeys_35)))+((((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WriteBundles_42)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Pair_43))+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WindowInto(WindowIntoFn)_44))+(OutputSplittrain/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-20 08:31:32,124] {tfrecordio.py:57} WARNING - Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "[2019-06-20 08:31:32,444] {fn_api_runner.py:437} INFO - Running ((OutputSplittrain/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Extract_49))+(ref_PCollection_PCollection_32/Write)\n",
      "[2019-06-20 08:31:32,458] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/PreFinalize_50)+(ref_PCollection_PCollection_33/Write))\n",
      "[2019-06-20 08:31:32,474] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/FinalizeWrite_51)\n",
      "[2019-06-20 08:31:32,487] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:31:32,593] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-20 08:31:32,613] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/DoOnce/Read_66)+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/InitializeWrite_67)+(ref_PCollection_PCollection_43/Write)))+(ref_PCollection_PCollection_42/Write)\n",
      "[2019-06-20 08:31:32,637] {fn_api_runner.py:437} INFO - Running ((((ShuffleSpliteval/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/FlatMap(restore_timestamps)_60))+(ref_AppliedPTransform_ShuffleSpliteval/RemoveRandomKeys_61))+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WriteBundles_68)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Pair_69)))+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WindowInto(WindowIntoFn)_70)+(OutputSpliteval/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-20 08:31:32,927] {fn_api_runner.py:437} INFO - Running (OutputSpliteval/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Extract_75)+(ref_PCollection_PCollection_50/Write))\n",
      "[2019-06-20 08:31:32,940] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/PreFinalize_76)+(ref_PCollection_PCollection_51/Write))\n",
      "[2019-06-20 08:31:32,954] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/FinalizeWrite_77)\n",
      "[2019-06-20 08:31:32,968] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:31:33,071] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Examples generated.\n",
      "[2019-06-20 08:31:33,097] {base_example_gen_executor.py:145} INFO - Examples generated.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-20 08:31:33,100] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "[2019-06-20 08:31:33,103] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}, \"uri\": \"/root/taxi/data/simple/statistics_gen/train/\"}, \"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}, \"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}}]}\n",
      "[2019-06-20 08:31:33,106] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}, \"uri\": \"/root/taxi/data/simple/statistics_gen/train/\"}, \"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}}, \"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {}\n",
      "[2019-06-20 08:31:33,108] {base_executor.py:78} INFO - Execution properties for Executor is: {}\n",
      "[2019-06-20 08:31:33,114] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Generating statistics for split train\n",
      "[2019-06-20 08:31:33,147] {executor.py:62} INFO - Generating statistics for split train\n",
      "INFO:tensorflow:Generating statistics for split eval\n",
      "[2019-06-20 08:31:33,899] {executor.py:62} INFO - Generating statistics for split eval\n",
      "INFO:tensorflow:Statistics written to /root/taxi/data/simple/statistics_gen/eval/.\n",
      "[2019-06-20 08:31:34,768] {executor.py:78} INFO - Statistics written to /root/taxi/data/simple/statistics_gen/eval/.\n",
      "[2019-06-20 08:31:37,787] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f09a246fae8> ====================\n",
      "[2019-06-20 08:31:37,791] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f09a246fbf8> ====================\n",
      "[2019-06-20 08:31:37,796] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f09a246fc80> ====================\n",
      "[2019-06-20 08:31:37,801] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f09a246fd08> ====================\n",
      "[2019-06-20 08:31:37,803] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f09a246fd90> ====================\n",
      "[2019-06-20 08:31:37,809] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f09a246fea0> ====================\n",
      "[2019-06-20 08:31:37,813] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f09a246ff28> ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:31:37,824] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f09a2471048> ====================\n",
      "[2019-06-20 08:31:37,827] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f09a24710d0> ====================\n",
      "[2019-06-20 08:31:37,829] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f09a2471268> ====================\n",
      "[2019-06-20 08:31:37,832] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f09a24712f0> ====================\n",
      "[2019-06-20 08:31:37,834] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f09a2471378> ====================\n",
      "[2019-06-20 08:31:37,870] {fn_api_runner.py:437} INFO - Running (((((ref_AppliedPTransform_ReadData.eval/Read_106)+(ref_AppliedPTransform_DecodeData.eval/ParseTFExamples_108))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/KeyWithVoid_111))+((((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_115)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_116)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write))))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_135)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_136)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_138)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write)))\n",
      "[2019-06-20 08:31:40,974] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs))+((((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_146)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_156))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_158)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write))\n",
      "[2019-06-20 08:31:41,009] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_166)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1))\n",
      "[2019-06-20 08:31:41,034] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_155)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)))\n",
      "[2019-06-20 08:31:41,059] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_168))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "[2019-06-20 08:31:41,084] {fn_api_runner.py:437} INFO - Running (((((((ref_AppliedPTransform_ReadData.train/Read_3)+(ref_AppliedPTransform_DecodeData.train/ParseTFExamples_5))+(((((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/KeyWithVoid_8)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_12))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_32))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_13))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_33)))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_35))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write)))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:31:47,048] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_21))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_22)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1)))\n",
      "[2019-06-20 08:31:47,298] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "[2019-06-20 08:31:47,438] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge))+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0))\n",
      "[2019-06-20 08:31:47,723] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+((((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_43))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_53))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_55)))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write))\n",
      "[2019-06-20 08:31:47,757] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_63))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1))\n",
      "[2019-06-20 08:31:47,773] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_52)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)))\n",
      "[2019-06-20 08:31:47,795] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_65)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)))\n",
      "[2019-06-20 08:31:47,812] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "[2019-06-20 08:31:47,828] {fn_api_runner.py:437} INFO - Running ((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_74)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_77)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "[2019-06-20 08:31:47,860] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_85)+(ref_PCollection_PCollection_51/Write)))\n",
      "[2019-06-20 08:31:47,880] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_124)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_125))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:31:48,311] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write))\n",
      "[2019-06-20 08:31:48,452] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)))\n",
      "[2019-06-20 08:31:48,739] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "[2019-06-20 08:31:48,755] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_177)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_180))))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write))\n",
      "[2019-06-20 08:31:48,796] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_188)+(ref_PCollection_PCollection_115/Write)))\n",
      "[2019-06-20 08:31:48,819] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_190)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_191))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_192))+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/Map(<lambda at iobase.py:984>)_199))+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WindowInto(WindowIntoFn)_200)+(WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-20 08:31:48,856] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/DoOnce/Read_197)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/InitializeWrite_198))+(ref_PCollection_PCollection_119/Write))+(ref_PCollection_PCollection_120/Write)\n",
      "[2019-06-20 08:31:48,873] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WriteBundles_205))+(ref_PCollection_PCollection_126/Write)\n",
      "[2019-06-20 08:31:48,895] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/DoOnce/Read_94)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/InitializeWrite_95)+(ref_PCollection_PCollection_56/Write)))+(ref_PCollection_PCollection_55/Write)\n",
      "[2019-06-20 08:31:48,923] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_87)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_88))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_89)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/Map(<lambda at iobase.py:984>)_96)))+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WindowInto(WindowIntoFn)_97)+(WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-20 08:31:48,962] {fn_api_runner.py:437} INFO - Running (WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WriteBundles_102)+(ref_PCollection_PCollection_62/Write))\n",
      "[2019-06-20 08:31:48,981] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/PreFinalize_103)+(ref_PCollection_PCollection_63/Write))\n",
      "[2019-06-20 08:31:48,997] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/FinalizeWrite_104)\n",
      "[2019-06-20 08:31:49,033] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:31:49,148] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "[2019-06-20 08:31:49,183] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/PreFinalize_206)+(ref_PCollection_PCollection_127/Write))\n",
      "[2019-06-20 08:31:49,200] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/FinalizeWrite_207)\n",
      "[2019-06-20 08:31:49,213] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:31:49,316] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Infering schema from statistics.\n",
      "[2019-06-20 08:31:49,331] {executor.py:62} INFO - Infering schema from statistics.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "[2019-06-20 08:31:49,336] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:tensorflow:Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
      "[2019-06-20 08:31:49,354] {executor.py:66} INFO - Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-20 08:31:49,359] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"schema\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}, \"uri\": \"/root/taxi/data/simple/schema_gen/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"SchemaPath\"}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:31:49,364] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}], \"schema\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}, \"uri\": \"/root/taxi/data/simple/schema_gen/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"SchemaPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"transform_output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transform_output/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"TransformPath\"}}], \"transformed_examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "[2019-06-20 08:31:49,369] {base_executor.py:76} INFO - Outputs for Executor is: {\"transform_output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transform_output/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"TransformPath\"}}], \"transformed_examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\"}\n",
      "[2019-06-20 08:31:49,373] {base_executor.py:78} INFO - Execution properties for Executor is: {\"module_file\": \"/root/taxi/taxi_utils.py\"}\n",
      "INFO:tensorflow:Inputs to executor.Transform function: {'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*', 'compute_statistics': False, 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'examples_data_format': 'FORMAT_TF_EXAMPLE', 'preprocessing_fn': '/root/taxi/taxi_utils.py', 'tft_statistics_use_tfdv': True, 'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt'}\n",
      "[2019-06-20 08:31:49,391] {executor.py:567} INFO - Inputs to executor.Transform function: {'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*', 'compute_statistics': False, 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'examples_data_format': 'FORMAT_TF_EXAMPLE', 'preprocessing_fn': '/root/taxi/taxi_utils.py', 'tft_statistics_use_tfdv': True, 'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt'}\n",
      "INFO:tensorflow:Outputs to executor.Transform function: {'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path', 'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples'], 'transform_output_path': '/root/taxi/data/simple/transform/transform_output/'}\n",
      "[2019-06-20 08:31:49,397] {executor.py:569} INFO - Outputs to executor.Transform function: {'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path', 'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples'], 'transform_output_path': '/root/taxi/data/simple/transform/transform_output/'}\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_transform/mappers.py:1027: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[2019-06-20 08:31:49,545] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_transform/mappers.py:1027: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
      "[2019-06-20 08:31:49,652] {executor.py:653} INFO - Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
      "INFO:tensorflow:Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
      "[2019-06-20 08:31:49,654] {executor.py:655} INFO - Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
      "INFO:tensorflow:Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n",
      "[2019-06-20 08:31:49,657] {executor.py:657} INFO - Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n",
      "INFO:tensorflow:Transform output path: /root/taxi/data/simple/transform/transform_output/\n",
      "[2019-06-20 08:31:49,660] {executor.py:658} INFO - Transform output path: /root/taxi/data/simple/transform/transform_output/\n",
      "[2019-06-20 08:31:50,064] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "[2019-06-20 08:31:50,613] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-20 08:31:50,621] {builder_impl.py:654} INFO - Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n",
      "[2019-06-20 08:31:50,623] {builder_impl.py:449} INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/6dd54de0fa9e4fd88c5622d2adb3d541/saved_model.pb\n",
      "[2019-06-20 08:31:50,680] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/6dd54de0fa9e4fd88c5622d2adb3d541/saved_model.pb\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-20 08:31:53,027] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "[2019-06-20 08:31:53,029] {builder_impl.py:449} INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/1a148f09dce844b7b191a02f6ede33d6/saved_model.pb\n",
      "[2019-06-20 08:31:53,065] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/1a148f09dce844b7b191a02f6ede33d6/saved_model.pb\n",
      "[2019-06-20 08:31:57,710] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f09a246fae8> ====================\n",
      "[2019-06-20 08:31:57,719] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f09a246fbf8> ====================\n",
      "[2019-06-20 08:31:57,725] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f09a246fc80> ====================\n",
      "[2019-06-20 08:31:57,744] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f09a246fd08> ====================\n",
      "[2019-06-20 08:31:57,749] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f09a246fd90> ====================\n",
      "[2019-06-20 08:31:57,760] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f09a246fea0> ====================\n",
      "[2019-06-20 08:31:57,766] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f09a246ff28> ====================\n",
      "[2019-06-20 08:31:57,803] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f09a2471048> ====================\n",
      "[2019-06-20 08:31:57,812] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f09a24710d0> ====================\n",
      "[2019-06-20 08:31:57,816] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f09a2471268> ====================\n",
      "[2019-06-20 08:31:57,824] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f09a24712f0> ====================\n",
      "[2019-06-20 08:31:57,827] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f09a2471378> ====================\n",
      "[2019-06-20 08:31:57,875] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadAnalysisDataset[0]/Read/Read_4)+(ref_AppliedPTransform_ReadAnalysisDataset[0]/AddKey_5))+(ref_AppliedPTransform_ReadAnalysisDataset[0]/ParseExamples_6))+((ref_AppliedPTransform_DecodeAnalysisDataset[0]/ApplyDecodeFn_8)+(FlattenAnalysisDatasets/Write/0))\n",
      "[2019-06-20 08:31:59,865] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CreateSavedModelForAnalyzerInputs[0]/CreateSavedModel/Read_13)+(ref_PCollection_PCollection_6/Write)\n",
      "[2019-06-20 08:31:59,877] {fn_api_runner.py:437} INFO - Running ((((((((((FlattenAnalysisDatasets/Read)+((ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/BatchInputs/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_17)+(ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/ApplySavedModel_18)))+(((ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score/mean_and_var]_19)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/KeyWithVoid_22))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write))))+(((ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_1/mean_and_var]_46)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/KeyWithVoid_49))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write))))+(((ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_2/mean_and_var]_73)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/KeyWithVoid_76)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write)))+((((ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary/vocabulary]_100)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FlattenStringsAndMaybeWeightsLabels_102))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CountPerString:PairWithVoid_104)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine)))+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write)))+((ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_1/vocabulary]_158)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FlattenStringsAndMaybeWeightsLabels_160)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CountPerString:PairWithVoid_162)+((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write))))))+((ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize/quantiles]_216)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/KeyWithVoid_219)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))))+(((ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_1/quantiles]_248)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/KeyWithVoid_251))+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))))+(((ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_2/quantiles]_280)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/KeyWithVoid_283))+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))))+(((ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_3/quantiles]_312)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/KeyWithVoid_315)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:32:00,430] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:32:01,355] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/UnKey_323)+(ref_PCollection_PCollection_201/Write))\n",
      "[2019-06-20 08:32:01,493] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/UnKey_227)+(ref_PCollection_PCollection_141/Write))\n",
      "[2019-06-20 08:32:01,628] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_127)+((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1))\n",
      "[2019-06-20 08:32:01,643] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+(((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FilterProblematicStrings_112)))+((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Precombine)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Write))\n",
      "[2019-06-20 08:32:01,677] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Read)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Merge))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/ExtractOutputs))+((((ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/SwapStringsAndCounts_121)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_125))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0))\n",
      "[2019-06-20 08:32:01,705] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
      "[2019-06-20 08:32:01,718] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_133)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_134)))+(ref_PCollection_PCollection_83/Write)\n",
      "[2019-06-20 08:32:01,737] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/Prepare/Read_137)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/OrderElements_138))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_145)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_146)+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-20 08:32:01,771] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_143)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_144)+(ref_PCollection_PCollection_87/Write)))+(ref_PCollection_PCollection_86/Write)\n",
      "[2019-06-20 08:32:01,790] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_151))+(ref_PCollection_PCollection_93/Write)\n",
      "[2019-06-20 08:32:01,808] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_185)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1)\n",
      "[2019-06-20 08:32:01,824] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+(((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge)+((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FilterProblematicStrings_170)))+((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Precombine)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Write)))\n",
      "[2019-06-20 08:32:01,851] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Read)+((((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Merge)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/SwapStringsAndCounts_179)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_183)))+((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:32:01,885] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
      "[2019-06-20 08:32:01,903] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_191))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_192)+(ref_PCollection_PCollection_119/Write))\n",
      "[2019-06-20 08:32:01,925] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/Prepare/Read_195)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/OrderElements_196))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_203)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_204)))+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write)\n",
      "[2019-06-20 08:32:01,963] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/DoOnce/Read_325)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/InjectDefault_326))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/KeyWithVoid_329))+((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
      "[2019-06-20 08:32:02,051] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/UnKey_337)))+(ref_PCollection_PCollection_209/Write)\n",
      "[2019-06-20 08:32:02,223] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/DoOnce/Read_339)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/InjectDefault_340)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_342))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_3/quantiles/Placeholder]_343)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/11))))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/11)\n",
      "[2019-06-20 08:32:02,257] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+(((((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/UnKey_30))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_33))+((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n",
      "[2019-06-20 08:32:02,287] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Merge))+(((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/UnKey_291))+(ref_PCollection_PCollection_181/Write))\n",
      "[2019-06-20 08:32:02,458] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/DoOnce/Read_293)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/InjectDefault_294)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/KeyWithVoid_297))+((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n",
      "[2019-06-20 08:32:02,566] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/UnKey_305))+(ref_PCollection_PCollection_189/Write)\n",
      "[2019-06-20 08:32:02,713] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/DoOnce/Read_423)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/InitializeWrite_424)+(ref_PCollection_PCollection_263/Write)))+(ref_PCollection_PCollection_262/Write)\n",
      "[2019-06-20 08:32:02,732] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/UnKey_84)))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_87)+((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))))\n",
      "[2019-06-20 08:32:02,759] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+(((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/UnKey_95))+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_97)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder_1]_99))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder]_98))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/5)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/5)))))+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/4))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:32:02,806] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/DoOnce/Read_307)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/InjectDefault_308))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_310)+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_2/quantiles/Placeholder]_311)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/10))))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/10)\n",
      "[2019-06-20 08:32:02,842] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/UnKey_41)+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_43)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder]_44))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/0)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/0))))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder_1]_45))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/1)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/1))))\n",
      "[2019-06-20 08:32:02,893] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+(((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/UnKey_259))+(ref_PCollection_PCollection_161/Write)))\n",
      "[2019-06-20 08:32:03,080] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/DoOnce/Read_261)+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/InjectDefault_262)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/KeyWithVoid_265))+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
      "[2019-06-20 08:32:03,180] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/UnKey_273)+(ref_PCollection_PCollection_169/Write))\n",
      "[2019-06-20 08:32:03,331] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/DoOnce/Read_275)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/InjectDefault_276))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_278)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_1/quantiles/Placeholder]_279)))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/9)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/9))\n",
      "[2019-06-20 08:32:03,357] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_86/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_152)+(ref_PCollection_PCollection_94/Write))\n",
      "[2019-06-20 08:32:03,380] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_86/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_153)+(ref_PCollection_PCollection_95/Write))\n",
      "[2019-06-20 08:32:03,397] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:32:03,503] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-20 08:32:03,523] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/CreatePath/Read_155)+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WaitForVocabularyFile_156)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary/vocabulary/Placeholder]_157))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/6)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/6)))\n",
      "[2019-06-20 08:32:03,554] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/UnKey_57))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_60)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
      "[2019-06-20 08:32:03,586] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/UnKey_68)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_70)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder]_71)))+(((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder_1]_72)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/3))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/3)))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/2)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/2)))\n",
      "[2019-06-20 08:32:03,627] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_201)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_202)+(ref_PCollection_PCollection_123/Write)))+(ref_PCollection_PCollection_122/Write)\n",
      "[2019-06-20 08:32:03,652] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_209))+(ref_PCollection_PCollection_129/Write)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:32:03,677] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_122/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_210)+(ref_PCollection_PCollection_130/Write))\n",
      "[2019-06-20 08:32:03,700] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_122/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_211)+(ref_PCollection_PCollection_131/Write))\n",
      "[2019-06-20 08:32:03,722] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:32:03,827] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-20 08:32:03,849] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/CreatePath/Read_213)+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WaitForVocabularyFile_214)+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1/vocabulary/Placeholder]_215)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/7)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/7))\n",
      "[2019-06-20 08:32:03,880] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/DoOnce/Read_229)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/InjectDefault_230))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/KeyWithVoid_233)+((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n",
      "[2019-06-20 08:32:03,985] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/UnKey_241)+(ref_PCollection_PCollection_149/Write))\n",
      "[2019-06-20 08:32:04,138] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/DoOnce/Read_243)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/InjectDefault_244)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_246)))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize/quantiles/Placeholder]_247)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/8)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/8)))\n",
      "[2019-06-20 08:32:04,166] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CreateSavedModel/Flatten/Read)+(ref_PCollection_PCollection_216/Write)\n",
      "[2019-06-20 08:32:04,183] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/CreateSavedModel/Read_346)+(((((((ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/BindTensors_348)+(ref_AppliedPTransform_AnalyzeDataset/ComputeDeferredMetadata_349))+((ref_AppliedPTransform_AnalyzeDataset/MakeCheapBarrier_350)+(ref_PCollection_PCollection_219/Write)))+(ref_AppliedPTransform_WriteTransformFn/WriteTransformFn_361))+(ref_PCollection_PCollection_217/Write))+(ref_PCollection_PCollection_225/Write))+((ref_AppliedPTransform_WriteTransformFn/WriteMetadata/WriteMetadata_360)+(ref_PCollection_PCollection_224/Write)))\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:32:04,329] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-20 08:32:04,364] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/169f733deb9b4ae3ba0aa24663548f4f/assets\n",
      "[2019-06-20 08:32:04,372] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/169f733deb9b4ae3ba0aa24663548f4f/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/169f733deb9b4ae3ba0aa24663548f4f/saved_model.pb\n",
      "[2019-06-20 08:32:04,422] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/169f733deb9b4ae3ba0aa24663548f4f/saved_model.pb\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:32:04,745] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:32:04,747] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:32:04,752] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:32:04,890] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadTransformDataset[1]/Read/Read_383)+(((((ref_AppliedPTransform_ReadTransformDataset[1]/AddKey_384)+(ref_AppliedPTransform_ReadTransformDataset[1]/ParseExamples_385))+(ref_AppliedPTransform_DecodeTransformDataset[1]/ApplyDecodeFn_387))+((ref_AppliedPTransform_TransformDataset[1]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_391)+(ref_AppliedPTransform_TransformDataset[1]/Transform_392)))+((((ref_AppliedPTransform_TransformDataset[1]/ConvertAndUnbatch_393)+(ref_AppliedPTransform_TransformDataset[1]/MakeCheapBarrier_394))+((ref_AppliedPTransform_EncodeTransformedDataset[1]_398)+(ref_AppliedPTransform_Materialize[1]/DropNoneKeys_418)))+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WriteBundles_425))))+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Pair_426)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_427)+(Materialize[1]/Write/Write/WriteImpl/GroupByKey/Write))))+(ref_PCollection_PCollection_245/Write)\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:32:05,049] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:32:05,050] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:32:05,053] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:32:07,317] {fn_api_runner.py:437} INFO - Running ((Materialize[1]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Extract_432))+(ref_PCollection_PCollection_270/Write)\n",
      "[2019-06-20 08:32:07,332] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_262/Read)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/PreFinalize_433))+(ref_PCollection_PCollection_271/Write)\n",
      "[2019-06-20 08:32:07,356] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/DoOnce/Read_405)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/InitializeWrite_406))+(ref_PCollection_PCollection_250/Write))+(ref_PCollection_PCollection_251/Write)\n",
      "[2019-06-20 08:32:07,379] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_ReadTransformDataset[0]/Read/Read_365)+(ref_AppliedPTransform_ReadTransformDataset[0]/AddKey_366))+(ref_AppliedPTransform_ReadTransformDataset[0]/ParseExamples_367))+(ref_AppliedPTransform_DecodeTransformDataset[0]/ApplyDecodeFn_369))+(((((((((ref_AppliedPTransform_TransformDataset[0]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_373)+(ref_AppliedPTransform_TransformDataset[0]/Transform_374))+(ref_AppliedPTransform_TransformDataset[0]/ConvertAndUnbatch_375))+(ref_AppliedPTransform_TransformDataset[0]/MakeCheapBarrier_376))+(ref_AppliedPTransform_EncodeTransformedDataset[0]_380))+(ref_AppliedPTransform_Materialize[0]/DropNoneKeys_400))+(ref_PCollection_PCollection_234/Write))+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WriteBundles_407)+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Pair_408)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_409))))+(Materialize[0]/Write/Write/WriteImpl/GroupByKey/Write))\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:32:07,766] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:32:07,768] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:32:07,772] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:32:12,005] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[0]/PrepareToClearSharedKeepAlives/Read_378)+(ref_AppliedPTransform_TransformDataset[0]/WaitAndClearSharedKeepAlives_379)\n",
      "[2019-06-20 08:32:12,027] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_WriteMetadata/Create/Read_356)+(ref_AppliedPTransform_WriteMetadata/WriteMetadata_357)\n",
      "[2019-06-20 08:32:12,049] {fn_api_runner.py:437} INFO - Running ((Materialize[0]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Extract_414))+(ref_PCollection_PCollection_258/Write)\n",
      "[2019-06-20 08:32:12,067] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_262/Read)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/FinalizeWrite_434)\n",
      "[2019-06-20 08:32:12,081] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:32:12,187] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-20 08:32:12,209] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_225/Read)+(ref_AppliedPTransform_WriteTransformFn/WaitOnWriteMetadataDone_362)\n",
      "[2019-06-20 08:32:12,231] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/PrepareToClearSharedKeepAlives/Read_352)+(ref_AppliedPTransform_AnalyzeDataset/WaitAndClearSharedKeepAlives_353)\n",
      "[2019-06-20 08:32:12,253] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/PreFinalize_415))+(ref_PCollection_PCollection_259/Write)\n",
      "[2019-06-20 08:32:12,273] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/FinalizeWrite_416)\n",
      "[2019-06-20 08:32:12,292] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:32:12,396] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-20 08:32:12,422] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[1]/PrepareToClearSharedKeepAlives/Read_396)+(ref_AppliedPTransform_TransformDataset[1]/WaitAndClearSharedKeepAlives_397)\n",
      "INFO:tensorflow:Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n",
      "[2019-06-20 08:32:12,451] {executor.py:248} INFO - Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-20 08:32:12,462] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"transform_output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transform_output/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"TransformPath\"}}], \"schema\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}, \"uri\": \"/root/taxi/data/simple/schema_gen/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"SchemaPath\"}}], \"transformed_examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "[2019-06-20 08:32:12,465] {base_executor.py:74} INFO - Inputs for Executor is: {\"transform_output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"TransformPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transform_output/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"TransformPath\"}}], \"schema\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"SchemaPath\"}}, \"uri\": \"/root/taxi/data/simple/schema_gen/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"SchemaPath\"}}], \"transformed_examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}]}\n",
      "[2019-06-20 08:32:12,469] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"custom_config\": null, \"train_args\": \"{\\n  \\\"numSteps\\\": 10000\\n}\", \"module_file\": \"/root/taxi/taxi_utils.py\", \"eval_args\": \"{\\n  \\\"numSteps\\\": 5000\\n}\"}\n",
      "[2019-06-20 08:32:12,477] {base_executor.py:78} INFO - Execution properties for Executor is: {\"custom_config\": null, \"train_args\": \"{\\n  \\\"numSteps\\\": 10000\\n}\", \"module_file\": \"/root/taxi/taxi_utils.py\", \"eval_args\": \"{\\n  \\\"numSteps\\\": 5000\\n}\"}\n",
      "INFO:tensorflow:Using config: {'_global_id_in_cluster': 0, '_log_step_count_steps': 100, '_task_type': 'worker', '_experimental_distribute': None, '_master': '', '_task_id': 0, '_tf_random_seed': None, '_save_checkpoints_steps': 999, '_train_distribute': None, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_protocol': None, '_eval_distribute': None, '_service': None, '_keep_checkpoint_max': 1, '_evaluation_master': '', '_save_checkpoints_secs': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09880c7940>, '_model_dir': '/root/taxi/data/simple/trainer/current/serving_model_dir', '_num_ps_replicas': 0, '_is_chief': True, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_device_fn': None}\n",
      "[2019-06-20 08:32:12,493] {estimator.py:201} INFO - Using config: {'_global_id_in_cluster': 0, '_log_step_count_steps': 100, '_task_type': 'worker', '_experimental_distribute': None, '_master': '', '_task_id': 0, '_tf_random_seed': None, '_save_checkpoints_steps': 999, '_train_distribute': None, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_protocol': None, '_eval_distribute': None, '_service': None, '_keep_checkpoint_max': 1, '_evaluation_master': '', '_save_checkpoints_secs': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09880c7940>, '_model_dir': '/root/taxi/data/simple/trainer/current/serving_model_dir', '_num_ps_replicas': 0, '_is_chief': True, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_device_fn': None}\n",
      "INFO:tensorflow:Training model.\n",
      "[2019-06-20 08:32:12,495] {executor.py:141} INFO - Training model.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "[2019-06-20 08:32:12,501] {estimator_training.py:185} INFO - Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "[2019-06-20 08:32:12,507] {training.py:610} INFO - Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 999 or save_checkpoints_secs None.\n",
      "[2019-06-20 08:32:12,510] {training.py:698} INFO - Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 999 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[2019-06-20 08:32:12,525] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-20 08:32:12,636] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[2019-06-20 08:32:12,648] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-20 08:32:15,127] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "[2019-06-20 08:32:15,130] {basic_session_run_hooks.py:527} INFO - Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2019-06-20 08:32:15,772] {monitored_session.py:222} INFO - Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2019-06-20 08:32:15,965] {session_manager.py:491} INFO - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2019-06-20 08:32:16,004] {session_manager.py:493} INFO - Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:32:17,196] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 0 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:loss = 27.933823, step = 1\n",
      "[2019-06-20 08:32:18,104] {basic_session_run_hooks.py:249} INFO - loss = 27.933823, step = 1\n",
      "INFO:tensorflow:global_step/sec: 153.671\n",
      "[2019-06-20 08:32:18,755] {basic_session_run_hooks.py:680} INFO - global_step/sec: 153.671\n",
      "INFO:tensorflow:loss = 22.811275, step = 101 (0.653 sec)\n",
      "[2019-06-20 08:32:18,758] {basic_session_run_hooks.py:247} INFO - loss = 22.811275, step = 101 (0.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.75\n",
      "[2019-06-20 08:32:19,078] {basic_session_run_hooks.py:680} INFO - global_step/sec: 309.75\n",
      "INFO:tensorflow:loss = 23.325867, step = 201 (0.324 sec)\n",
      "[2019-06-20 08:32:19,081] {basic_session_run_hooks.py:247} INFO - loss = 23.325867, step = 201 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.041\n",
      "[2019-06-20 08:32:19,408] {basic_session_run_hooks.py:680} INFO - global_step/sec: 303.041\n",
      "INFO:tensorflow:loss = 18.212223, step = 301 (0.330 sec)\n",
      "[2019-06-20 08:32:19,411] {basic_session_run_hooks.py:247} INFO - loss = 18.212223, step = 301 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.414\n",
      "[2019-06-20 08:32:19,732] {basic_session_run_hooks.py:680} INFO - global_step/sec: 308.414\n",
      "INFO:tensorflow:loss = 22.808674, step = 401 (0.326 sec)\n",
      "[2019-06-20 08:32:19,738] {basic_session_run_hooks.py:247} INFO - loss = 22.808674, step = 401 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 304.393\n",
      "[2019-06-20 08:32:20,060] {basic_session_run_hooks.py:680} INFO - global_step/sec: 304.393\n",
      "INFO:tensorflow:loss = 15.975277, step = 501 (0.329 sec)\n",
      "[2019-06-20 08:32:20,066] {basic_session_run_hooks.py:247} INFO - loss = 15.975277, step = 501 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 352.776\n",
      "[2019-06-20 08:32:20,344] {basic_session_run_hooks.py:680} INFO - global_step/sec: 352.776\n",
      "INFO:tensorflow:loss = 16.371986, step = 601 (0.285 sec)\n",
      "[2019-06-20 08:32:20,351] {basic_session_run_hooks.py:247} INFO - loss = 16.371986, step = 601 (0.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.423\n",
      "[2019-06-20 08:32:20,732] {basic_session_run_hooks.py:680} INFO - global_step/sec: 258.423\n",
      "INFO:tensorflow:loss = 18.37844, step = 701 (0.387 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:32:20,738] {basic_session_run_hooks.py:247} INFO - loss = 18.37844, step = 701 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.594\n",
      "[2019-06-20 08:32:21,143] {basic_session_run_hooks.py:680} INFO - global_step/sec: 242.594\n",
      "INFO:tensorflow:loss = 17.579018, step = 801 (0.410 sec)\n",
      "[2019-06-20 08:32:21,148] {basic_session_run_hooks.py:247} INFO - loss = 17.579018, step = 801 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.97\n",
      "[2019-06-20 08:32:21,513] {basic_session_run_hooks.py:680} INFO - global_step/sec: 269.97\n",
      "INFO:tensorflow:loss = 19.37462, step = 901 (0.370 sec)\n",
      "[2019-06-20 08:32:21,518] {basic_session_run_hooks.py:247} INFO - loss = 19.37462, step = 901 (0.370 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 999 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:32:21,833] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 999 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[2019-06-20 08:32:21,849] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-20 08:32:22,157] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "[2019-06-20 08:32:23,597] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-20 08:32:24,034] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-20 08:32:24,065] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-20 08:32:24,097] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-06-20T08:32:24Z\n",
      "[2019-06-20 08:32:24,130] {evaluation.py:257} INFO - Starting evaluation at 2019-06-20T08:32:24Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2019-06-20 08:32:24,350] {monitored_session.py:222} INFO - Graph was finalized.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "[2019-06-20 08:32:24,353] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "[2019-06-20 08:32:24,357] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2019-06-20 08:32:24,485] {session_manager.py:491} INFO - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2019-06-20 08:32:24,540] {session_manager.py:493} INFO - Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [500/5000]\n",
      "[2019-06-20 08:32:26,950] {evaluation.py:169} INFO - Evaluation [500/5000]\n",
      "INFO:tensorflow:Evaluation [1000/5000]\n",
      "[2019-06-20 08:32:28,681] {evaluation.py:169} INFO - Evaluation [1000/5000]\n",
      "INFO:tensorflow:Evaluation [1500/5000]\n",
      "[2019-06-20 08:32:30,041] {evaluation.py:169} INFO - Evaluation [1500/5000]\n",
      "INFO:tensorflow:Evaluation [2000/5000]\n",
      "[2019-06-20 08:32:31,712] {evaluation.py:169} INFO - Evaluation [2000/5000]\n",
      "INFO:tensorflow:Evaluation [2500/5000]\n",
      "[2019-06-20 08:32:33,030] {evaluation.py:169} INFO - Evaluation [2500/5000]\n",
      "INFO:tensorflow:Evaluation [3000/5000]\n",
      "[2019-06-20 08:32:34,627] {evaluation.py:169} INFO - Evaluation [3000/5000]\n",
      "INFO:tensorflow:Evaluation [3500/5000]\n",
      "[2019-06-20 08:32:36,276] {evaluation.py:169} INFO - Evaluation [3500/5000]\n",
      "INFO:tensorflow:Evaluation [4000/5000]\n",
      "[2019-06-20 08:32:37,731] {evaluation.py:169} INFO - Evaluation [4000/5000]\n",
      "INFO:tensorflow:Evaluation [4500/5000]\n",
      "[2019-06-20 08:32:39,207] {evaluation.py:169} INFO - Evaluation [4500/5000]\n",
      "INFO:tensorflow:Evaluation [5000/5000]\n",
      "[2019-06-20 08:32:40,927] {evaluation.py:169} INFO - Evaluation [5000/5000]\n",
      "INFO:tensorflow:Finished evaluation at 2019-06-20-08:32:41\n",
      "[2019-06-20 08:32:41,042] {evaluation.py:277} INFO - Finished evaluation at 2019-06-20-08:32:41\n",
      "INFO:tensorflow:Saving dict for global step 999: accuracy = 0.76965, accuracy_baseline = 0.76965, auc = 0.899603, auc_precision_recall = 0.6356882, average_loss = 0.4543829, global_step = 999, label/mean = 0.23035, loss = 18.175316, precision = 0.0, prediction/mean = 0.2286533, recall = 0.0\n",
      "[2019-06-20 08:32:41,048] {estimator.py:1979} INFO - Saving dict for global step 999: accuracy = 0.76965, accuracy_baseline = 0.76965, auc = 0.899603, auc_precision_recall = 0.6356882, average_loss = 0.4543829, global_step = 999, label/mean = 0.23035, loss = 18.175316, precision = 0.0, prediction/mean = 0.2286533, recall = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 999: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "[2019-06-20 08:32:41,486] {estimator.py:2039} INFO - Saving 'checkpoint_path' summary for global step 999: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-999\n",
      "INFO:tensorflow:global_step/sec: 5.00248\n",
      "[2019-06-20 08:32:41,503] {basic_session_run_hooks.py:680} INFO - global_step/sec: 5.00248\n",
      "INFO:tensorflow:loss = 17.988537, step = 1001 (19.993 sec)\n",
      "[2019-06-20 08:32:41,511] {basic_session_run_hooks.py:247} INFO - loss = 17.988537, step = 1001 (19.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.576\n",
      "[2019-06-20 08:32:41,826] {basic_session_run_hooks.py:680} INFO - global_step/sec: 309.576\n",
      "INFO:tensorflow:loss = 16.946928, step = 1101 (0.320 sec)\n",
      "[2019-06-20 08:32:41,831] {basic_session_run_hooks.py:247} INFO - loss = 16.946928, step = 1101 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.679\n",
      "[2019-06-20 08:32:42,335] {basic_session_run_hooks.py:680} INFO - global_step/sec: 196.679\n",
      "INFO:tensorflow:loss = 20.097498, step = 1201 (0.510 sec)\n",
      "[2019-06-20 08:32:42,342] {basic_session_run_hooks.py:247} INFO - loss = 20.097498, step = 1201 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.529\n",
      "[2019-06-20 08:32:42,784] {basic_session_run_hooks.py:680} INFO - global_step/sec: 222.529\n",
      "INFO:tensorflow:loss = 17.05196, step = 1301 (0.450 sec)\n",
      "[2019-06-20 08:32:42,792] {basic_session_run_hooks.py:247} INFO - loss = 17.05196, step = 1301 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.99\n",
      "[2019-06-20 08:32:43,520] {basic_session_run_hooks.py:680} INFO - global_step/sec: 135.99\n",
      "INFO:tensorflow:loss = 18.385931, step = 1401 (0.738 sec)\n",
      "[2019-06-20 08:32:43,534] {basic_session_run_hooks.py:247} INFO - loss = 18.385931, step = 1401 (0.738 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 197.135\n",
      "[2019-06-20 08:32:44,027] {basic_session_run_hooks.py:680} INFO - global_step/sec: 197.135\n",
      "INFO:tensorflow:loss = 17.837294, step = 1501 (0.502 sec)\n",
      "[2019-06-20 08:32:44,032] {basic_session_run_hooks.py:247} INFO - loss = 17.837294, step = 1501 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.15\n",
      "[2019-06-20 08:32:44,416] {basic_session_run_hooks.py:680} INFO - global_step/sec: 257.15\n",
      "INFO:tensorflow:loss = 19.760386, step = 1601 (0.389 sec)\n",
      "[2019-06-20 08:32:44,420] {basic_session_run_hooks.py:247} INFO - loss = 19.760386, step = 1601 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.784\n",
      "[2019-06-20 08:32:44,853] {basic_session_run_hooks.py:680} INFO - global_step/sec: 228.784\n",
      "INFO:tensorflow:loss = 14.11869, step = 1701 (0.523 sec)\n",
      "[2019-06-20 08:32:44,943] {basic_session_run_hooks.py:247} INFO - loss = 14.11869, step = 1701 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.237\n",
      "[2019-06-20 08:32:45,440] {basic_session_run_hooks.py:680} INFO - global_step/sec: 170.237\n",
      "INFO:tensorflow:loss = 17.852573, step = 1801 (0.514 sec)\n",
      "[2019-06-20 08:32:45,457] {basic_session_run_hooks.py:247} INFO - loss = 17.852573, step = 1801 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.134\n",
      "[2019-06-20 08:32:46,169] {basic_session_run_hooks.py:680} INFO - global_step/sec: 137.134\n",
      "INFO:tensorflow:loss = 15.377602, step = 1901 (0.738 sec)\n",
      "[2019-06-20 08:32:46,196] {basic_session_run_hooks.py:247} INFO - loss = 15.377602, step = 1901 (0.738 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1998 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:32:46,766] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 1998 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:32:47,000] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 117.497\n",
      "[2019-06-20 08:32:47,021] {basic_session_run_hooks.py:680} INFO - global_step/sec: 117.497\n",
      "INFO:tensorflow:loss = 12.631643, step = 2001 (0.836 sec)\n",
      "[2019-06-20 08:32:47,032] {basic_session_run_hooks.py:247} INFO - loss = 12.631643, step = 2001 (0.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.191\n",
      "[2019-06-20 08:32:47,419] {basic_session_run_hooks.py:680} INFO - global_step/sec: 251.191\n",
      "INFO:tensorflow:loss = 15.946165, step = 2101 (0.401 sec)\n",
      "[2019-06-20 08:32:47,433] {basic_session_run_hooks.py:247} INFO - loss = 15.946165, step = 2101 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.834\n",
      "[2019-06-20 08:32:47,889] {basic_session_run_hooks.py:680} INFO - global_step/sec: 212.834\n",
      "INFO:tensorflow:loss = 14.87216, step = 2201 (0.462 sec)\n",
      "[2019-06-20 08:32:47,895] {basic_session_run_hooks.py:247} INFO - loss = 14.87216, step = 2201 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.77\n",
      "[2019-06-20 08:32:48,311] {basic_session_run_hooks.py:680} INFO - global_step/sec: 236.77\n",
      "INFO:tensorflow:loss = 19.562304, step = 2301 (0.423 sec)\n",
      "[2019-06-20 08:32:48,318] {basic_session_run_hooks.py:247} INFO - loss = 19.562304, step = 2301 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.255\n",
      "[2019-06-20 08:32:48,860] {basic_session_run_hooks.py:680} INFO - global_step/sec: 182.255\n",
      "INFO:tensorflow:loss = 21.171967, step = 2401 (0.551 sec)\n",
      "[2019-06-20 08:32:48,869] {basic_session_run_hooks.py:247} INFO - loss = 21.171967, step = 2401 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.998\n",
      "[2019-06-20 08:32:49,535] {basic_session_run_hooks.py:680} INFO - global_step/sec: 147.998\n",
      "INFO:tensorflow:loss = 14.580359, step = 2501 (0.679 sec)\n",
      "[2019-06-20 08:32:49,548] {basic_session_run_hooks.py:247} INFO - loss = 14.580359, step = 2501 (0.679 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.484\n",
      "[2019-06-20 08:32:49,943] {basic_session_run_hooks.py:680} INFO - global_step/sec: 245.484\n",
      "INFO:tensorflow:loss = 15.835941, step = 2601 (0.406 sec)\n",
      "[2019-06-20 08:32:49,954] {basic_session_run_hooks.py:247} INFO - loss = 15.835941, step = 2601 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.384\n",
      "[2019-06-20 08:32:50,315] {basic_session_run_hooks.py:680} INFO - global_step/sec: 268.384\n",
      "INFO:tensorflow:loss = 15.143697, step = 2701 (0.369 sec)\n",
      "[2019-06-20 08:32:50,323] {basic_session_run_hooks.py:247} INFO - loss = 15.143697, step = 2701 (0.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.884\n",
      "[2019-06-20 08:32:50,682] {basic_session_run_hooks.py:680} INFO - global_step/sec: 272.884\n",
      "INFO:tensorflow:loss = 13.416733, step = 2801 (0.367 sec)\n",
      "[2019-06-20 08:32:50,690] {basic_session_run_hooks.py:247} INFO - loss = 13.416733, step = 2801 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.604\n",
      "[2019-06-20 08:32:51,218] {basic_session_run_hooks.py:680} INFO - global_step/sec: 186.604\n",
      "INFO:tensorflow:loss = 14.561526, step = 2901 (0.535 sec)\n",
      "[2019-06-20 08:32:51,225] {basic_session_run_hooks.py:247} INFO - loss = 14.561526, step = 2901 (0.535 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2997 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:32:51,568] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 2997 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:32:51,938] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 128.651\n",
      "[2019-06-20 08:32:51,995] {basic_session_run_hooks.py:680} INFO - global_step/sec: 128.651\n",
      "INFO:tensorflow:loss = 17.564056, step = 3001 (0.794 sec)\n",
      "[2019-06-20 08:32:52,020] {basic_session_run_hooks.py:247} INFO - loss = 17.564056, step = 3001 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.894\n",
      "[2019-06-20 08:32:52,490] {basic_session_run_hooks.py:680} INFO - global_step/sec: 201.894\n",
      "INFO:tensorflow:loss = 13.42437, step = 3101 (0.476 sec)\n",
      "[2019-06-20 08:32:52,496] {basic_session_run_hooks.py:247} INFO - loss = 13.42437, step = 3101 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.498\n",
      "[2019-06-20 08:32:52,834] {basic_session_run_hooks.py:680} INFO - global_step/sec: 290.498\n",
      "INFO:tensorflow:loss = 15.246332, step = 3201 (0.347 sec)\n",
      "[2019-06-20 08:32:52,843] {basic_session_run_hooks.py:247} INFO - loss = 15.246332, step = 3201 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.016\n",
      "[2019-06-20 08:32:53,430] {basic_session_run_hooks.py:680} INFO - global_step/sec: 168.016\n",
      "INFO:tensorflow:loss = 16.748228, step = 3301 (0.592 sec)\n",
      "[2019-06-20 08:32:53,434] {basic_session_run_hooks.py:247} INFO - loss = 16.748228, step = 3301 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 324.866\n",
      "[2019-06-20 08:32:53,737] {basic_session_run_hooks.py:680} INFO - global_step/sec: 324.866\n",
      "INFO:tensorflow:loss = 19.43639, step = 3401 (0.308 sec)\n",
      "[2019-06-20 08:32:53,742] {basic_session_run_hooks.py:247} INFO - loss = 19.43639, step = 3401 (0.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.783\n",
      "[2019-06-20 08:32:54,136] {basic_session_run_hooks.py:680} INFO - global_step/sec: 250.783\n",
      "INFO:tensorflow:loss = 19.518707, step = 3501 (0.404 sec)\n",
      "[2019-06-20 08:32:54,146] {basic_session_run_hooks.py:247} INFO - loss = 19.518707, step = 3501 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.892\n",
      "[2019-06-20 08:32:54,812] {basic_session_run_hooks.py:680} INFO - global_step/sec: 147.892\n",
      "INFO:tensorflow:loss = 19.05616, step = 3601 (0.673 sec)\n",
      "[2019-06-20 08:32:54,820] {basic_session_run_hooks.py:247} INFO - loss = 19.05616, step = 3601 (0.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.325\n",
      "[2019-06-20 08:32:55,275] {basic_session_run_hooks.py:680} INFO - global_step/sec: 216.325\n",
      "INFO:tensorflow:loss = 16.441566, step = 3701 (0.464 sec)\n",
      "[2019-06-20 08:32:55,283] {basic_session_run_hooks.py:247} INFO - loss = 16.441566, step = 3701 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.107\n",
      "[2019-06-20 08:32:55,663] {basic_session_run_hooks.py:680} INFO - global_step/sec: 257.107\n",
      "INFO:tensorflow:loss = 15.56508, step = 3801 (0.391 sec)\n",
      "[2019-06-20 08:32:55,675] {basic_session_run_hooks.py:247} INFO - loss = 15.56508, step = 3801 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.182\n",
      "[2019-06-20 08:32:56,168] {basic_session_run_hooks.py:680} INFO - global_step/sec: 198.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 12.892968, step = 3901 (0.502 sec)\n",
      "[2019-06-20 08:32:56,177] {basic_session_run_hooks.py:247} INFO - loss = 12.892968, step = 3901 (0.502 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3996 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:32:56,504] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 3996 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:32:56,770] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 155.317\n",
      "[2019-06-20 08:32:56,812] {basic_session_run_hooks.py:680} INFO - global_step/sec: 155.317\n",
      "INFO:tensorflow:loss = 15.549839, step = 4001 (0.647 sec)\n",
      "[2019-06-20 08:32:56,824] {basic_session_run_hooks.py:247} INFO - loss = 15.549839, step = 4001 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.427\n",
      "[2019-06-20 08:32:57,274] {basic_session_run_hooks.py:680} INFO - global_step/sec: 216.427\n",
      "INFO:tensorflow:loss = 16.911633, step = 4101 (0.461 sec)\n",
      "[2019-06-20 08:32:57,285] {basic_session_run_hooks.py:247} INFO - loss = 16.911633, step = 4101 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.745\n",
      "[2019-06-20 08:32:57,678] {basic_session_run_hooks.py:680} INFO - global_step/sec: 247.745\n",
      "INFO:tensorflow:loss = 17.646536, step = 4201 (0.404 sec)\n",
      "[2019-06-20 08:32:57,689] {basic_session_run_hooks.py:247} INFO - loss = 17.646536, step = 4201 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.044\n",
      "[2019-06-20 08:32:58,143] {basic_session_run_hooks.py:680} INFO - global_step/sec: 215.044\n",
      "INFO:tensorflow:loss = 18.01408, step = 4301 (0.462 sec)\n",
      "[2019-06-20 08:32:58,151] {basic_session_run_hooks.py:247} INFO - loss = 18.01408, step = 4301 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.134\n",
      "[2019-06-20 08:32:58,671] {basic_session_run_hooks.py:680} INFO - global_step/sec: 189.134\n",
      "INFO:tensorflow:loss = 15.309187, step = 4401 (0.531 sec)\n",
      "[2019-06-20 08:32:58,682] {basic_session_run_hooks.py:247} INFO - loss = 15.309187, step = 4401 (0.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.281\n",
      "[2019-06-20 08:32:59,059] {basic_session_run_hooks.py:680} INFO - global_step/sec: 258.281\n",
      "INFO:tensorflow:loss = 17.035282, step = 4501 (0.406 sec)\n",
      "[2019-06-20 08:32:59,088] {basic_session_run_hooks.py:247} INFO - loss = 17.035282, step = 4501 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.963\n",
      "[2019-06-20 08:32:59,725] {basic_session_run_hooks.py:680} INFO - global_step/sec: 149.963\n",
      "INFO:tensorflow:loss = 13.3002205, step = 4601 (0.641 sec)\n",
      "[2019-06-20 08:32:59,730] {basic_session_run_hooks.py:247} INFO - loss = 13.3002205, step = 4601 (0.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.975\n",
      "[2019-06-20 08:33:00,137] {basic_session_run_hooks.py:680} INFO - global_step/sec: 242.975\n",
      "INFO:tensorflow:loss = 14.9015255, step = 4701 (0.414 sec)\n",
      "[2019-06-20 08:33:00,143] {basic_session_run_hooks.py:247} INFO - loss = 14.9015255, step = 4701 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.897\n",
      "[2019-06-20 08:33:00,451] {basic_session_run_hooks.py:680} INFO - global_step/sec: 317.897\n",
      "INFO:tensorflow:loss = 14.314224, step = 4801 (0.314 sec)\n",
      "[2019-06-20 08:33:00,458] {basic_session_run_hooks.py:247} INFO - loss = 14.314224, step = 4801 (0.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.707\n",
      "[2019-06-20 08:33:00,798] {basic_session_run_hooks.py:680} INFO - global_step/sec: 288.707\n",
      "INFO:tensorflow:loss = 14.753254, step = 4901 (0.345 sec)\n",
      "[2019-06-20 08:33:00,802] {basic_session_run_hooks.py:247} INFO - loss = 14.753254, step = 4901 (0.345 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4995 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:33:01,132] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 4995 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:33:01,354] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 170.461\n",
      "[2019-06-20 08:33:01,385] {basic_session_run_hooks.py:680} INFO - global_step/sec: 170.461\n",
      "INFO:tensorflow:loss = 14.640665, step = 5001 (0.588 sec)\n",
      "[2019-06-20 08:33:01,391] {basic_session_run_hooks.py:247} INFO - loss = 14.640665, step = 5001 (0.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.42\n",
      "[2019-06-20 08:33:01,650] {basic_session_run_hooks.py:680} INFO - global_step/sec: 377.42\n",
      "INFO:tensorflow:loss = 15.681483, step = 5101 (0.264 sec)\n",
      "[2019-06-20 08:33:01,654] {basic_session_run_hooks.py:247} INFO - loss = 15.681483, step = 5101 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.846\n",
      "[2019-06-20 08:33:01,966] {basic_session_run_hooks.py:680} INFO - global_step/sec: 315.846\n",
      "INFO:tensorflow:loss = 13.980492, step = 5201 (0.319 sec)\n",
      "[2019-06-20 08:33:01,973] {basic_session_run_hooks.py:247} INFO - loss = 13.980492, step = 5201 (0.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 359.073\n",
      "[2019-06-20 08:33:02,245] {basic_session_run_hooks.py:680} INFO - global_step/sec: 359.073\n",
      "INFO:tensorflow:loss = 13.4404545, step = 5301 (0.276 sec)\n",
      "[2019-06-20 08:33:02,249] {basic_session_run_hooks.py:247} INFO - loss = 13.4404545, step = 5301 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.351\n",
      "[2019-06-20 08:33:02,522] {basic_session_run_hooks.py:680} INFO - global_step/sec: 360.351\n",
      "INFO:tensorflow:loss = 12.641796, step = 5401 (0.277 sec)\n",
      "[2019-06-20 08:33:02,526] {basic_session_run_hooks.py:247} INFO - loss = 12.641796, step = 5401 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.494\n",
      "[2019-06-20 08:33:02,812] {basic_session_run_hooks.py:680} INFO - global_step/sec: 344.494\n",
      "INFO:tensorflow:loss = 14.475501, step = 5501 (0.291 sec)\n",
      "[2019-06-20 08:33:02,817] {basic_session_run_hooks.py:247} INFO - loss = 14.475501, step = 5501 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.831\n",
      "[2019-06-20 08:33:03,247] {basic_session_run_hooks.py:680} INFO - global_step/sec: 229.831\n",
      "INFO:tensorflow:loss = 17.728981, step = 5601 (0.440 sec)\n",
      "[2019-06-20 08:33:03,256] {basic_session_run_hooks.py:247} INFO - loss = 17.728981, step = 5601 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.661\n",
      "[2019-06-20 08:33:03,614] {basic_session_run_hooks.py:680} INFO - global_step/sec: 272.661\n",
      "INFO:tensorflow:loss = 16.538265, step = 5701 (0.363 sec)\n",
      "[2019-06-20 08:33:03,619] {basic_session_run_hooks.py:247} INFO - loss = 16.538265, step = 5701 (0.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 357.405\n",
      "[2019-06-20 08:33:03,894] {basic_session_run_hooks.py:680} INFO - global_step/sec: 357.405\n",
      "INFO:tensorflow:loss = 16.564938, step = 5801 (0.279 sec)\n",
      "[2019-06-20 08:33:03,898] {basic_session_run_hooks.py:247} INFO - loss = 16.564938, step = 5801 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.273\n",
      "[2019-06-20 08:33:04,163] {basic_session_run_hooks.py:680} INFO - global_step/sec: 372.273\n",
      "INFO:tensorflow:loss = 15.792578, step = 5901 (0.274 sec)\n",
      "[2019-06-20 08:33:04,172] {basic_session_run_hooks.py:247} INFO - loss = 15.792578, step = 5901 (0.274 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5994 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:33:04,477] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 5994 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:33:04,668] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 185.946\n",
      "[2019-06-20 08:33:04,700] {basic_session_run_hooks.py:680} INFO - global_step/sec: 185.946\n",
      "INFO:tensorflow:loss = 15.112879, step = 6001 (0.533 sec)\n",
      "[2019-06-20 08:33:04,705] {basic_session_run_hooks.py:247} INFO - loss = 15.112879, step = 6001 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.915\n",
      "[2019-06-20 08:33:05,045] {basic_session_run_hooks.py:680} INFO - global_step/sec: 289.915\n",
      "INFO:tensorflow:loss = 12.553654, step = 6101 (0.348 sec)\n",
      "[2019-06-20 08:33:05,052] {basic_session_run_hooks.py:247} INFO - loss = 12.553654, step = 6101 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 338.185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:33:05,341] {basic_session_run_hooks.py:680} INFO - global_step/sec: 338.185\n",
      "INFO:tensorflow:loss = 19.382275, step = 6201 (0.298 sec)\n",
      "[2019-06-20 08:33:05,350] {basic_session_run_hooks.py:247} INFO - loss = 19.382275, step = 6201 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 338.919\n",
      "[2019-06-20 08:33:05,636] {basic_session_run_hooks.py:680} INFO - global_step/sec: 338.919\n",
      "INFO:tensorflow:loss = 15.390554, step = 6301 (0.292 sec)\n",
      "[2019-06-20 08:33:05,642] {basic_session_run_hooks.py:247} INFO - loss = 15.390554, step = 6301 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 326.147\n",
      "[2019-06-20 08:33:05,943] {basic_session_run_hooks.py:680} INFO - global_step/sec: 326.147\n",
      "INFO:tensorflow:loss = 16.34752, step = 6401 (0.307 sec)\n",
      "[2019-06-20 08:33:05,949] {basic_session_run_hooks.py:247} INFO - loss = 16.34752, step = 6401 (0.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.887\n",
      "[2019-06-20 08:33:06,291] {basic_session_run_hooks.py:680} INFO - global_step/sec: 286.887\n",
      "INFO:tensorflow:loss = 12.514748, step = 6501 (0.349 sec)\n",
      "[2019-06-20 08:33:06,297] {basic_session_run_hooks.py:247} INFO - loss = 12.514748, step = 6501 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 340.796\n",
      "[2019-06-20 08:33:06,585] {basic_session_run_hooks.py:680} INFO - global_step/sec: 340.796\n",
      "INFO:tensorflow:loss = 19.321018, step = 6601 (0.294 sec)\n",
      "[2019-06-20 08:33:06,592] {basic_session_run_hooks.py:247} INFO - loss = 19.321018, step = 6601 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 337.339\n",
      "[2019-06-20 08:33:06,881] {basic_session_run_hooks.py:680} INFO - global_step/sec: 337.339\n",
      "INFO:tensorflow:loss = 14.071686, step = 6701 (0.295 sec)\n",
      "[2019-06-20 08:33:06,886] {basic_session_run_hooks.py:247} INFO - loss = 14.071686, step = 6701 (0.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.917\n",
      "[2019-06-20 08:33:07,199] {basic_session_run_hooks.py:680} INFO - global_step/sec: 314.917\n",
      "INFO:tensorflow:loss = 13.438082, step = 6801 (0.322 sec)\n",
      "[2019-06-20 08:33:07,208] {basic_session_run_hooks.py:247} INFO - loss = 13.438082, step = 6801 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.561\n",
      "[2019-06-20 08:33:07,544] {basic_session_run_hooks.py:680} INFO - global_step/sec: 289.561\n",
      "INFO:tensorflow:loss = 13.164002, step = 6901 (0.342 sec)\n",
      "[2019-06-20 08:33:07,550] {basic_session_run_hooks.py:247} INFO - loss = 13.164002, step = 6901 (0.342 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6993 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:33:07,787] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 6993 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:33:07,976] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 206.786\n",
      "[2019-06-20 08:33:08,028] {basic_session_run_hooks.py:680} INFO - global_step/sec: 206.786\n",
      "INFO:tensorflow:loss = 14.254576, step = 7001 (0.485 sec)\n",
      "[2019-06-20 08:33:08,035] {basic_session_run_hooks.py:247} INFO - loss = 14.254576, step = 7001 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.269\n",
      "[2019-06-20 08:33:08,325] {basic_session_run_hooks.py:680} INFO - global_step/sec: 336.269\n",
      "INFO:tensorflow:loss = 13.671809, step = 7101 (0.296 sec)\n",
      "[2019-06-20 08:33:08,331] {basic_session_run_hooks.py:247} INFO - loss = 13.671809, step = 7101 (0.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.141\n",
      "[2019-06-20 08:33:08,657] {basic_session_run_hooks.py:680} INFO - global_step/sec: 301.141\n",
      "INFO:tensorflow:loss = 11.236282, step = 7201 (0.338 sec)\n",
      "[2019-06-20 08:33:08,669] {basic_session_run_hooks.py:247} INFO - loss = 11.236282, step = 7201 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 421.963\n",
      "[2019-06-20 08:33:08,894] {basic_session_run_hooks.py:680} INFO - global_step/sec: 421.963\n",
      "INFO:tensorflow:loss = 12.271639, step = 7301 (0.230 sec)\n",
      "[2019-06-20 08:33:08,899] {basic_session_run_hooks.py:247} INFO - loss = 12.271639, step = 7301 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.874\n",
      "[2019-06-20 08:33:09,142] {basic_session_run_hooks.py:680} INFO - global_step/sec: 403.874\n",
      "INFO:tensorflow:loss = 14.950558, step = 7401 (0.247 sec)\n",
      "[2019-06-20 08:33:09,146] {basic_session_run_hooks.py:247} INFO - loss = 14.950558, step = 7401 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.507\n",
      "[2019-06-20 08:33:09,493] {basic_session_run_hooks.py:680} INFO - global_step/sec: 284.507\n",
      "INFO:tensorflow:loss = 13.543501, step = 7501 (0.353 sec)\n",
      "[2019-06-20 08:33:09,500] {basic_session_run_hooks.py:247} INFO - loss = 13.543501, step = 7501 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.063\n",
      "[2019-06-20 08:33:09,900] {basic_session_run_hooks.py:680} INFO - global_step/sec: 246.063\n",
      "INFO:tensorflow:loss = 14.107691, step = 7601 (0.406 sec)\n",
      "[2019-06-20 08:33:09,905] {basic_session_run_hooks.py:247} INFO - loss = 14.107691, step = 7601 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.174\n",
      "[2019-06-20 08:33:10,202] {basic_session_run_hooks.py:680} INFO - global_step/sec: 330.174\n",
      "INFO:tensorflow:loss = 14.808285, step = 7701 (0.304 sec)\n",
      "[2019-06-20 08:33:10,209] {basic_session_run_hooks.py:247} INFO - loss = 14.808285, step = 7701 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.14\n",
      "[2019-06-20 08:33:10,525] {basic_session_run_hooks.py:680} INFO - global_step/sec: 310.14\n",
      "INFO:tensorflow:loss = 12.884224, step = 7801 (0.322 sec)\n",
      "[2019-06-20 08:33:10,531] {basic_session_run_hooks.py:247} INFO - loss = 12.884224, step = 7801 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.74\n",
      "[2019-06-20 08:33:10,930] {basic_session_run_hooks.py:680} INFO - global_step/sec: 246.74\n",
      "INFO:tensorflow:loss = 14.250964, step = 7901 (0.411 sec)\n",
      "[2019-06-20 08:33:10,942] {basic_session_run_hooks.py:247} INFO - loss = 14.250964, step = 7901 (0.411 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7992 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:33:11,307] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 7992 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:33:11,541] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 156.327\n",
      "[2019-06-20 08:33:11,570] {basic_session_run_hooks.py:680} INFO - global_step/sec: 156.327\n",
      "INFO:tensorflow:loss = 15.223634, step = 8001 (0.640 sec)\n",
      "[2019-06-20 08:33:11,583] {basic_session_run_hooks.py:247} INFO - loss = 15.223634, step = 8001 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.088\n",
      "[2019-06-20 08:33:11,898] {basic_session_run_hooks.py:680} INFO - global_step/sec: 305.088\n",
      "INFO:tensorflow:loss = 12.558194, step = 8101 (0.332 sec)\n",
      "[2019-06-20 08:33:11,915] {basic_session_run_hooks.py:247} INFO - loss = 12.558194, step = 8101 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.393\n",
      "[2019-06-20 08:33:12,288] {basic_session_run_hooks.py:680} INFO - global_step/sec: 256.393\n",
      "INFO:tensorflow:loss = 15.032063, step = 8201 (0.377 sec)\n",
      "[2019-06-20 08:33:12,292] {basic_session_run_hooks.py:247} INFO - loss = 15.032063, step = 8201 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 281.643\n",
      "[2019-06-20 08:33:12,643] {basic_session_run_hooks.py:680} INFO - global_step/sec: 281.643\n",
      "INFO:tensorflow:loss = 10.521639, step = 8301 (0.362 sec)\n",
      "[2019-06-20 08:33:12,653] {basic_session_run_hooks.py:247} INFO - loss = 10.521639, step = 8301 (0.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.866\n",
      "[2019-06-20 08:33:12,951] {basic_session_run_hooks.py:680} INFO - global_step/sec: 323.866\n",
      "INFO:tensorflow:loss = 13.421619, step = 8401 (0.305 sec)\n",
      "[2019-06-20 08:33:12,958] {basic_session_run_hooks.py:247} INFO - loss = 13.421619, step = 8401 (0.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.822\n",
      "[2019-06-20 08:33:13,264] {basic_session_run_hooks.py:680} INFO - global_step/sec: 319.822\n",
      "INFO:tensorflow:loss = 11.9199705, step = 8501 (0.311 sec)\n",
      "[2019-06-20 08:33:13,269] {basic_session_run_hooks.py:247} INFO - loss = 11.9199705, step = 8501 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.195\n",
      "[2019-06-20 08:33:13,740] {basic_session_run_hooks.py:680} INFO - global_step/sec: 210.195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 12.187817, step = 8601 (0.489 sec)\n",
      "[2019-06-20 08:33:13,758] {basic_session_run_hooks.py:247} INFO - loss = 12.187817, step = 8601 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.194\n",
      "[2019-06-20 08:33:14,280] {basic_session_run_hooks.py:680} INFO - global_step/sec: 185.194\n",
      "INFO:tensorflow:loss = 12.776365, step = 8701 (0.535 sec)\n",
      "[2019-06-20 08:33:14,292] {basic_session_run_hooks.py:247} INFO - loss = 12.776365, step = 8701 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.658\n",
      "[2019-06-20 08:33:14,761] {basic_session_run_hooks.py:680} INFO - global_step/sec: 207.658\n",
      "INFO:tensorflow:loss = 14.660483, step = 8801 (0.474 sec)\n",
      "[2019-06-20 08:33:14,766] {basic_session_run_hooks.py:247} INFO - loss = 14.660483, step = 8801 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.02\n",
      "[2019-06-20 08:33:15,060] {basic_session_run_hooks.py:680} INFO - global_step/sec: 335.02\n",
      "INFO:tensorflow:loss = 15.654726, step = 8901 (0.302 sec)\n",
      "[2019-06-20 08:33:15,068] {basic_session_run_hooks.py:247} INFO - loss = 15.654726, step = 8901 (0.302 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8991 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:33:15,404] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 8991 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:33:15,770] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 128.647\n",
      "[2019-06-20 08:33:15,837] {basic_session_run_hooks.py:680} INFO - global_step/sec: 128.647\n",
      "INFO:tensorflow:loss = 14.807522, step = 9001 (0.774 sec)\n",
      "[2019-06-20 08:33:15,842] {basic_session_run_hooks.py:247} INFO - loss = 14.807522, step = 9001 (0.774 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.724\n",
      "[2019-06-20 08:33:16,341] {basic_session_run_hooks.py:680} INFO - global_step/sec: 198.724\n",
      "INFO:tensorflow:loss = 12.60111, step = 9101 (0.518 sec)\n",
      "[2019-06-20 08:33:16,360] {basic_session_run_hooks.py:247} INFO - loss = 12.60111, step = 9101 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.727\n",
      "[2019-06-20 08:33:16,971] {basic_session_run_hooks.py:680} INFO - global_step/sec: 158.727\n",
      "INFO:tensorflow:loss = 10.382345, step = 9201 (0.624 sec)\n",
      "[2019-06-20 08:33:16,985] {basic_session_run_hooks.py:247} INFO - loss = 10.382345, step = 9201 (0.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.722\n",
      "[2019-06-20 08:33:17,343] {basic_session_run_hooks.py:680} INFO - global_step/sec: 268.722\n",
      "INFO:tensorflow:loss = 12.621042, step = 9301 (0.364 sec)\n",
      "[2019-06-20 08:33:17,349] {basic_session_run_hooks.py:247} INFO - loss = 12.621042, step = 9301 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.449\n",
      "[2019-06-20 08:33:17,618] {basic_session_run_hooks.py:680} INFO - global_step/sec: 362.449\n",
      "INFO:tensorflow:loss = 12.049435, step = 9401 (0.279 sec)\n",
      "[2019-06-20 08:33:17,628] {basic_session_run_hooks.py:247} INFO - loss = 12.049435, step = 9401 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.016\n",
      "[2019-06-20 08:33:17,951] {basic_session_run_hooks.py:680} INFO - global_step/sec: 301.016\n",
      "INFO:tensorflow:loss = 12.320592, step = 9501 (0.334 sec)\n",
      "[2019-06-20 08:33:17,962] {basic_session_run_hooks.py:247} INFO - loss = 12.320592, step = 9501 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 363.21\n",
      "[2019-06-20 08:33:18,226] {basic_session_run_hooks.py:680} INFO - global_step/sec: 363.21\n",
      "INFO:tensorflow:loss = 11.227177, step = 9601 (0.272 sec)\n",
      "[2019-06-20 08:33:18,234] {basic_session_run_hooks.py:247} INFO - loss = 11.227177, step = 9601 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.798\n",
      "[2019-06-20 08:33:18,564] {basic_session_run_hooks.py:680} INFO - global_step/sec: 295.798\n",
      "INFO:tensorflow:loss = 13.199339, step = 9701 (0.335 sec)\n",
      "[2019-06-20 08:33:18,569] {basic_session_run_hooks.py:247} INFO - loss = 13.199339, step = 9701 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.037\n",
      "[2019-06-20 08:33:18,892] {basic_session_run_hooks.py:680} INFO - global_step/sec: 305.037\n",
      "INFO:tensorflow:loss = 14.555927, step = 9801 (0.333 sec)\n",
      "[2019-06-20 08:33:18,902] {basic_session_run_hooks.py:247} INFO - loss = 14.555927, step = 9801 (0.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.893\n",
      "[2019-06-20 08:33:19,243] {basic_session_run_hooks.py:680} INFO - global_step/sec: 284.893\n",
      "INFO:tensorflow:loss = 13.956644, step = 9901 (0.349 sec)\n",
      "[2019-06-20 08:33:19,250] {basic_session_run_hooks.py:247} INFO - loss = 13.956644, step = 9901 (0.349 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9990 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:33:19,573] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 9990 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:33:19,834] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "[2019-06-20 08:33:19,877] {basic_session_run_hooks.py:594} INFO - Saving checkpoints for 10000 into /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "[2019-06-20 08:33:20,074] {training.py:525} INFO - Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-20 08:33:20,172] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-20 08:33:23,014] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-20 08:33:23,078] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-20 08:33:23,130] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-06-20T08:33:23Z\n",
      "[2019-06-20 08:33:23,199] {evaluation.py:257} INFO - Starting evaluation at 2019-06-20T08:33:23Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2019-06-20 08:33:23,497] {monitored_session.py:222} INFO - Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-20 08:33:23,500] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2019-06-20 08:33:23,623] {session_manager.py:491} INFO - Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2019-06-20 08:33:23,680] {session_manager.py:493} INFO - Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [500/5000]\n",
      "[2019-06-20 08:33:26,606] {evaluation.py:169} INFO - Evaluation [500/5000]\n",
      "INFO:tensorflow:Evaluation [1000/5000]\n",
      "[2019-06-20 08:33:29,741] {evaluation.py:169} INFO - Evaluation [1000/5000]\n",
      "INFO:tensorflow:Evaluation [1500/5000]\n",
      "[2019-06-20 08:33:31,504] {evaluation.py:169} INFO - Evaluation [1500/5000]\n",
      "INFO:tensorflow:Evaluation [2000/5000]\n",
      "[2019-06-20 08:33:33,624] {evaluation.py:169} INFO - Evaluation [2000/5000]\n",
      "INFO:tensorflow:Evaluation [2500/5000]\n",
      "[2019-06-20 08:33:35,350] {evaluation.py:169} INFO - Evaluation [2500/5000]\n",
      "INFO:tensorflow:Evaluation [3000/5000]\n",
      "[2019-06-20 08:33:37,227] {evaluation.py:169} INFO - Evaluation [3000/5000]\n",
      "INFO:tensorflow:Evaluation [3500/5000]\n",
      "[2019-06-20 08:33:39,005] {evaluation.py:169} INFO - Evaluation [3500/5000]\n",
      "INFO:tensorflow:Evaluation [4000/5000]\n",
      "[2019-06-20 08:33:40,829] {evaluation.py:169} INFO - Evaluation [4000/5000]\n",
      "INFO:tensorflow:Evaluation [4500/5000]\n",
      "[2019-06-20 08:33:42,676] {evaluation.py:169} INFO - Evaluation [4500/5000]\n",
      "INFO:tensorflow:Evaluation [5000/5000]\n",
      "[2019-06-20 08:33:44,382] {evaluation.py:169} INFO - Evaluation [5000/5000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2019-06-20-08:33:44\n",
      "[2019-06-20 08:33:44,498] {evaluation.py:277} INFO - Finished evaluation at 2019-06-20-08:33:44\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.79386, accuracy_baseline = 0.76983, auc = 0.94384927, auc_precision_recall = 0.7359488, average_loss = 0.33710176, global_step = 10000, label/mean = 0.23017, loss = 13.484071, precision = 0.7267409, prediction/mean = 0.22684945, recall = 0.16731112\n",
      "[2019-06-20 08:33:44,507] {estimator.py:1979} INFO - Saving dict for global step 10000: accuracy = 0.79386, accuracy_baseline = 0.76983, auc = 0.94384927, auc_precision_recall = 0.7359488, average_loss = 0.33710176, global_step = 10000, label/mean = 0.23017, loss = 13.484071, precision = 0.7267409, prediction/mean = 0.22684945, recall = 0.16731112\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-20 08:33:44,525] {estimator.py:2039} INFO - Saving 'checkpoint_path' summary for global step 10000: /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "[2019-06-20 08:33:44,534] {exporter.py:415} INFO - Performing the final export in the end of training.\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:33:44,750] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:33:44,754] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:33:44,761] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-20 08:33:44,792] {estimator.py:1111} INFO - Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-20 08:33:46,436] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\n",
      "[2019-06-20 08:33:46,440] {export.py:587} INFO - Signatures INCLUDED in export for Regress: ['regression']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "[2019-06-20 08:33:46,443] {export.py:587} INFO - Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
      "[2019-06-20 08:33:46,448] {export.py:587} INFO - Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "[2019-06-20 08:33:46,452] {export.py:587} INFO - Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "[2019-06-20 08:33:46,454] {export.py:587} INFO - Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-20 08:33:46,624] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-20 08:33:46,706] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561019624'/assets\n",
      "[2019-06-20 08:33:46,712] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561019624'/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561019624'/saved_model.pb\n",
      "[2019-06-20 08:33:47,282] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/temp-b'1561019624'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 15.066839.\n",
      "[2019-06-20 08:33:47,333] {estimator.py:359} INFO - Loss for final step: 15.066839.\n",
      "INFO:tensorflow:Training complete.  Model written to /root/taxi/data/simple/trainer/current/serving_model_dir\n",
      "[2019-06-20 08:33:47,342] {executor.py:146} INFO - Training complete.  Model written to /root/taxi/data/simple/trainer/current/serving_model_dir\n",
      "INFO:tensorflow:Exporting eval_savedmodel for TFMA.\n",
      "[2019-06-20 08:33:47,345] {executor.py:149} INFO - Exporting eval_savedmodel for TFMA.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/export.py:476: export_all_saved_models (from tensorflow_estimator.contrib.estimator.python.estimator.export) is deprecated and will be removed after 2018-12-03.\n",
      "Instructions for updating:\n",
      "Use estimator.experimental_export_all_saved_models\n",
      "[2019-06-20 08:33:47,349] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/export.py:476: export_all_saved_models (from tensorflow_estimator.contrib.estimator.python.estimator.export) is deprecated and will be removed after 2018-12-03.\n",
      "Instructions for updating:\n",
      "Use estimator.experimental_export_all_saved_models\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:33:47,507] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-20 08:33:47,509] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-20 08:33:47,516] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2019-06-20 08:33:47,606] {estimator.py:1111} INFO - Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-20 08:33:49,849] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "[2019-06-20 08:33:49,890] {metrics_impl.py:783} WARNING - Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2019-06-20 08:33:49,925] {estimator.py:1113} INFO - Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "[2019-06-20 08:33:49,935] {export.py:587} INFO - Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "[2019-06-20 08:33:49,938] {export.py:587} INFO - Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "[2019-06-20 08:33:49,940] {export.py:587} INFO - Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "[2019-06-20 08:33:49,943] {export.py:587} INFO - Signatures INCLUDED in export for Train: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n",
      "[2019-06-20 08:33:49,958] {export.py:587} INFO - Signatures INCLUDED in export for Eval: ['eval']\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "[2019-06-20 08:33:49,963] {tf_logging.py:161} WARNING - Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "[2019-06-20 08:33:50,220] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/serving_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-20 08:33:50,330] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561019627'/assets\n",
      "[2019-06-20 08:33:50,333] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561019627'/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561019627'/saved_model.pb\n",
      "[2019-06-20 08:33:50,800] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/trainer/current/eval_model_dir/temp-b'1561019627'/saved_model.pb\n",
      "INFO:tensorflow:Exported eval_savedmodel to /root/taxi/data/simple/trainer/current/eval_model_dir.\n",
      "[2019-06-20 08:33:50,817] {executor.py:155} INFO - Exported eval_savedmodel to /root/taxi/data/simple/trainer/current/eval_model_dir.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-20 08:33:50,830] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"model_exports\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}], \"examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "[2019-06-20 08:33:50,836] {base_executor.py:74} INFO - Inputs for Executor is: {\"model_exports\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelExportPath\"}}, \"uri\": \"/root/taxi/data/simple/trainer/current/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ModelExportPath\"}}], \"examples\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"train\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}, {\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"eval\"}, \"type_name\": {\"stringValue\": \"ExamplesPath\"}}, \"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\"}, \"artifact_type\": {\"properties\": {\"split\": \"STRING\", \"span\": \"INT\", \"type_name\": \"STRING\", \"name\": \"STRING\", \"state\": \"STRING\"}, \"name\": \"ExamplesPath\"}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelEvalPath\"}}, \"uri\": \"/root/taxi/data/simple/eval_output/\"}, \"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\"}, \"name\": \"ModelEvalPath\"}}]}\n",
      "[2019-06-20 08:33:50,843] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"properties\": {\"split\": {\"stringValue\": \"\"}, \"type_name\": {\"stringValue\": \"ModelEvalPath\"}}, \"uri\": \"/root/taxi/data/simple/eval_output/\"}, \"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\"}, \"name\": \"ModelEvalPath\"}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"feature_slicing_spec\": \"{\\n  \\\"specs\\\": [\\n    {\\n      \\\"columnForSlicing\\\": [\\n        \\\"trip_start_hour\\\"\\n      ]\\n    }\\n  ]\\n}\"}\n",
      "[2019-06-20 08:33:50,846] {base_executor.py:78} INFO - Execution properties for Executor is: {\"feature_slicing_spec\": \"{\\n  \\\"specs\\\": [\\n    {\\n      \\\"columnForSlicing\\\": [\\n        \\\"trip_start_hour\\\"\\n      ]\\n    }\\n  ]\\n}\"}\n",
      "INFO:tensorflow:Using /root/taxi/data/simple/trainer/current/eval_model_dir/1561019627 for model eval.\n",
      "[2019-06-20 08:33:50,850] {executor.py:96} INFO - Using /root/taxi/data/simple/trainer/current/eval_model_dir/1561019627 for model eval.\n",
      "INFO:tensorflow:Evaluating model.\n",
      "[2019-06-20 08:33:50,854] {executor.py:100} INFO - Evaluating model.\n",
      "[2019-06-20 08:33:50,873] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/slicer/slicer.py:407: BeamDeprecationWarning: RemoveDuplicates is deprecated since 2.12. Use Distinct instead.\n",
      "  | 'IncrementCounter' >> beam.Map(increment_counter))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:33:51,739] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f09a246fae8> ====================\n",
      "[2019-06-20 08:33:51,741] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f09a246fbf8> ====================\n",
      "[2019-06-20 08:33:51,745] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f09a246fc80> ====================\n",
      "[2019-06-20 08:33:51,749] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f09a246fd08> ====================\n",
      "[2019-06-20 08:33:51,753] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f09a246fd90> ====================\n",
      "[2019-06-20 08:33:51,759] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f09a246fea0> ====================\n",
      "[2019-06-20 08:33:51,762] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f09a246ff28> ====================\n",
      "[2019-06-20 08:33:51,776] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f09a2471048> ====================\n",
      "[2019-06-20 08:33:51,779] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f09a24710d0> ====================\n",
      "[2019-06-20 08:33:51,782] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f09a2471268> ====================\n",
      "[2019-06-20 08:33:51,785] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f09a24712f0> ====================\n",
      "[2019-06-20 08:33:51,793] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f09a2471378> ====================\n",
      "[2019-06-20 08:33:51,808] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/DoOnce/Read_84)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/InitializeWrite_85))+(ref_PCollection_PCollection_43/Write))+(ref_PCollection_PCollection_44/Write)\n",
      "[2019-06-20 08:33:51,832] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/DoOnce/Read_102)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/InitializeWrite_103))+(ref_PCollection_PCollection_54/Write))+(ref_PCollection_PCollection_55/Write)\n",
      "[2019-06-20 08:33:51,847] {fn_api_runner.py:437} INFO - Running ((((((ref_AppliedPTransform_ReadData/Read_3)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/InputsToExtracts/Map(<lambda at model_eval_lib.py:393>)_6))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/Predict/Batch/ParDo(_GlobalWindowsBatchingDoFn)_10))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/Predict/Predict_11)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/ExtractSliceKeys/ParDo(_ExtractSliceKeysFn)_13)))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/Filter/Map(filter_extracts)_16)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/DoSlicing_19)))+((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/ExtractSliceKeys_21)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/ToPairs_24))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Group/Write)))+((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Precombine)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Group/Write))\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/load.py:150: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "[2019-06-20 08:33:51,905] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/load.py:150: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from /root/taxi/data/simple/trainer/current/eval_model_dir/1561019627/variables/variables\n",
      "[2019-06-20 08:33:53,246] {saver.py:1270} INFO - Restoring parameters from /root/taxi/data/simple/trainer/current/eval_model_dir/1561019627/variables/variables\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/graph_ref.py:189: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.get_tensor_from_tensor_info or tf.compat.v1.saved_model.get_tensor_from_tensor_info.\n",
      "[2019-06-20 08:33:53,436] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/eval_saved_model/graph_ref.py:189: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.get_tensor_from_tensor_info or tf.compat.v1.saved_model.get_tensor_from_tensor_info.\n",
      "[2019-06-20 08:34:02,483] {fn_api_runner.py:437} INFO - Running ((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/Merge))+((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/CombinePerSlice/ExtractOutputs)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/InterpretOutput_56))+((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/ComputePerSliceMetrics/ParDo(_SeparateMetricsAndPlotsFn)/ParDo(_SeparateMetricsAndPlotsFn)_58)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/SerializeMetricsAndPlots/SerializeMetrics_60))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/Map(<lambda at iobase.py:984>)_70)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/WindowInto(WindowIntoFn)_71)))+(ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Write)))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/SerializeMetricsAndPlots/SerializePlots_61)))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/Map(<lambda at iobase.py:984>)_86))+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/WindowInto(WindowIntoFn)_87)+(ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:34:02,736] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/DoOnce/Read_68)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/InitializeWrite_69))+(ref_PCollection_PCollection_33/Write))+(ref_PCollection_PCollection_34/Write)\n",
      "[2019-06-20 08:34:02,757] {fn_api_runner.py:437} INFO - Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/WriteBundles_76))+(ref_PCollection_PCollection_40/Write)\n",
      "[2019-06-20 08:34:02,792] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_33/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/PreFinalize_77))+(ref_PCollection_PCollection_41/Write)\n",
      "[2019-06-20 08:34:02,821] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_33/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/metrics)/WriteToTFRecord/Write/WriteImpl/FinalizeWrite_78)\n",
      "[2019-06-20 08:34:02,844] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:34:02,949] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-20 08:34:02,982] {fn_api_runner.py:437} INFO - Running (ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/WriteBundles_92)+(ref_PCollection_PCollection_50/Write))\n",
      "[2019-06-20 08:34:03,015] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/CreateEvalConfig/Read_97)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/Map(<lambda at iobase.py:984>)_104))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/WindowInto(WindowIntoFn)_105))+(ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/GroupByKey/Write)\n",
      "[2019-06-20 08:34:03,079] {fn_api_runner.py:437} INFO - Running ((ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/WriteBundles_110))+(ref_PCollection_PCollection_61/Write)\n",
      "[2019-06-20 08:34:03,114] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_54/Read)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/PreFinalize_111)+(ref_PCollection_PCollection_62/Write))\n",
      "[2019-06-20 08:34:03,138] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/PreFinalize_93))+(ref_PCollection_PCollection_51/Write)\n",
      "[2019-06-20 08:34:03,171] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteResults/WriteTFRecord(/root/taxi/data/simple/eval_output/plots)/WriteToTFRecord/Write/WriteImpl/FinalizeWrite_94)\n",
      "[2019-06-20 08:34:03,203] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:34:03,310] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-20 08:34:03,332] {fn_api_runner.py:437} INFO - Running (ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Group/Read)+((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/Merge)+((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Group/ExtractOutputs)+(((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/RemoveDuplicates/Distinct_32)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/KeyWithVoid_35))+((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write)))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-20 08:34:03,415] {fn_api_runner.py:437} INFO - Running (ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge)+(((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/UnKey_43))+(ref_PCollection_PCollection_20/Write)))\n",
      "[2019-06-20 08:34:03,454] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/DoOnce/Read_45)+((ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/InjectDefault_46)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots/FanoutSlices/TrackDistinctSliceKeys/IncrementCounter_47))\n",
      "[2019-06-20 08:34:03,488] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_54/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults/WriteEvalConfig(EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))/WriteEvalConfig/Write/WriteImpl/FinalizeWrite_112)\n",
      "[2019-06-20 08:34:03,517] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-20 08:34:03,631] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Evaluation complete. Results written to /root/taxi/data/simple/eval_output/.\n",
      "[2019-06-20 08:34:03,650] {executor.py:113} INFO - Evaluation complete. Results written to /root/taxi/data/simple/eval_output/.\n"
     ]
    }
   ],
   "source": [
    "pipeline = DirectRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/taxi/data/simple/:\r\n",
      "total 1.9M\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 20 08:31 csv_example_gen\r\n",
      "1.9M -rw-r--r-- 1 root root 1.9M Jun 20 08:31 data.csv\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:34 eval_output\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:31 schema_gen\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 20 08:31 statistics_gen\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 20 08:32 trainer\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 20 08:32 transform\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:31 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:31 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen/eval:\r\n",
      "total 204K\r\n",
      "204K -rw-r--r-- 1 root root 201K Jun 20 08:31 data_tfrecord-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen/train:\r\n",
      "total 408K\r\n",
      "408K -rw-r--r-- 1 root root 405K Jun 20 08:31 data_tfrecord-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/eval_output:\r\n",
      "total 16K\r\n",
      "4.0K -rw-r--r-- 1 root root  506 Jun 20 08:34 eval_config\r\n",
      " 12K -rw-r--r-- 1 root root 8.4K Jun 20 08:34 metrics\r\n",
      "   0 -rw-r--r-- 1 root root    0 Jun 20 08:34 plots\r\n",
      "\r\n",
      "/root/taxi/data/simple/schema_gen:\r\n",
      "total 8.0K\r\n",
      "8.0K -rw-r--r-- 1 root root 4.5K Jun 20 08:31 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:31 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:31 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen/eval:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 17K Jun 20 08:31 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen/train:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 18K Jun 20 08:31 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 20 08:33 current\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 20 08:33 eval_model_dir\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 20 08:33 serving_model_dir\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 20 08:33 1561019627\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627:\r\n",
      "total 864K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:33 assets\r\n",
      "856K -rw-r--r-- 1 root root 854K Jun 20 08:33 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:33 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 20 08:33 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 20 08:33 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627/variables:\r\n",
      "total 68K\r\n",
      "4.0K -rw-r--r-- 1 root root   8 Jun 20 08:33 variables.data-00000-of-00002\r\n",
      " 60K -rw-r--r-- 1 root root 58K Jun 20 08:33 variables.data-00001-of-00002\r\n",
      "4.0K -rw-r--r-- 1 root root 995 Jun 20 08:33 variables.index\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir:\r\n",
      "total 6.1M\r\n",
      "4.0K -rw-r--r-- 1 root root   89 Jun 20 08:33 checkpoint\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:32 eval_chicago-taxi-eval\r\n",
      "3.6M -rw-r--r-- 1 root root 3.6M Jun 20 08:33 events.out.tfevents.1561019535.72e977282d1f\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 20 08:33 export\r\n",
      "1.6M -rw-r--r-- 1 root root 1.6M Jun 20 08:32 graph.pbtxt\r\n",
      "4.0K -rw-r--r-- 1 root root    8 Jun 20 08:33 model.ckpt-10000.data-00000-of-00002\r\n",
      "124K -rw-r--r-- 1 root root 124K Jun 20 08:33 model.ckpt-10000.data-00001-of-00002\r\n",
      "4.0K -rw-r--r-- 1 root root 2.1K Jun 20 08:33 model.ckpt-10000.index\r\n",
      "824K -rw-r--r-- 1 root root 821K Jun 20 08:33 model.ckpt-10000.meta\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/eval_chicago-taxi-eval:\r\n",
      "total 1.3M\r\n",
      "1.3M -rw-r--r-- 1 root root 1.3M Jun 20 08:33 events.out.tfevents.1561019561.72e977282d1f\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 20 08:33 chicago-taxi\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi:\r\n",
      "total 4.0K\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 20 08:33 1561019624\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1561019624:\r\n",
      "total 564K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:33 assets\r\n",
      "556K -rw-r--r-- 1 root root 554K Jun 20 08:33 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:33 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1561019624/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 20 08:33 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 20 08:33 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/trainer/current/serving_model_dir/export/chicago-taxi/1561019624/variables:\r\n",
      "total 68K\r\n",
      "4.0K -rw-r--r-- 1 root root   8 Jun 20 08:33 variables.data-00000-of-00002\r\n",
      " 60K -rw-r--r-- 1 root root 58K Jun 20 08:33 variables.data-00001-of-00002\r\n",
      "4.0K -rw-r--r-- 1 root root 995 Jun 20 08:33 variables.index\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 5 root root 4.0K Jun 20 08:32 transform_output\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 20 08:32 transformed_examples\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output:\r\n",
      "total 12K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:32 metadata\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 20 08:32 transform_fn\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:32 transformed_metadata\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/metadata:\r\n",
      "total 4.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 916 Jun 20 08:32 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn:\r\n",
      "total 84K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:32 assets\r\n",
      " 76K -rw-r--r-- 1 root root  76K Jun 20 08:32 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:32 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 20 08:32 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 20 08:32 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn/variables:\r\n",
      "total 0\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transformed_metadata:\r\n",
      "total 4.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 2.2K Jun 20 08:32 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:32 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 20 08:32 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples/eval:\r\n",
      "total 172K\r\n",
      "172K -rw-r--r-- 1 root root 172K Jun 20 08:32 transformed_examples-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples/train:\r\n",
      "total 352K\r\n",
      "352K -rw-r--r-- 1 root root 351K Jun 20 08:32 transformed_examples-00000-of-00001.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -Rlhs /root/taxi/data/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_dir(model_analyzer):\n",
    "    artifact = model_analyzer.outputs.output.get()\n",
    "    return types.get_single_uri(artifact)\n",
    "    \n",
    "eval_dir = get_eval_dir(model_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "result = tfma.load_eval_result(eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1f7be2e7cc441c9717a4c30f523069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'post_export_metrics/example_count'}, data=[{'metrics':…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfma.view.render_slicing_metrics(result, slicing_column='trip_start_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No slice matching slicing spec is found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-29ecf71ae9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSingleSliceSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trip_start_hour'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/view/widget_view.py\u001b[0m in \u001b[0;36mrender_plot\u001b[0;34m(result, slicing_spec, label)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0mslice_spec_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslicing_spec\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mslicing_spec\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mSingleSliceSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   data, config = util.get_plot_data_and_config(result.plots, slice_spec_to_use,\n\u001b[0;32m---> 96\u001b[0;31m                                                label)\n\u001b[0m\u001b[1;32m     97\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_model_analysis/view/util.py\u001b[0m in \u001b[0;36mget_plot_data_and_config\u001b[0;34m(results, slicing_spec, label)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No slice matching slicing spec is found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'More than one slice matching slicing spec is found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No slice matching slicing spec is found."
     ]
    }
   ],
   "source": [
    "tfma.view.render_plot(result, tfma.slicer.SingleSliceSpec(features=[('trip_start_hour', 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalResult(slicing_metrics=[((('trip_start_hour', 3),), {'label/mean': {'doubleValue': 0.2527472674846649}, 'precision': {'doubleValue': 0.0}, 'recall': {'doubleValue': 0.0}, 'auc_precision_recall': {'doubleValue': 0.6943898797035217}, 'accuracy_baseline': {'doubleValue': 0.7472527027130127}, 'average_loss': {'doubleValue': 0.3715084195137024}, 'auc': {'doubleValue': 0.9299871921539307}, 'prediction/mean': {'doubleValue': 0.20461934804916382}, 'post_export_metrics/example_count': {'doubleValue': 91.0}, 'accuracy': {'doubleValue': 0.7362637519836426}}), ((('trip_start_hour', 2),), {'accuracy': {'doubleValue': 0.8333333134651184}, 'precision': {'doubleValue': 1.0}, 'recall': {'doubleValue': 0.07999999821186066}, 'auc_precision_recall': {'doubleValue': 0.68250572681427}, 'accuracy_baseline': {'doubleValue': 0.8188405632972717}, 'average_loss': {'doubleValue': 0.3084699511528015}, 'post_export_metrics/example_count': {'doubleValue': 138.0}, 'prediction/mean': {'doubleValue': 0.19989913702011108}, 'auc': {'doubleValue': 0.9453097581863403}, 'label/mean': {'doubleValue': 0.18115942180156708}}), ((('trip_start_hour', 9),), {'label/mean': {'doubleValue': 0.1681034415960312}, 'precision': {'doubleValue': 0.4444444477558136}, 'recall': {'doubleValue': 0.10256410390138626}, 'auc_precision_recall': {'doubleValue': 0.5250459909439087}, 'accuracy_baseline': {'doubleValue': 0.8318965435028076}, 'average_loss': {'doubleValue': 0.3256772756576538}, 'post_export_metrics/example_count': {'doubleValue': 232.0}, 'prediction/mean': {'doubleValue': 0.23059937357902527}, 'auc': {'doubleValue': 0.9158363342285156}, 'accuracy': {'doubleValue': 0.8275862336158752}}), ((('trip_start_hour', 11),), {'label/mean': {'doubleValue': 0.190265491604805}, 'precision': {'doubleValue': 0.6428571343421936}, 'recall': {'doubleValue': 0.20930232107639313}, 'auc_precision_recall': {'doubleValue': 0.7256834506988525}, 'accuracy_baseline': {'doubleValue': 0.8097345232963562}, 'average_loss': {'doubleValue': 0.3085705637931824}, 'auc': {'doubleValue': 0.9478968381881714}, 'prediction/mean': {'doubleValue': 0.2287960946559906}, 'post_export_metrics/example_count': {'doubleValue': 226.0}, 'accuracy': {'doubleValue': 0.8274336457252502}}), ((('trip_start_hour', 21),), {'label/mean': {'doubleValue': 0.2471482902765274}, 'precision': {'doubleValue': 0.8333333134651184}, 'recall': {'doubleValue': 0.1538461595773697}, 'auc_precision_recall': {'doubleValue': 0.7908080816268921}, 'accuracy_baseline': {'doubleValue': 0.7528517246246338}, 'average_loss': {'doubleValue': 0.344769686460495}, 'auc': {'doubleValue': 0.9523698687553406}, 'prediction/mean': {'doubleValue': 0.23174358904361725}, 'post_export_metrics/example_count': {'doubleValue': 263.0}, 'accuracy': {'doubleValue': 0.7832699418067932}}), ((('trip_start_hour', 5),), {'label/mean': {'doubleValue': 0.25581395626068115}, 'precision': {'doubleValue': 0.4000000059604645}, 'recall': {'doubleValue': 0.1818181872367859}, 'auc_precision_recall': {'doubleValue': 0.5169847011566162}, 'accuracy_baseline': {'doubleValue': 0.7441860437393188}, 'average_loss': {'doubleValue': 0.38050469756126404}, 'post_export_metrics/example_count': {'doubleValue': 43.0}, 'prediction/mean': {'doubleValue': 0.2461710423231125}, 'auc': {'doubleValue': 0.875}, 'accuracy': {'doubleValue': 0.7209302186965942}}), ((('trip_start_hour', 4),), {'label/mean': {'doubleValue': 0.1111111119389534}, 'precision': {'doubleValue': 0.5}, 'recall': {'doubleValue': 0.1428571492433548}, 'auc_precision_recall': {'doubleValue': 0.6918675899505615}, 'accuracy_baseline': {'doubleValue': 0.8888888955116272}, 'average_loss': {'doubleValue': 0.22948293387889862}, 'auc': {'doubleValue': 0.9617347121238708}, 'prediction/mean': {'doubleValue': 0.16401831805706024}, 'post_export_metrics/example_count': {'doubleValue': 63.0}, 'accuracy': {'doubleValue': 0.8888888955116272}}), ((('trip_start_hour', 8),), {'prediction/mean': {'doubleValue': 0.23356033861637115}, 'precision': {'doubleValue': 0.9230769276618958}, 'auc_precision_recall': {'doubleValue': 0.7838519215583801}, 'recall': {'doubleValue': 0.30000001192092896}, 'accuracy_baseline': {'doubleValue': 0.7740113139152527}, 'average_loss': {'doubleValue': 0.33486226201057434}, 'auc': {'doubleValue': 0.94370436668396}, 'label/mean': {'doubleValue': 0.2259887009859085}, 'post_export_metrics/example_count': {'doubleValue': 177.0}, 'accuracy': {'doubleValue': 0.8361582159996033}}), ((('trip_start_hour', 18),), {'label/mean': {'doubleValue': 0.28478965163230896}, 'precision': {'doubleValue': 0.7894737124443054}, 'recall': {'doubleValue': 0.17045454680919647}, 'auc_precision_recall': {'doubleValue': 0.7780493497848511}, 'accuracy_baseline': {'doubleValue': 0.7152103185653687}, 'average_loss': {'doubleValue': 0.3797607123851776}, 'post_export_metrics/example_count': {'doubleValue': 309.0}, 'prediction/mean': {'doubleValue': 0.24000127613544464}, 'auc': {'doubleValue': 0.9324095249176025}, 'accuracy': {'doubleValue': 0.7508090734481812}}), ((('trip_start_hour', 22),), {'accuracy': {'doubleValue': 0.7806691527366638}, 'precision': {'doubleValue': 0.7222222089767456}, 'recall': {'doubleValue': 0.19402985274791718}, 'auc_precision_recall': {'doubleValue': 0.7696647644042969}, 'accuracy_baseline': {'doubleValue': 0.7509293556213379}, 'average_loss': {'doubleValue': 0.3553062677383423}, 'post_export_metrics/example_count': {'doubleValue': 269.0}, 'prediction/mean': {'doubleValue': 0.2435951679944992}, 'auc': {'doubleValue': 0.9374907612800598}, 'label/mean': {'doubleValue': 0.24907062947750092}}), ((('trip_start_hour', 19),), {'prediction/mean': {'doubleValue': 0.24187332391738892}, 'precision': {'doubleValue': 0.739130437374115}, 'auc_precision_recall': {'doubleValue': 0.705178439617157}, 'recall': {'doubleValue': 0.23943662643432617}, 'accuracy_baseline': {'doubleValue': 0.7753164768218994}, 'average_loss': {'doubleValue': 0.3371431231498718}, 'auc': {'doubleValue': 0.9393216371536255}, 'label/mean': {'doubleValue': 0.22468353807926178}, 'post_export_metrics/example_count': {'doubleValue': 316.0}, 'accuracy': {'doubleValue': 0.8101266026496887}}), ((('trip_start_hour', 10),), {'accuracy': {'doubleValue': 0.8170731663703918}, 'precision': {'doubleValue': 0.8500000238418579}, 'recall': {'doubleValue': 0.2881355881690979}, 'auc_precision_recall': {'doubleValue': 0.8492279648780823}, 'accuracy_baseline': {'doubleValue': 0.7601625919342041}, 'average_loss': {'doubleValue': 0.32360056042671204}, 'post_export_metrics/example_count': {'doubleValue': 246.0}, 'prediction/mean': {'doubleValue': 0.23671738803386688}, 'auc': {'doubleValue': 0.9622043371200562}, 'label/mean': {'doubleValue': 0.2398373931646347}}), ((('trip_start_hour', 7),), {'label/mean': {'doubleValue': 0.17894737422466278}, 'precision': {'doubleValue': 0.3333333432674408}, 'recall': {'doubleValue': 0.05882352963089943}, 'auc_precision_recall': {'doubleValue': 0.6911222338676453}, 'accuracy_baseline': {'doubleValue': 0.821052610874176}, 'average_loss': {'doubleValue': 0.28930237889289856}, 'auc': {'doubleValue': 0.9566365480422974}, 'prediction/mean': {'doubleValue': 0.19019873440265656}, 'post_export_metrics/example_count': {'doubleValue': 95.0}, 'accuracy': {'doubleValue': 0.8105263113975525}}), ((('trip_start_hour', 17),), {'label/mean': {'doubleValue': 0.24060150980949402}, 'precision': {'doubleValue': 0.625}, 'recall': {'doubleValue': 0.078125}, 'auc_precision_recall': {'doubleValue': 0.7350549697875977}, 'accuracy_baseline': {'doubleValue': 0.7593984603881836}, 'average_loss': {'doubleValue': 0.3400249481201172}, 'auc': {'doubleValue': 0.9532410502433777}, 'prediction/mean': {'doubleValue': 0.21329593658447266}, 'post_export_metrics/example_count': {'doubleValue': 266.0}, 'accuracy': {'doubleValue': 0.7669172883033752}}), ((), {'label/mean': {'doubleValue': 0.23025785386562347}, 'precision': {'doubleValue': 0.7262357473373413}, 'recall': {'doubleValue': 0.167104110121727}, 'auc_precision_recall': {'doubleValue': 0.7358675599098206}, 'accuracy_baseline': {'doubleValue': 0.7697421312332153}, 'average_loss': {'doubleValue': 0.3371615409851074}, 'auc': {'doubleValue': 0.9438409805297852}, 'prediction/mean': {'doubleValue': 0.22685694694519043}, 'post_export_metrics/example_count': {'doubleValue': 4964.0}, 'accuracy': {'doubleValue': 0.7937147617340088}}), ((('trip_start_hour', 23),), {'label/mean': {'doubleValue': 0.1872340440750122}, 'precision': {'doubleValue': 0.699999988079071}, 'recall': {'doubleValue': 0.15909090638160706}, 'auc_precision_recall': {'doubleValue': 0.6646647453308105}, 'accuracy_baseline': {'doubleValue': 0.8127659559249878}, 'average_loss': {'doubleValue': 0.31426215171813965}, 'auc': {'doubleValue': 0.9313422441482544}, 'prediction/mean': {'doubleValue': 0.20761550962924957}, 'post_export_metrics/example_count': {'doubleValue': 235.0}, 'accuracy': {'doubleValue': 0.8297872543334961}}), ((('trip_start_hour', 14),), {'label/mean': {'doubleValue': 0.24380165338516235}, 'precision': {'doubleValue': 0.9230769276618958}, 'recall': {'doubleValue': 0.20338982343673706}, 'auc_precision_recall': {'doubleValue': 0.8407099843025208}, 'accuracy_baseline': {'doubleValue': 0.7561983466148376}, 'average_loss': {'doubleValue': 0.3327888250350952}, 'auc': {'doubleValue': 0.9628137350082397}, 'prediction/mean': {'doubleValue': 0.23183299601078033}, 'post_export_metrics/example_count': {'doubleValue': 242.0}, 'accuracy': {'doubleValue': 0.8016529083251953}}), ((('trip_start_hour', 16),), {'prediction/mean': {'doubleValue': 0.2364329695701599}, 'precision': {'doubleValue': 0.7222222089767456}, 'auc_precision_recall': {'doubleValue': 0.7241955399513245}, 'recall': {'doubleValue': 0.20967741310596466}, 'accuracy_baseline': {'doubleValue': 0.7712177038192749}, 'average_loss': {'doubleValue': 0.3380199372768402}, 'post_export_metrics/example_count': {'doubleValue': 271.0}, 'accuracy': {'doubleValue': 0.8007380366325378}, 'auc': {'doubleValue': 0.9464423656463623}, 'label/mean': {'doubleValue': 0.2287822812795639}}), ((('trip_start_hour', 20),), {'prediction/mean': {'doubleValue': 0.22808118164539337}, 'precision': {'doubleValue': 0.699999988079071}, 'auc_precision_recall': {'doubleValue': 0.7451549768447876}, 'recall': {'doubleValue': 0.15909090638160706}, 'accuracy_baseline': {'doubleValue': 0.7396450042724609}, 'average_loss': {'doubleValue': 0.35592105984687805}, 'auc': {'doubleValue': 0.9370454549789429}, 'label/mean': {'doubleValue': 0.26035502552986145}, 'post_export_metrics/example_count': {'doubleValue': 338.0}, 'accuracy': {'doubleValue': 0.7633135914802551}}), ((('trip_start_hour', 12),), {'label/mean': {'doubleValue': 0.24472573399543762}, 'precision': {'doubleValue': 0.7142857313156128}, 'recall': {'doubleValue': 0.17241379618644714}, 'auc_precision_recall': {'doubleValue': 0.8177218437194824}, 'accuracy_baseline': {'doubleValue': 0.7552742958068848}, 'average_loss': {'doubleValue': 0.3252800703048706}, 'auc': {'doubleValue': 0.9662396907806396}, 'prediction/mean': {'doubleValue': 0.22361671924591064}, 'post_export_metrics/example_count': {'doubleValue': 237.0}, 'accuracy': {'doubleValue': 0.7805907130241394}}), ((('trip_start_hour', 1),), {'label/mean': {'doubleValue': 0.2565445005893707}, 'precision': {'doubleValue': 1.0}, 'recall': {'doubleValue': 0.020408162847161293}, 'auc_precision_recall': {'doubleValue': 0.8718143701553345}, 'accuracy_baseline': {'doubleValue': 0.7434555292129517}, 'average_loss': {'doubleValue': 0.35465356707572937}, 'auc': {'doubleValue': 0.9583932161331177}, 'prediction/mean': {'doubleValue': 0.22013768553733826}, 'post_export_metrics/example_count': {'doubleValue': 191.0}, 'accuracy': {'doubleValue': 0.7486910820007324}}), ((('trip_start_hour', 13),), {'label/mean': {'doubleValue': 0.2467532455921173}, 'precision': {'doubleValue': 0.7368420958518982}, 'recall': {'doubleValue': 0.24561403691768646}, 'auc_precision_recall': {'doubleValue': 0.7300373315811157}, 'accuracy_baseline': {'doubleValue': 0.7532467842102051}, 'average_loss': {'doubleValue': 0.35366111993789673}, 'auc': {'doubleValue': 0.9338072538375854}, 'prediction/mean': {'doubleValue': 0.23805683851242065}, 'post_export_metrics/example_count': {'doubleValue': 231.0}, 'accuracy': {'doubleValue': 0.7922077775001526}}), ((('trip_start_hour', 6),), {'label/mean': {'doubleValue': 0.28333333134651184}, 'precision': {'doubleValue': 1.0}, 'recall': {'doubleValue': 0.05882352963089943}, 'auc_precision_recall': {'doubleValue': 0.8313006162643433}, 'accuracy_baseline': {'doubleValue': 0.7166666984558105}, 'average_loss': {'doubleValue': 0.38616758584976196}, 'post_export_metrics/example_count': {'doubleValue': 60.0}, 'prediction/mean': {'doubleValue': 0.23176731169223785}, 'auc': {'doubleValue': 0.9528043270111084}, 'accuracy': {'doubleValue': 0.7333333492279053}}), ((('trip_start_hour', 15),), {'label/mean': {'doubleValue': 0.22624434530735016}, 'precision': {'doubleValue': 0.75}, 'recall': {'doubleValue': 0.18000000715255737}, 'auc_precision_recall': {'doubleValue': 0.7172993421554565}, 'accuracy_baseline': {'doubleValue': 0.773755669593811}, 'average_loss': {'doubleValue': 0.33712852001190186}, 'auc': {'doubleValue': 0.9466666579246521}, 'prediction/mean': {'doubleValue': 0.22218874096870422}, 'post_export_metrics/example_count': {'doubleValue': 221.0}, 'accuracy': {'doubleValue': 0.8009049892425537}}), ((('trip_start_hour', 0),), {'accuracy': {'doubleValue': 0.7941176295280457}, 'precision': {'doubleValue': 0.3333333432674408}, 'recall': {'doubleValue': 0.05000000074505806}, 'auc_precision_recall': {'doubleValue': 0.7243268489837646}, 'accuracy_baseline': {'doubleValue': 0.8039215803146362}, 'average_loss': {'doubleValue': 0.312480628490448}, 'post_export_metrics/example_count': {'doubleValue': 204.0}, 'prediction/mean': {'doubleValue': 0.21194805204868317}, 'auc': {'doubleValue': 0.9613566994667053}, 'label/mean': {'doubleValue': 0.19607843458652496}})], plots=[], config=EvalConfig(model_location='/root/taxi/data/simple/trainer/current/eval_model_dir/1561019627', data_location='<user provided PCollection>', slice_spec=[SingleSliceSpec(columns=frozenset({'trip_start_hour'}), features=frozenset()), SingleSliceSpec(columns=frozenset(), features=frozenset())], example_weight_metric_key='post_export_metrics/example_count', num_bootstrap_samples=1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
