{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SchemaGen Test run\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AseiSugiyama/TFXTestRun/blob/master/notebooks/SchemaGenTestRun.ipynb)\n",
    "\n",
    "## Set up\n",
    "\n",
    "TFX requires apache-airflow and docker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'apache-airflow[gcp]' docker tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use TFX version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfx\n",
    "tfx.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX requires TensorFlow >= 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX supports Python 3.5 from version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This enables you to run this notebook twice.\n",
    "# There should not be train/eval files at ~/taxi/data, since TFX can handle only single file with version 0.13.0\n",
    "if [ -e ~/taxi/data ]; then\n",
    "    rm -rf ~/taxi/data\n",
    "fi\n",
    "\n",
    "# download taxi data\n",
    "mkdir -p ~/taxi/data/simple\n",
    "mkdir -p ~/taxi/serving_model/taxi_simple\n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv -O ~/taxi/data/simple/data.csv\n",
    "\n",
    "# download \n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py -O ~/taxi/taxi_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.protobuf import json_format\n",
    "\n",
    "from tfx.components.base.base_component import ComponentOutputs\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.orchestration.tfx_runner import TfxRunner\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import csv_input\n",
    "from tfx.utils.channel import Channel\n",
    "from tfx.utils import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example assumes that the taxi data is stored in ~/taxi/data and the\n",
    "# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n",
    "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
    "_data_root = os.path.join(_taxi_root, 'data/simple')\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(_taxi_root, 'serving_model/taxi_simple')\n",
    "\n",
    "# Directory and data locations.  This example assumes all of the chicago taxi\n",
    "# example code and metadata library is relative to $HOME, but you can store\n",
    "# these files anywhere on your local filesystem.\n",
    "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines')\n",
    "_metadata_db_root = os.path.join(_tfx_root, 'metadata')\n",
    "_log_root = os.path.join(_tfx_root, 'logs')\n",
    "\n",
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "_airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2019, 1, 1),\n",
    "}\n",
    "\n",
    "# Logging overrides\n",
    "logger_overrides = {'log_root': _log_root, 'log_level': logging.INFO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implements the chicago taxi pipeline with TFX.\"\"\"\n",
    "examples = csv_input(_data_root)\n",
    "\n",
    "# Brings data into the pipeline or otherwise joins/converts training data.\n",
    "train_config = example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2)\n",
    "eval_config = example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        train_config,\n",
    "        eval_config\n",
    "    ]))\n",
    "\n",
    "# Create outputs\n",
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root, 'train/')\n",
    "\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root, 'eval/')\n",
    "\n",
    "output_dict = {'examples': Channel(\n",
    "    type_name='ExamplesPath',\n",
    "    static_artifact_collection=[train_examples, eval_examples])}\n",
    "\n",
    "example_outputs = ComponentOutputs(output_dict)\n",
    "\n",
    "example_gen = CsvExampleGen(\n",
    "    input_base=examples, # A Channel of 'ExternalPath' type, it contains path of data source.\n",
    "    output_config=output_config,  # An example_gen_pb2.Output instance, it contains train-eval split ratio.\n",
    "    outputs=example_outputs # dict from name to output channel, it will be stored example_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='train')\n",
    "train_statistics.uri = os.path.join(_data_root, 'train/stats/')\n",
    "\n",
    "eval_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='eval')\n",
    "eval_statistics.uri = os.path.join(_data_root, 'eval/stats/')\n",
    "\n",
    "output_dict = {'output': Channel(\n",
    "    type_name='ExampleStatisticsPath',\n",
    "    static_artifact_collection=[train_statistics, eval_statistics])}\n",
    "\n",
    "statistics_outputs = ComponentOutputs(output_dict)\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    input_data=example_gen.outputs.examples, # A Channel of 'ExamplesPath' type, it is equal to example_outputs\n",
    "    name='Statistics Generator', # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=statistics_outputs # dict from name to output channel, it will be stored statistics_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_schema_path = types.TfxType(type_name='SchemaPath', split='train')\n",
    "train_schema_path.uri = os.path.join(_data_root, 'train/schema/')\n",
    "\n",
    "schema_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='SchemaPath',\n",
    "        # SchemaGen.executor can handle just one SchemaPath.\n",
    "        # Two or more SchemaPaths will cause ValueError\n",
    "        # such as \"ValueError: expected list length of one but got 2\".\n",
    "        static_artifact_collection=[train_schema_path] \n",
    "    )\n",
    "})\n",
    "\n",
    "schema_gen = SchemaGen(\n",
    "    stats=statistics_gen.outputs.output, # A Channel of 'ExampleStatisticsPath' type, it is equal to statistics_outputs\n",
    "    name='Schema Generator',  # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=schema_outputs # dict from name to output channel, it will be stored schema_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"TFX Pipeline\",\n",
    "    pipeline_root=_pipeline_root,\n",
    "    components=[example_gen, statistics_gen, schema_gen]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectRunner(TfxRunner):\n",
    "    \"\"\"Tfx runner on local\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self._config = config or {}\n",
    "    \n",
    "    def run(self, pipeline):\n",
    "        for component in pipeline.components:\n",
    "            self._execute_component(component)\n",
    "            \n",
    "        return pipeline\n",
    "            \n",
    "    def _execute_component(self, component):\n",
    "        input_dict = {key:value.get() for key, value in component.input_dict.items()}\n",
    "        output_dict = {key: value.get() for key, value in component.outputs.get_all().items()}\n",
    "        exec_properties = component.exec_properties\n",
    "        executor = component.executor()\n",
    "        executor.Do(input_dict, output_dict, exec_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DirectRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -Rlhs /root/taxi/data/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /root/taxi/data/simple/train/schema/schema.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`schema.pbtxt` is only simple text file, we are going to;\n",
    "\n",
    "1. get the path of `schema.pbtxt`from `schema_gen`\n",
    "2. parse `schema.pbtxt` into schema (protobuf)\n",
    "3. visualize schema with [tfdv](https://www.tensorflow.org/tfx/data_validation/get_started)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the path of `schema.pbtxt`\n",
    "def get_schema_directory(schema_gen):\n",
    "    output_dict = {key: value.get() for key, value in schema_gen.outputs.get_all().items()}\n",
    "    input_dict = {key:value.get() for key, value in schema_gen.input_dict.items()}\n",
    "    split_to_instance = {x.split: x for x in input_dict['stats']}\n",
    "    directory = types.get_split_uri(output_dict['output'], 'train')\n",
    "    return directory\n",
    "\n",
    "schema_directory = get_schema_directory(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. parse schema.pbtxt\n",
    "\n",
    "from pathlib import Path\n",
    "from google.protobuf.text_format import Parse\n",
    "from google.protobuf.message import Message\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "def parse_schema_proto_string(schema_directory):\n",
    "    path = Path(schema_directory)\n",
    "    for filepath in path.glob('*'):\n",
    "        with open(str(filepath), 'r') as file: # since we are using python 3.5, not 3.6+\n",
    "            schema_proto_string = file.read()\n",
    "            schema = schema_pb2.Schema()\n",
    "            text_format.Parse(schema_proto_string, schema)\n",
    "            return schema\n",
    "\n",
    "schema = parse_schema_proto_string(schema_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. visualize schema with tfdv\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "tfdv.display_schema(schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
