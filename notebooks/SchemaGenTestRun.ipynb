{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SchemaGen Test run\n",
    "\n",
    "## Set up\n",
    "\n",
    "TFX requires apache-airflow and docker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-airflow[gcp] in /usr/local/lib/python3.5/dist-packages (1.10.3)\n",
      "\u001b[33m  WARNING: apache-airflow 1.10.3 does not provide the extra 'gcp'\u001b[0m\n",
      "Requirement already satisfied: docker in /usr/local/lib/python3.5/dist-packages (4.0.1)\n",
      "Requirement already satisfied: tfx in /usr/local/lib/python3.5/dist-packages (0.13.0)\n",
      "Requirement already satisfied: iso8601>=0.1.12 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.1.12)\n",
      "Requirement already satisfied: configparser<3.6.0,>=3.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (3.5.3)\n",
      "Requirement already satisfied: flask<2.0,>=1.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.3)\n",
      "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: flask-caching<1.4.0,>=1.3.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.3.3)\n",
      "Requirement already satisfied: thrift>=0.9.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.11.0)\n",
      "Requirement already satisfied: alembic<1.0,>=0.9 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.9.10)\n",
      "Requirement already satisfied: jinja2<=2.10.0,>=2.7.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.10)\n",
      "Requirement already satisfied: future<0.17,>=0.16.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.16.0)\n",
      "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2)\n",
      "Requirement already satisfied: croniter<0.4,>=0.3.17 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.3.30)\n",
      "Requirement already satisfied: flask-appbuilder==1.12.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.12.3)\n",
      "Requirement already satisfied: werkzeug<0.15.0,>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: markdown<3.0,>=2.5.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.6.11)\n",
      "Requirement already satisfied: psutil<6.0.0,>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (5.6.3)\n",
      "Requirement already satisfied: dill<0.3,>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.9)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.22.0)\n",
      "Requirement already satisfied: gunicorn<20.0,>=19.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (19.9.0)\n",
      "Requirement already satisfied: tabulate<0.9,>=0.7.5 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.8.3)\n",
      "Requirement already satisfied: lxml>=4.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.3.4)\n",
      "Requirement already satisfied: pendulum==1.4.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.4.4)\n",
      "Requirement already satisfied: tenacity==4.12.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.12.0)\n",
      "Requirement already satisfied: pandas<1.0.0,>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.24.2)\n",
      "Requirement already satisfied: zope.deprecation<5.0,>=4.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.4.0)\n",
      "Requirement already satisfied: sqlalchemy<1.3.0,>=1.1.15 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2.19)\n",
      "Requirement already satisfied: json-merge-patch==0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2)\n",
      "Requirement already satisfied: python-daemon<2.2,>=2.1.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.2)\n",
      "Requirement already satisfied: gitpython>=2.0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.11)\n",
      "Requirement already satisfied: flask-login<0.5,>=0.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.8.0)\n",
      "Requirement already satisfied: tzlocal>=1.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.1)\n",
      "Requirement already satisfied: flask-swagger==0.2.13 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.13)\n",
      "Requirement already satisfied: setproctitle<2,>=1.1.8 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.1.10)\n",
      "Requirement already satisfied: flask-wtf<0.15,>=0.14.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.2)\n",
      "Requirement already satisfied: pygments<3.0,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.3.1)\n",
      "Requirement already satisfied: flask-admin==1.5.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.3)\n",
      "Requirement already satisfied: funcsigs==1.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.5/dist-packages (from docker) (1.12.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.5/dist-packages (from docker) (0.56.0)\n",
      "Requirement already satisfied: tensorflow-transform<0.14,>=0.13 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.0)\n",
      "Requirement already satisfied: ml-metadata<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: tensorflow-data-validation<0.14,>=0.13.1 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.1)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.12 in /usr/local/lib/python3.5/dist-packages (from tfx) (2.13.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.5/dist-packages (from tfx) (1.7.9)\n",
      "Requirement already satisfied: absl-py<1,>=0.1.6 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.7.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.7 in /usr/local/lib/python3.5/dist-packages (from tfx) (3.7.1)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.5/dist-packages (from flask<2.0,>=1.0->apache-airflow[gcp]) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.5/dist-packages (from flask<2.0,>=1.0->apache-airflow[gcp]) (7.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.12)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.5/dist-packages (from jinja2<=2.10.0,>=2.7.3->apache-airflow[gcp]) (1.1.1)\n",
      "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.4.0)\n",
      "Requirement already satisfied: Flask-OpenID<2,>=1.2.5 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (1.2.5)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.4.1)\n",
      "Requirement already satisfied: Flask-Babel<1,>=0.11.1 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2.8)\n",
      "Requirement already satisfied: pytzdata>=2018.3.0.0 in /usr/local/lib/python3.5/dist-packages (from pendulum==1.4.4->apache-airflow[gcp]) (2019.1)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas<1.0.0,>=0.17.1->apache-airflow[gcp]) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas<1.0.0,>=0.17.1->apache-airflow[gcp]) (1.16.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from zope.deprecation<5.0,>=4.0->apache-airflow[gcp]) (41.0.1)\n",
      "Requirement already satisfied: lockfile>=0.10 in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: docutils in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.14)\n",
      "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.5/dist-packages (from flask-swagger==0.2.13->apache-airflow[gcp]) (3.13)\n",
      "Requirement already satisfied: WTForms in /usr/local/lib/python3.5/dist-packages (from flask-wtf<0.15,>=0.14.2->apache-airflow[gcp]) (2.2.1)\n",
      "Requirement already satisfied: ordereddict in /usr/local/lib/python3.5/dist-packages (from funcsigs==1.0.0->apache-airflow[gcp]) (1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot<1.3,>=1.2.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-transform<0.14,>=0.13->tfx) (1.2.4)\n",
      "Requirement already satisfied: tensorflow-metadata<0.14,>=0.12.1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-transform<0.14,>=0.13->tfx) (0.13.0)\n",
      "Requirement already satisfied: IPython>=5.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (7.5.0)\n",
      "Requirement already satisfied: joblib<1,>=0.12 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn<1,>=0.18 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.21.2)\n",
      "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.21.24)\n",
      "Requirement already satisfied: pyarrow<0.14.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.13.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.5.4)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.0.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7)\n",
      "Requirement already satisfied: grpcio<2,>=1.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.20.1)\n",
      "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.9.0)\n",
      "Requirement already satisfied: httplib2<=0.12.0,>=0.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.12.0)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.0.0)\n",
      "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.1.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.6.1)\n",
      "Requirement already satisfied: google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.39.1)\n",
      "Requirement already satisfied: google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7.4)\n",
      "Requirement already satisfied: google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.5.28)\n",
      "Requirement already satisfied: google-cloud-bigtable<0.33.0,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.32.2)\n",
      "Requirement already satisfied: google-cloud-core<0.30.0,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.29.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.3)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.6.3)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (7.4.2)\n",
      "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.0.0)\n",
      "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.1.0)\n",
      "Requirement already satisfied: python3-openid>=2.0 in /usr/local/lib/python3.5/dist-packages (from Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (3.1.0)\n",
      "Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.5/dist-packages (from Flask-Babel<1,>=0.11.1->flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.7.0)\n",
      "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitdb2>=2.0.0->gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot<1.3,>=1.2.0->tensorflow-transform<0.14,>=0.13->tfx) (2.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.5/dist-packages (from tensorflow-metadata<0.14,>=0.12.1->tensorflow-transform<0.14,>=0.13->tfx) (1.6.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.3.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (2.0.9)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.7.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.0)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.5/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.12->tfx) (0.6.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (0.2.5)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (0.4.5)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.5/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.12->tfx) (5.2.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.0.0 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.11.1)\n",
      "Requirement already satisfied: google-resumable-media>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.3.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.12dev,>=0.11.4 in /usr/local/lib/python3.5/dist-packages (from google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.11.4)\n",
      "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.5/dist-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.15)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.1.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.4.2)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.3)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.0)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.7.8)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.5/dist-packages (from python3-openid>=2.0->Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.2->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from jedi>=0.10->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.4.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.5/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.6.0)\n",
      "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.5/dist-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.5)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.0.1)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.5/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.2)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.5/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.2.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.4.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.1.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.3)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.4.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.4)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (18.0.1)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.6.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.15.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.5/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.5.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'apache-airflow[gcp]' docker tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use TFX version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tfx\n",
    "tfx.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX requires TensorFlow >= 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX supports Python 3.5 from version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 (default, Nov 12 2018, 13:43:14) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-06-13 08:51:50--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1922668 (1.8M) [text/plain]\n",
      "Saving to: ‘/root/taxi/data/simple/data.csv’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2% 3.02M 1s\n",
      "    50K .......... .......... .......... .......... ..........  5% 10.4M 0s\n",
      "   100K .......... .......... .......... .......... ..........  7% 6.14M 0s\n",
      "   150K .......... .......... .......... .......... .......... 10% 8.94M 0s\n",
      "   200K .......... .......... .......... .......... .......... 13% 6.61M 0s\n",
      "   250K .......... .......... .......... .......... .......... 15% 8.74M 0s\n",
      "   300K .......... .......... .......... .......... .......... 18% 14.5M 0s\n",
      "   350K .......... .......... .......... .......... .......... 21% 4.88M 0s\n",
      "   400K .......... .......... .......... .......... .......... 23% 6.74M 0s\n",
      "   450K .......... .......... .......... .......... .......... 26% 5.15M 0s\n",
      "   500K .......... .......... .......... .......... .......... 29% 15.3M 0s\n",
      "   550K .......... .......... .......... .......... .......... 31% 7.12M 0s\n",
      "   600K .......... .......... .......... .......... .......... 34% 10.1M 0s\n",
      "   650K .......... .......... .......... .......... .......... 37% 13.9M 0s\n",
      "   700K .......... .......... .......... .......... .......... 39% 7.45M 0s\n",
      "   750K .......... .......... .......... .......... .......... 42% 7.34M 0s\n",
      "   800K .......... .......... .......... .......... .......... 45% 20.1M 0s\n",
      "   850K .......... .......... .......... .......... .......... 47% 6.70M 0s\n",
      "   900K .......... .......... .......... .......... .......... 50% 8.79M 0s\n",
      "   950K .......... .......... .......... .......... .......... 53% 9.19M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 55% 7.09M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 58% 38.0M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 61% 11.6M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 63% 18.2M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 66% 7.75M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 69% 19.5M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 71% 7.10M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 74% 22.8M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 77% 12.9M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 79% 20.3M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 82% 6.62M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 85% 8.79M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 87% 9.67M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 90% 17.4M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 93% 13.5M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 95% 19.0M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 98% 12.1M 0s\n",
      "  1850K .......... .......... .......                         100% 27.9M=0.2s\n",
      "\n",
      "2019-06-13 08:51:51 (9.14 MB/s) - ‘/root/taxi/data/simple/data.csv’ saved [1922668/1922668]\n",
      "\n",
      "--2019-06-13 08:51:51--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12084 (12K) [text/plain]\n",
      "Saving to: ‘/root/taxi/taxi_utils.py’\n",
      "\n",
      "     0K .......... .                                          100% 15.1M=0.001s\n",
      "\n",
      "2019-06-13 08:51:51 (15.1 MB/s) - ‘/root/taxi/taxi_utils.py’ saved [12084/12084]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# This enables you to run this notebook twice.\n",
    "# There should not be train/eval files at ~/taxi/data, since TFX can handle only single file with version 0.13.0\n",
    "if [ -e ~/taxi/data ]; then\n",
    "    rm -rf ~/taxi/data\n",
    "fi\n",
    "\n",
    "# download taxi data\n",
    "mkdir -p ~/taxi/data/simple\n",
    "mkdir -p ~/taxi/serving_model/taxi_simple\n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv -O ~/taxi/data/simple/data.csv\n",
    "\n",
    "# download \n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py -O ~/taxi/taxi_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
      "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.protobuf import json_format\n",
    "\n",
    "from tfx.components.base.base_component import ComponentOutputs\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.orchestration.tfx_runner import TfxRunner\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import csv_input\n",
    "from tfx.utils.channel import Channel\n",
    "from tfx.utils import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example assumes that the taxi data is stored in ~/taxi/data and the\n",
    "# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n",
    "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
    "_data_root = os.path.join(_taxi_root, 'data/simple')\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(_taxi_root, 'serving_model/taxi_simple')\n",
    "\n",
    "# Directory and data locations.  This example assumes all of the chicago taxi\n",
    "# example code and metadata library is relative to $HOME, but you can store\n",
    "# these files anywhere on your local filesystem.\n",
    "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines')\n",
    "_metadata_db_root = os.path.join(_tfx_root, 'metadata')\n",
    "_log_root = os.path.join(_tfx_root, 'logs')\n",
    "\n",
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "_airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2019, 1, 1),\n",
    "}\n",
    "\n",
    "# Logging overrides\n",
    "logger_overrides = {'log_root': _log_root, 'log_level': logging.INFO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implements the chicago taxi pipeline with TFX.\"\"\"\n",
    "examples = csv_input(_data_root)\n",
    "\n",
    "# Brings data into the pipeline or otherwise joins/converts training data.\n",
    "train_config = example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2)\n",
    "eval_config = example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        train_config,\n",
    "        eval_config\n",
    "    ]))\n",
    "\n",
    "# Create outputs\n",
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root, 'train/')\n",
    "\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root, 'eval/')\n",
    "\n",
    "output_dict = {'examples': Channel(\n",
    "    type_name='ExamplesPath',\n",
    "    static_artifact_collection=[train_examples, eval_examples])}\n",
    "\n",
    "example_outputs = ComponentOutputs(output_dict)\n",
    "\n",
    "example_gen = CsvExampleGen(\n",
    "    input_base=examples, # A Channel of 'ExternalPath' type, it contains path of data source.\n",
    "    output_config=output_config,  # An example_gen_pb2.Output instance, it contains train-eval split ratio.\n",
    "    outputs=example_outputs # dict from name to output channel, it will be stored example_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='train')\n",
    "train_statistics.uri = os.path.join(_data_root, 'train/stats/')\n",
    "\n",
    "eval_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='eval')\n",
    "eval_statistics.uri = os.path.join(_data_root, 'eval/stats/')\n",
    "\n",
    "output_dict = {'output': Channel(\n",
    "    type_name='ExampleStatisticsPath',\n",
    "    static_artifact_collection=[train_statistics, eval_statistics])}\n",
    "\n",
    "statistics_outputs = ComponentOutputs(output_dict)\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    input_data=example_gen.outputs.examples, # A Channel of 'ExamplesPath' type, it is equal to example_outputs\n",
    "    name='Statistics Generator', # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=statistics_outputs # dict from name to output channel, it will be stored statistics_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_schema_path = types.TfxType(type_name='SchemaPath', split='train')\n",
    "train_schema_path.uri = os.path.join(_data_root, 'train/schema/')\n",
    "\n",
    "schema_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='SchemaPath',\n",
    "        # SchemaGen.executor can handle just one SchemaPath.\n",
    "        # Two or more SchemaPaths will cause ValueError\n",
    "        # such as \"ValueError: expected list length of one but got 2\".\n",
    "        static_artifact_collection=[train_schema_path] \n",
    "    )\n",
    "})\n",
    "\n",
    "schema_gen = SchemaGen(\n",
    "    stats=statistics_gen.outputs.output, # A Channel of 'ExampleStatisticsPath' type, it is equal to statistics_outputs\n",
    "    name='Schema Generator',  # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=schema_outputs # dict from name to output channel, it will be stored schema_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"TFX Pipeline\",\n",
    "    pipeline_root=_pipeline_root,\n",
    "    components=[example_gen, statistics_gen, schema_gen]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectRunner(TfxRunner):\n",
    "    \"\"\"Tfx runner on local\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self._config = config or {}\n",
    "    \n",
    "    def run(self, pipeline):\n",
    "        for component in pipeline.components:\n",
    "            self._execute_component(component)\n",
    "            \n",
    "        return pipeline\n",
    "            \n",
    "    def _execute_component(self, component):\n",
    "        input_dict = {key:value.get() for key, value in component.input_dict.items()}\n",
    "        output_dict = {key: value.get() for key, value in component.outputs.get_all().items()}\n",
    "        exec_properties = component.exec_properties\n",
    "        executor = component.executor()\n",
    "        executor.Do(input_dict, output_dict, exec_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-13 08:51:55,399] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"type_name\": {\"stringValue\": \"ExternalPath\"}, \"split\": {\"stringValue\": \"\"}}}, \"artifact_type\": {\"name\": \"ExternalPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}]}\n",
      "[2019-06-13 08:51:55,409] {base_executor.py:74} INFO - Inputs for Executor is: {\"input-base\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"type_name\": {\"stringValue\": \"ExternalPath\"}, \"split\": {\"stringValue\": \"\"}}}, \"artifact_type\": {\"name\": \"ExternalPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}]}\n",
      "[2019-06-13 08:51:55,415] {base_executor.py:76} INFO - Outputs for Executor is: {\"examples\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hashBuckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hashBuckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"}\n",
      "[2019-06-13 08:51:55,420] {base_executor.py:78} INFO - Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hashBuckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hashBuckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"}\n",
      "INFO:tensorflow:Generating examples.\n",
      "[2019-06-13 08:51:55,426] {base_example_gen_executor.py:122} INFO - Generating examples.\n",
      "[2019-06-13 08:51:55,434] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-13 08:51:55,459] {executor.py:70} INFO - Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-13 08:51:56,314] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f2f3dde48c8> ====================\n",
      "[2019-06-13 08:51:56,319] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f2f3dde49d8> ====================\n",
      "[2019-06-13 08:51:56,324] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f2f3dde4a60> ====================\n",
      "[2019-06-13 08:51:56,326] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f2f3dde4ae8> ====================\n",
      "[2019-06-13 08:51:56,332] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f2f3dde4b70> ====================\n",
      "[2019-06-13 08:51:56,339] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f2f3dde4c80> ====================\n",
      "[2019-06-13 08:51:56,340] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f2f3dde4d08> ====================\n",
      "[2019-06-13 08:51:56,363] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f2f3dde4d90> ====================\n",
      "[2019-06-13 08:51:56,369] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f2f3dde4e18> ====================\n",
      "[2019-06-13 08:51:56,373] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f2f3dde5048> ====================\n",
      "[2019-06-13 08:51:56,379] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f2f3dde50d0> ====================\n",
      "[2019-06-13 08:51:56,385] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f2f3dde5158> ====================\n",
      "[2019-06-13 08:51:56,397] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/DoOnce/Read_40)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/InitializeWrite_41)+(ref_PCollection_PCollection_25/Write)))+(ref_PCollection_PCollection_24/Write)\n",
      "[2019-06-13 08:51:56,425] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_InputSourceToExample/ReadFromText/Read_4)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/ParseCSVRecords_6))+(((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/KeyWithVoid_8)+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Precombine))+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Write)))+(ref_PCollection_PCollection_2/Write)\n",
      "[2019-06-13 08:51:57,801] {fn_api_runner.py:437} INFO - Running (InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Read)+((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Merge)+(((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/UnKey_16))+(ref_PCollection_PCollection_8/Write)))\n",
      "[2019-06-13 08:51:57,820] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/DoOnce/Read_18)+((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/InjectDefault_19)+(ref_PCollection_PCollection_10/Write))\n",
      "[2019-06-13 08:51:57,839] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_2/Read)+((ref_AppliedPTransform_InputSourceToExample/ParseCSV/CreateInMemoryDict_20)+(ref_AppliedPTransform_InputSourceToExample/ToTFExample_21)))+((ref_AppliedPTransform_SerializeDeterministically_22)+(((((ref_AppliedPTransform_SplitData/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_25)+(ref_AppliedPTransform_ShuffleSpliteval/AddRandomKeys_53))+(ref_AppliedPTransform_ShuffleSplittrain/AddRandomKeys_27))+((ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/Map(reify_timestamps)_55)+(ShuffleSpliteval/ReshufflePerKey/GroupByKey/Write)))+((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/Map(reify_timestamps)_29)+(ShuffleSplittrain/ReshufflePerKey/GroupByKey/Write))))\n",
      "[2019-06-13 08:52:03,015] {fn_api_runner.py:437} INFO - Running (((ShuffleSplittrain/ReshufflePerKey/GroupByKey/Read)+(((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/FlatMap(restore_timestamps)_34)+(ref_AppliedPTransform_ShuffleSplittrain/RemoveRandomKeys_35))+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WriteBundles_42)))+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Pair_43))+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WindowInto(WindowIntoFn)_44)+(OutputSplittrain/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-13 08:52:03,171] {tfrecordio.py:57} WARNING - Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "[2019-06-13 08:52:03,497] {fn_api_runner.py:437} INFO - Running (OutputSplittrain/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Extract_49)+(ref_PCollection_PCollection_32/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-13 08:52:03,510] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/PreFinalize_50)+(ref_PCollection_PCollection_33/Write))\n",
      "[2019-06-13 08:52:03,527] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/FinalizeWrite_51)\n",
      "[2019-06-13 08:52:03,538] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-13 08:52:03,644] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-13 08:52:03,667] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/DoOnce/Read_66)+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/InitializeWrite_67)+(ref_PCollection_PCollection_43/Write)))+(ref_PCollection_PCollection_42/Write)\n",
      "[2019-06-13 08:52:03,684] {fn_api_runner.py:437} INFO - Running ((ShuffleSpliteval/ReshufflePerKey/GroupByKey/Read)+((ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/FlatMap(restore_timestamps)_60)+(ref_AppliedPTransform_ShuffleSpliteval/RemoveRandomKeys_61)))+(((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WriteBundles_68)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Pair_69))+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WindowInto(WindowIntoFn)_70)+(OutputSpliteval/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-13 08:52:03,966] {fn_api_runner.py:437} INFO - Running ((OutputSpliteval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Extract_75))+(ref_PCollection_PCollection_50/Write)\n",
      "[2019-06-13 08:52:03,978] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/PreFinalize_76)+(ref_PCollection_PCollection_51/Write))\n",
      "[2019-06-13 08:52:03,995] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/FinalizeWrite_77)\n",
      "[2019-06-13 08:52:04,006] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-13 08:52:04,112] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Examples generated.\n",
      "[2019-06-13 08:52:04,138] {base_example_gen_executor.py:145} INFO - Examples generated.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-13 08:52:04,141] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}]}\n",
      "[2019-06-13 08:52:04,145] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExamplesPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}]}\n",
      "[2019-06-13 08:52:04,149] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact\": {\"uri\": \"/root/taxi/data/simple/train/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}, {\"artifact\": {\"uri\": \"/root/taxi/data/simple/eval/stats/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}}, \"artifact_type\": {\"name\": \"ExampleStatisticsPath\", \"properties\": {\"state\": \"STRING\", \"span\": \"INT\", \"name\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\"}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {}\n",
      "[2019-06-13 08:52:04,152] {base_executor.py:78} INFO - Execution properties for Executor is: {}\n",
      "[2019-06-13 08:52:04,156] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Generating statistics for split eval\n",
      "[2019-06-13 08:52:04,165] {executor.py:62} INFO - Generating statistics for split eval\n",
      "INFO:tensorflow:Generating statistics for split train\n",
      "[2019-06-13 08:52:04,855] {executor.py:62} INFO - Generating statistics for split train\n",
      "INFO:tensorflow:Statistics written to /root/taxi/data/simple/train/stats/.\n",
      "[2019-06-13 08:52:05,673] {executor.py:78} INFO - Statistics written to /root/taxi/data/simple/train/stats/.\n",
      "[2019-06-13 08:52:08,794] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f2f3dde48c8> ====================\n",
      "[2019-06-13 08:52:08,798] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f2f3dde49d8> ====================\n",
      "[2019-06-13 08:52:08,802] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f2f3dde4a60> ====================\n",
      "[2019-06-13 08:52:08,812] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f2f3dde4ae8> ====================\n",
      "[2019-06-13 08:52:08,815] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f2f3dde4b70> ====================\n",
      "[2019-06-13 08:52:08,820] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f2f3dde4c80> ====================\n",
      "[2019-06-13 08:52:08,824] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f2f3dde4d08> ====================\n",
      "[2019-06-13 08:52:08,833] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f2f3dde4d90> ====================\n",
      "[2019-06-13 08:52:08,836] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f2f3dde4e18> ====================\n",
      "[2019-06-13 08:52:08,839] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f2f3dde5048> ====================\n",
      "[2019-06-13 08:52:08,842] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f2f3dde50d0> ====================\n",
      "[2019-06-13 08:52:08,845] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f2f3dde5158> ====================\n",
      "[2019-06-13 08:52:08,872] {fn_api_runner.py:437} INFO - Running (((((((ref_AppliedPTransform_ReadData.eval/Read_3)+(ref_AppliedPTransform_DecodeData.eval/ParseTFExamples_5))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/KeyWithVoid_8))+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_12)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0))+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_13)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write))))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_32))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0))+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_33)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_35))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-13 08:52:12,181] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge)+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_43)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_53)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write))))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_55)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "[2019-06-13 08:52:12,213] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_63)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1)))\n",
      "[2019-06-13 08:52:12,234] {fn_api_runner.py:437} INFO - Running ((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_52))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)\n",
      "[2019-06-13 08:52:12,255] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_65))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "[2019-06-13 08:52:12,274] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_21)))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_22))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1))\n",
      "[2019-06-13 08:52:12,542] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write))\n",
      "[2019-06-13 08:52:12,694] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs)))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0))\n",
      "[2019-06-13 08:52:13,017] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "[2019-06-13 08:52:13,034] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_74)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_77)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)))\n",
      "[2019-06-13 08:52:13,075] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_85)))+(ref_PCollection_PCollection_51/Write)\n",
      "[2019-06-13 08:52:13,096] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadData.train/Read_106)+(((((ref_AppliedPTransform_DecodeData.train/ParseTFExamples_108)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/KeyWithVoid_111))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_115))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_135))+((((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_136)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_138))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write))))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_116)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-13 08:52:18,567] {fn_api_runner.py:437} INFO - Running ((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_146))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_156)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_158)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "[2019-06-13 08:52:18,602] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_166)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1))\n",
      "[2019-06-13 08:52:18,618] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_155)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)\n",
      "[2019-06-13 08:52:18,647] {fn_api_runner.py:437} INFO - Running ((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_124)))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_125))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1)\n",
      "[2019-06-13 08:52:18,911] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "[2019-06-13 08:52:19,075] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs)))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0))\n",
      "[2019-06-13 08:52:19,571] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_168))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "[2019-06-13 08:52:19,588] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "[2019-06-13 08:52:19,604] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_177)+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_180)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)))\n",
      "[2019-06-13 08:52:19,635] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_188)+(ref_PCollection_PCollection_115/Write)))\n",
      "[2019-06-13 08:52:19,656] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_190)+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_191)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_192))+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/Map(<lambda at iobase.py:984>)_199)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WindowInto(WindowIntoFn)_200)+(WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Write))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-13 08:52:19,691] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_87)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_88)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_89)))+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/Map(<lambda at iobase.py:984>)_96)+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WindowInto(WindowIntoFn)_97)+(WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Write)))\n",
      "[2019-06-13 08:52:19,726] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/DoOnce/Read_94)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/InitializeWrite_95))+(ref_PCollection_PCollection_55/Write))+(ref_PCollection_PCollection_56/Write)\n",
      "[2019-06-13 08:52:19,747] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WriteBundles_102))+(ref_PCollection_PCollection_62/Write)\n",
      "[2019-06-13 08:52:19,764] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/DoOnce/Read_197)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/InitializeWrite_198)+(ref_PCollection_PCollection_120/Write)))+(ref_PCollection_PCollection_119/Write)\n",
      "[2019-06-13 08:52:19,783] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WriteBundles_205))+(ref_PCollection_PCollection_126/Write)\n",
      "[2019-06-13 08:52:19,803] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/PreFinalize_103)+(ref_PCollection_PCollection_63/Write))\n",
      "[2019-06-13 08:52:19,823] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/PreFinalize_206))+(ref_PCollection_PCollection_127/Write)\n",
      "[2019-06-13 08:52:19,840] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/FinalizeWrite_207)\n",
      "[2019-06-13 08:52:19,853] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-13 08:52:19,958] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-13 08:52:19,981] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/FinalizeWrite_104)\n",
      "[2019-06-13 08:52:19,994] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-13 08:52:20,097] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Infering schema from statistics.\n",
      "[2019-06-13 08:52:20,124] {executor.py:62} INFO - Infering schema from statistics.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "[2019-06-13 08:52:20,126] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:tensorflow:Schema written to /root/taxi/data/simple/train/schema/schema.pbtxt.\n",
      "[2019-06-13 08:52:20,147] {executor.py:66} INFO - Schema written to /root/taxi/data/simple/train/schema/schema.pbtxt.\n"
     ]
    }
   ],
   "source": [
    "pipeline = DirectRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/taxi/data/simple/:\r\n",
      "total 1.9M\r\n",
      "1.9M -rw-r--r-- 1 root root 1.9M Jun 13 08:51 data.csv\r\n",
      "4.0K drwxr-xr-x 3 root root 4.0K Jun 13 08:52 eval\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 13 08:52 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/eval:\r\n",
      "total 208K\r\n",
      "204K -rw-r--r-- 1 root root 201K Jun 13 08:52 data_tfrecord-00000-of-00001.gz\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 13 08:52 stats\r\n",
      "\r\n",
      "/root/taxi/data/simple/eval/stats:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 17K Jun 13 08:52 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/train:\r\n",
      "total 416K\r\n",
      "408K -rw-r--r-- 1 root root 406K Jun 13 08:52 data_tfrecord-00000-of-00001.gz\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 13 08:52 schema\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 13 08:52 stats\r\n",
      "\r\n",
      "/root/taxi/data/simple/train/schema:\r\n",
      "total 8.0K\r\n",
      "8.0K -rw-r--r-- 1 root root 4.5K Jun 13 08:52 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/train/stats:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 18K Jun 13 08:52 stats_tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!ls -Rlhs /root/taxi/data/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\r\n",
      "  name: \"trip_start_hour\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"payment_type\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: BYTES\r\n",
      "  domain: \"payment_type\"\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_seconds\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"pickup_longitude\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"pickup_census_tract\"\r\n",
      "  type: BYTES\r\n",
      "  presence {\r\n",
      "    min_count: 0\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_miles\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_start_day\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_start_timestamp\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_start_month\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"tips\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"pickup_latitude\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"fare\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"dropoff_community_area\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"dropoff_census_tract\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"pickup_community_area\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"dropoff_latitude\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"company\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: BYTES\r\n",
      "  domain: \"company\"\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"dropoff_longitude\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "string_domain {\r\n",
      "  name: \"payment_type\"\r\n",
      "  value: \"Cash\"\r\n",
      "  value: \"Credit Card\"\r\n",
      "  value: \"Dispute\"\r\n",
      "  value: \"No Charge\"\r\n",
      "  value: \"Pcard\"\r\n",
      "  value: \"Prcard\"\r\n",
      "  value: \"Unknown\"\r\n",
      "}\r\n",
      "string_domain {\r\n",
      "  name: \"company\"\r\n",
      "  value: \"0118 - 42111 Godfrey S.Awir\"\r\n",
      "  value: \"0694 - 59280 Chinesco Trans Inc\"\r\n",
      "  value: \"2092 - 61288 Sbeih company\"\r\n",
      "  value: \"2192 - 73487 Zeymane Corp\"\r\n",
      "  value: \"2733 - 74600 Benny Jona\"\r\n",
      "  value: \"2823 - 73307 Seung Lee\"\r\n",
      "  value: \"3011 - 66308 JBL Cab Inc.\"\r\n",
      "  value: \"3094 - 24059 G.L.B. Cab Co\"\r\n",
      "  value: \"3152 - 97284 Crystal Abernathy\"\r\n",
      "  value: \"3201 - C&D Cab Co Inc\"\r\n",
      "  value: \"3201 - CID Cab Co Inc\"\r\n",
      "  value: \"3253 - 91138 Gaither Cab Co.\"\r\n",
      "  value: \"3319 - CD Cab Co\"\r\n",
      "  value: \"3385 - 23210 Eman Cab\"\r\n",
      "  value: \"3385 - Eman Cab\"\r\n",
      "  value: \"3623 - 72222 Arrington Enterprises\"\r\n",
      "  value: \"3897 - Ilie Malec\"\r\n",
      "  value: \"4053 - 40193 Adwar H. Nikola\"\r\n",
      "  value: \"4053 - Adwar H. Nikola\"\r\n",
      "  value: \"4197 - 41842 Royal Star\"\r\n",
      "  value: \"4197 - Royal Star\"\r\n",
      "  value: \"4615 - 83503 Tyrone Henderson\"\r\n",
      "  value: \"4623 - Jay Kim\"\r\n",
      "  value: \"5006 - 39261 Salifu Bawa\"\r\n",
      "  value: \"5006 - Salifu Bawa\"\r\n",
      "  value: \"5074 - 54002 Ahzmi Inc\"\r\n",
      "  value: \"5074 - Ahzmi Inc\"\r\n",
      "  value: \"5129 - 87128\"\r\n",
      "  value: \"5129 - 98755 Mengisti Taxi\"\r\n",
      "  value: \"5129 - Mengisti Taxi\"\r\n",
      "  value: \"5724 - KYVI Cab Inc\"\r\n",
      "  value: \"585 - 88805 Valley Cab Co\"\r\n",
      "  value: \"5864 - 73614 Thomas Owusu\"\r\n",
      "  value: \"5864 - Thomas Owusu\"\r\n",
      "  value: \"5874 - 73628 Sergey Cab Corp.\"\r\n",
      "  value: \"5997 - 65283 AW Services Inc.\"\r\n",
      "  value: \"6488 - 83287 Zuha Taxi\"\r\n",
      "  value: \"6742 - 83735 Tasha ride inc\"\r\n",
      "  value: \"Blue Ribbon Taxi Association Inc.\"\r\n",
      "  value: \"C & D Cab Co Inc\"\r\n",
      "  value: \"Chicago Elite Cab Corp.\"\r\n",
      "  value: \"Chicago Elite Cab Corp. (Chicago Carriag\"\r\n",
      "  value: \"Chicago Medallion Leasing INC\"\r\n",
      "  value: \"Chicago Medallion Management\"\r\n",
      "  value: \"Choice Taxi Association\"\r\n",
      "  value: \"Dispatch Taxi Affiliation\"\r\n",
      "  value: \"KOAM Taxi Association\"\r\n",
      "  value: \"Northwest Management LLC\"\r\n",
      "  value: \"Taxi Affiliation Services\"\r\n",
      "  value: \"Top Cab Affiliation\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat /root/taxi/data/simple/train/schema/schema.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`schema.pbtxt` is only simple text file, we are going to;\n",
    "\n",
    "1. get the path of `schema.pbtxt`from `schema_gen`\n",
    "2. parse `schema.pbtxt` into schema (protobuf)\n",
    "3. visualize schema with [tfdv](https://www.tensorflow.org/tfx/data_validation/get_started)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the path of `schema.pbtxt`\n",
    "def get_schema_directory(schema_gen):\n",
    "    output_dict = {key: value.get() for key, value in schema_gen.outputs.get_all().items()}\n",
    "    input_dict = {key:value.get() for key, value in schema_gen.input_dict.items()}\n",
    "    split_to_instance = {x.split: x for x in input_dict['stats']}\n",
    "    directory = types.get_split_uri(output_dict['output'], 'train')\n",
    "    return directory\n",
    "\n",
    "schema_directory = get_schema_directory(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. parse schema.pbtxt\n",
    "\n",
    "from pathlib import Path\n",
    "from google.protobuf.text_format import Parse\n",
    "from google.protobuf.message import Message\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "def parse_schema_proto_string(schema_directory):\n",
    "    path = Path(schema_directory)\n",
    "    for filepath in path.glob('*'):\n",
    "        with open(str(filepath), 'r') as file: # since we are using python 3.5, not 3.6+\n",
    "            schema_proto_string = file.read()\n",
    "            schema = schema_pb2.Schema()\n",
    "            text_format.Parse(schema_proto_string, schema)\n",
    "            return schema\n",
    "\n",
    "schema = parse_schema_proto_string(schema_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Valency</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'trip_start_hour'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'payment_type'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>'payment_type'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_seconds'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'pickup_longitude'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'pickup_census_tract'</th>\n",
       "      <td>BYTES</td>\n",
       "      <td>optional</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_miles'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_start_day'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_start_timestamp'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_start_month'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'tips'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'pickup_latitude'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'fare'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'dropoff_community_area'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'dropoff_census_tract'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'pickup_community_area'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'dropoff_latitude'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'company'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>'company'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'dropoff_longitude'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Type  Presence Valency          Domain\n",
       "Feature name                                                      \n",
       "'trip_start_hour'         INT     required  single  -             \n",
       "'payment_type'            STRING  required  single  'payment_type'\n",
       "'trip_seconds'            FLOAT   optional  single  -             \n",
       "'pickup_longitude'        FLOAT   required  single  -             \n",
       "'pickup_census_tract'     BYTES   optional          -             \n",
       "'trip_miles'              FLOAT   required  single  -             \n",
       "'trip_start_day'          INT     required  single  -             \n",
       "'trip_start_timestamp'    INT     required  single  -             \n",
       "'trip_start_month'        INT     required  single  -             \n",
       "'tips'                    FLOAT   required  single  -             \n",
       "'pickup_latitude'         FLOAT   required  single  -             \n",
       "'fare'                    FLOAT   required  single  -             \n",
       "'dropoff_community_area'  FLOAT   optional  single  -             \n",
       "'dropoff_census_tract'    FLOAT   optional  single  -             \n",
       "'pickup_community_area'   INT     required  single  -             \n",
       "'dropoff_latitude'        FLOAT   optional  single  -             \n",
       "'company'                 STRING  optional  single  'company'     \n",
       "'dropoff_longitude'       FLOAT   optional  single  -             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'payment_type'</th>\n",
       "      <td>'Cash', 'Credit Card', 'Dispute', 'No Charge', 'Pcard', 'Prcard', 'Unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'company'</th>\n",
       "      <td>'0118 - 42111 Godfrey S.Awir', '0694 - 59280 Chinesco Trans Inc', '2092 - 61288 Sbeih company', '2192 - 73487 Zeymane Corp', '2733 - 74600 Benny Jona', '2823 - 73307 Seung Lee', '3011 - 66308 JBL Cab Inc.', '3094 - 24059 G.L.B. Cab Co', '3152 - 97284 Crystal Abernathy', '3201 - C&amp;D Cab Co Inc', '3201 - CID Cab Co Inc', '3253 - 91138 Gaither Cab Co.', '3319 - CD Cab Co', '3385 - 23210 Eman Cab', '3385 - Eman Cab', '3623 - 72222 Arrington Enterprises', '3897 - Ilie Malec', '4053 - 40193 Adwar H. Nikola', '4053 - Adwar H. Nikola', '4197 - 41842 Royal Star', '4197 - Royal Star', '4615 - 83503 Tyrone Henderson', '4623 - Jay Kim', '5006 - 39261 Salifu Bawa', '5006 - Salifu Bawa', '5074 - 54002 Ahzmi Inc', '5074 - Ahzmi Inc', '5129 - 87128', '5129 - 98755 Mengisti Taxi', '5129 - Mengisti Taxi', '5724 - KYVI Cab Inc', '585 - 88805 Valley Cab Co', '5864 - 73614 Thomas Owusu', '5864 - Thomas Owusu', '5874 - 73628 Sergey Cab Corp.', '5997 - 65283 AW Services Inc.', '6488 - 83287 Zuha Taxi', '6742 - 83735 Tasha ride inc', 'Blue Ribbon Taxi Association Inc.', 'C &amp; D Cab Co Inc', 'Chicago Elite Cab Corp.', 'Chicago Elite Cab Corp. (Chicago Carriag', 'Chicago Medallion Leasing INC', 'Chicago Medallion Management', 'Choice Taxi Association', 'Dispatch Taxi Affiliation', 'KOAM Taxi Association', 'Northwest Management LLC', 'Taxi Affiliation Services', 'Top Cab Affiliation'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Values\n",
       "Domain                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "'payment_type'  'Cash', 'Credit Card', 'Dispute', 'No Charge', 'Pcard', 'Prcard', 'Unknown'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "'company'       '0118 - 42111 Godfrey S.Awir', '0694 - 59280 Chinesco Trans Inc', '2092 - 61288 Sbeih company', '2192 - 73487 Zeymane Corp', '2733 - 74600 Benny Jona', '2823 - 73307 Seung Lee', '3011 - 66308 JBL Cab Inc.', '3094 - 24059 G.L.B. Cab Co', '3152 - 97284 Crystal Abernathy', '3201 - C&D Cab Co Inc', '3201 - CID Cab Co Inc', '3253 - 91138 Gaither Cab Co.', '3319 - CD Cab Co', '3385 - 23210 Eman Cab', '3385 - Eman Cab', '3623 - 72222 Arrington Enterprises', '3897 - Ilie Malec', '4053 - 40193 Adwar H. Nikola', '4053 - Adwar H. Nikola', '4197 - 41842 Royal Star', '4197 - Royal Star', '4615 - 83503 Tyrone Henderson', '4623 - Jay Kim', '5006 - 39261 Salifu Bawa', '5006 - Salifu Bawa', '5074 - 54002 Ahzmi Inc', '5074 - Ahzmi Inc', '5129 - 87128', '5129 - 98755 Mengisti Taxi', '5129 - Mengisti Taxi', '5724 - KYVI Cab Inc', '585 - 88805 Valley Cab Co', '5864 - 73614 Thomas Owusu', '5864 - Thomas Owusu', '5874 - 73628 Sergey Cab Corp.', '5997 - 65283 AW Services Inc.', '6488 - 83287 Zuha Taxi', '6742 - 83735 Tasha ride inc', 'Blue Ribbon Taxi Association Inc.', 'C & D Cab Co Inc', 'Chicago Elite Cab Corp.', 'Chicago Elite Cab Corp. (Chicago Carriag', 'Chicago Medallion Leasing INC', 'Chicago Medallion Management', 'Choice Taxi Association', 'Dispatch Taxi Affiliation', 'KOAM Taxi Association', 'Northwest Management LLC', 'Taxi Affiliation Services', 'Top Cab Affiliation'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. visualize schema with tfdv\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "tfdv.display_schema(schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
