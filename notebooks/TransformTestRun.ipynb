{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Test Run\n",
    "\n",
    "## Set up\n",
    "\n",
    "TFX requires apache-airflow and docker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-airflow[gcp] in /usr/local/lib/python3.5/dist-packages (1.10.3)\n",
      "\u001b[33m  WARNING: apache-airflow 1.10.3 does not provide the extra 'gcp'\u001b[0m\n",
      "Requirement already satisfied: docker in /usr/local/lib/python3.5/dist-packages (4.0.1)\n",
      "Requirement already satisfied: tfx in /usr/local/lib/python3.5/dist-packages (0.13.0)\n",
      "Requirement already satisfied: flask-wtf<0.15,>=0.14.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.2)\n",
      "Requirement already satisfied: flask-admin==1.5.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.3)\n",
      "Requirement already satisfied: lxml>=4.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.3.4)\n",
      "Requirement already satisfied: croniter<0.4,>=0.3.17 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.3.30)\n",
      "Requirement already satisfied: iso8601>=0.1.12 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.1.12)\n",
      "Requirement already satisfied: werkzeug<0.15.0,>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: psutil<6.0.0,>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (5.6.3)\n",
      "Requirement already satisfied: python-daemon<2.2,>=2.1.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.2)\n",
      "Requirement already satisfied: gitpython>=2.0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.1.11)\n",
      "Requirement already satisfied: dill<0.3,>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.9)\n",
      "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2)\n",
      "Requirement already satisfied: thrift>=0.9.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.11.0)\n",
      "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.14.1)\n",
      "Requirement already satisfied: gunicorn<20.0,>=19.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (19.9.0)\n",
      "Requirement already satisfied: flask-login<0.5,>=0.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.8.0)\n",
      "Requirement already satisfied: zope.deprecation<5.0,>=4.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.4.0)\n",
      "Requirement already satisfied: markdown<3.0,>=2.5.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.6.11)\n",
      "Requirement already satisfied: alembic<1.0,>=0.9 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.9.10)\n",
      "Requirement already satisfied: flask-caching<1.4.0,>=1.3.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.3.3)\n",
      "Requirement already satisfied: json-merge-patch==0.2 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2)\n",
      "Requirement already satisfied: funcsigs==1.0.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.0)\n",
      "Requirement already satisfied: pendulum==1.4.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.4.4)\n",
      "Requirement already satisfied: tenacity==4.12.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (4.12.0)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.22.0)\n",
      "Requirement already satisfied: setproctitle<2,>=1.1.8 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.1.10)\n",
      "Requirement already satisfied: sqlalchemy<1.3.0,>=1.1.15 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.2.19)\n",
      "Requirement already satisfied: configparser<3.6.0,>=3.5.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (3.5.3)\n",
      "Requirement already satisfied: flask-swagger==0.2.13 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.2.13)\n",
      "Requirement already satisfied: tzlocal>=1.4 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.5.1)\n",
      "Requirement already satisfied: flask<2.0,>=1.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.0.3)\n",
      "Requirement already satisfied: tabulate<0.9,>=0.7.5 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.8.3)\n",
      "Requirement already satisfied: flask-appbuilder==1.12.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (1.12.3)\n",
      "Requirement already satisfied: future<0.17,>=0.16.0 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.16.0)\n",
      "Requirement already satisfied: pandas<1.0.0,>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (0.24.2)\n",
      "Requirement already satisfied: jinja2<=2.10.0,>=2.7.3 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.10)\n",
      "Requirement already satisfied: pygments<3.0,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-airflow[gcp]) (2.3.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.5/dist-packages (from docker) (0.56.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.5/dist-packages (from docker) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-data-validation<0.14,>=0.13.1 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.1)\n",
      "Requirement already satisfied: tensorflow-transform<0.14,>=0.13 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.5/dist-packages (from tfx) (1.7.9)\n",
      "Requirement already satisfied: absl-py<1,>=0.1.6 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.7.1)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: ml-metadata<0.14,>=0.13.2 in /usr/local/lib/python3.5/dist-packages (from tfx) (0.13.2)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.12 in /usr/local/lib/python3.5/dist-packages (from tfx) (2.13.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.7 in /usr/local/lib/python3.5/dist-packages (from tfx) (3.7.1)\n",
      "Requirement already satisfied: WTForms in /usr/local/lib/python3.5/dist-packages (from flask-wtf<0.15,>=0.14.2->apache-airflow[gcp]) (2.2.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (41.0.1)\n",
      "Requirement already satisfied: lockfile>=0.10 in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: docutils in /usr/local/lib/python3.5/dist-packages (from python-daemon<2.2,>=2.1.1->apache-airflow[gcp]) (0.14)\n",
      "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.4)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.5/dist-packages (from alembic<1.0,>=0.9->apache-airflow[gcp]) (1.0.12)\n",
      "Requirement already satisfied: ordereddict in /usr/local/lib/python3.5/dist-packages (from funcsigs==1.0.0->apache-airflow[gcp]) (1.1)\n",
      "Requirement already satisfied: pytzdata>=2018.3.0.0 in /usr/local/lib/python3.5/dist-packages (from pendulum==1.4.4->apache-airflow[gcp]) (2019.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->apache-airflow[gcp]) (1.25.3)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.5/dist-packages (from flask-swagger==0.2.13->apache-airflow[gcp]) (3.13)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.5/dist-packages (from tzlocal>=1.4->apache-airflow[gcp]) (2019.1)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.5/dist-packages (from flask<2.0,>=1.0->apache-airflow[gcp]) (7.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.5/dist-packages (from flask<2.0,>=1.0->apache-airflow[gcp]) (1.1.0)\n",
      "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.3 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.4.0)\n",
      "Requirement already satisfied: Flask-Babel<1,>=0.11.1 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.12.2)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask-OpenID<2,>=1.2.5 in /usr/local/lib/python3.5/dist-packages (from flask-appbuilder==1.12.3->apache-airflow[gcp]) (1.2.5)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas<1.0.0,>=0.17.1->apache-airflow[gcp]) (1.16.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.5/dist-packages (from jinja2<=2.10.0,>=2.7.3->apache-airflow[gcp]) (1.1.1)\n",
      "Requirement already satisfied: tensorflow-metadata<0.14,>=0.12.1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.0)\n",
      "Requirement already satisfied: scikit-learn<1,>=0.18 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.21.2)\n",
      "Requirement already satisfied: IPython>=5.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (7.5.0)\n",
      "Requirement already satisfied: joblib<1,>=0.12 in /usr/local/lib/python3.5/dist-packages (from tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.2)\n",
      "Requirement already satisfied: pydot<1.3,>=1.2.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-transform<0.14,>=0.13->tfx) (1.2.4)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.3)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.12.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.6.3)\n",
      "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.0.0)\n",
      "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.1.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.5/dist-packages (from tensorflow-model-analysis<0.14,>=0.13.2->tfx) (7.4.2)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.5.4)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (2.0.0)\n",
      "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.9.0)\n",
      "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.21.24)\n",
      "Requirement already satisfied: grpcio<2,>=1.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.20.1)\n",
      "Requirement already satisfied: pyarrow<0.14.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.13.0)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.0.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7)\n",
      "Requirement already satisfied: google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.7.4)\n",
      "Requirement already satisfied: google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (1.6.1)\n",
      "Requirement already satisfied: google-cloud-bigtable<0.33.0,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.32.2)\n",
      "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (3.1.1)\n",
      "Requirement already satisfied: google-cloud-pubsub<0.40.0,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.39.1)\n",
      "Requirement already satisfied: google-cloud-core<0.30.0,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.29.1)\n",
      "Requirement already satisfied: google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]<3,>=2.12->tfx) (0.5.28)\n",
      "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from gitdb2>=2.0.0->gitpython>=2.0.2->apache-airflow[gcp]) (2.0.5)\n",
      "Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.5/dist-packages (from Flask-Babel<1,>=0.11.1->flask-appbuilder==1.12.3->apache-airflow[gcp]) (2.7.0)\n",
      "Requirement already satisfied: python3-openid>=2.0 in /usr/local/lib/python3.5/dist-packages (from Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (3.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.5/dist-packages (from tensorflow-metadata<0.14,>=0.12.1->tensorflow-data-validation<0.14,>=0.13.1->tfx) (1.6.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.7.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.13.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (2.0.9)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.5/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (4.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot<1.3,>=1.2.0->tensorflow-transform<0.14,>=0.13->tfx) (2.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (0.2.5)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (4.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.1.0)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.3)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.5.0)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.5/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.7.8)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.4.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.5/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.12->tfx) (0.6.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.5/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.12->tfx) (5.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.12->tfx) (0.4.5)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.6.0 in /usr/local/lib/python3.5/dist-packages (from google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.11.1)\n",
      "Requirement already satisfied: google-resumable-media>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.3.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.12dev,>=0.11.4 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigtable<0.33.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.11.4)\n",
      "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.5/dist-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (0.15)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.5/dist-packages (from python3-openid>=2.0->Flask-OpenID<2,>=1.2.5->flask-appbuilder==1.12.3->apache-airflow[gcp]) (0.6.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.5/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from jedi>=0.10->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.4.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.1.7)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.2->IPython>=5.0->tensorflow-data-validation<0.14,>=0.13.1->tfx) (0.2.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.5/dist-packages (from ipykernel->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (6.0.2)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.5/dist-packages (from ipykernel->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (5.2.4)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.5/dist-packages (from qtconsole->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (4.4.0)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.4.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.4.2)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.6.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (18.0.1)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.8.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (3.0.1)\n",
      "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.5/dist-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.12->tfx) (1.5)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.5/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.5.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (0.15.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.5/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.14,>=0.13.2->tfx) (19.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'apache-airflow[gcp]' docker tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use TFX version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tfx\n",
    "tfx.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX requires TensorFlow >= 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX supports Python 3.5 from version 0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 (default, Nov 12 2018, 13:43:14) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-06-14 02:32:08--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1922668 (1.8M) [text/plain]\n",
      "Saving to: ‘/root/taxi/data/simple/data.csv’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2% 3.18M 1s\n",
      "    50K .......... .......... .......... .......... ..........  5% 3.59M 1s\n",
      "   100K .......... .......... .......... .......... ..........  7% 6.40M 0s\n",
      "   150K .......... .......... .......... .......... .......... 10% 5.24M 0s\n",
      "   200K .......... .......... .......... .......... .......... 13% 4.28M 0s\n",
      "   250K .......... .......... .......... .......... .......... 15% 6.39M 0s\n",
      "   300K .......... .......... .......... .......... .......... 18% 3.01M 0s\n",
      "   350K .......... .......... .......... .......... .......... 21% 11.6M 0s\n",
      "   400K .......... .......... .......... .......... .......... 23% 6.62M 0s\n",
      "   450K .......... .......... .......... .......... .......... 26% 4.89M 0s\n",
      "   500K .......... .......... .......... .......... .......... 29% 33.2M 0s\n",
      "   550K .......... .......... .......... .......... .......... 31% 4.05M 0s\n",
      "   600K .......... .......... .......... .......... .......... 34% 13.6M 0s\n",
      "   650K .......... .......... .......... .......... .......... 37% 9.03M 0s\n",
      "   700K .......... .......... .......... .......... .......... 39% 10.2M 0s\n",
      "   750K .......... .......... .......... .......... .......... 42% 7.47M 0s\n",
      "   800K .......... .......... .......... .......... .......... 45% 5.03M 0s\n",
      "   850K .......... .......... .......... .......... .......... 47% 11.3M 0s\n",
      "   900K .......... .......... .......... .......... .......... 50% 5.42M 0s\n",
      "   950K .......... .......... .......... .......... .......... 53% 45.8M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 55% 3.77M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 58% 24.5M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 61% 7.57M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 63% 6.52M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 66% 6.66M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 69% 28.9M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 71% 7.15M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 74% 12.6M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 77% 6.22M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 79% 72.0M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 82% 9.82M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 85% 22.6M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 87% 7.05M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 90% 26.5M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 93% 9.49M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 95% 19.9M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 98% 9.14M 0s\n",
      "  1850K .......... .......... .......                         100% 45.7M=0.2s\n",
      "\n",
      "2019-06-14 02:32:09 (7.42 MB/s) - ‘/root/taxi/data/simple/data.csv’ saved [1922668/1922668]\n",
      "\n",
      "--2019-06-14 02:32:09--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12084 (12K) [text/plain]\n",
      "Saving to: ‘/root/taxi/taxi_utils.py’\n",
      "\n",
      "     0K .......... .                                          100% 4.02M=0.003s\n",
      "\n",
      "2019-06-14 02:32:09 (4.02 MB/s) - ‘/root/taxi/taxi_utils.py’ saved [12084/12084]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# This enables you to run this notebook twice.\n",
    "# There should not be train/eval files at ~/taxi/data, since TFX can handle only single file with version 0.13.0\n",
    "if [ -e ~/taxi/data ]; then\n",
    "    rm -rf ~/taxi/data\n",
    "fi\n",
    "\n",
    "# download taxi data\n",
    "mkdir -p ~/taxi/data/simple\n",
    "mkdir -p ~/taxi/serving_model/taxi_simple\n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv -O ~/taxi/data/simple/data.csv\n",
    "\n",
    "# download \n",
    "wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/taxi_utils.py -O ~/taxi/taxi_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
      "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.protobuf import json_format\n",
    "\n",
    "from tfx.components.base.base_component import ComponentOutputs\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.orchestration.tfx_runner import TfxRunner\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import csv_input\n",
    "from tfx.utils.channel import Channel\n",
    "from tfx.utils import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example assumes that the taxi data is stored in ~/taxi/data and the\n",
    "# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n",
    "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
    "_data_root = os.path.join(_taxi_root, 'data/simple')\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "_taxi_module_file = os.path.join(_taxi_root, 'taxi_utils.py')\n",
    "\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(_taxi_root, 'serving_model/taxi_simple')\n",
    "\n",
    "# Directory and data locations.  This example assumes all of the chicago taxi\n",
    "# example code and metadata library is relative to $HOME, but you can store\n",
    "# these files anywhere on your local filesystem.\n",
    "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines')\n",
    "_metadata_db_root = os.path.join(_tfx_root, 'metadata')\n",
    "_log_root = os.path.join(_tfx_root, 'logs')\n",
    "\n",
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "_airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2019, 1, 1),\n",
    "}\n",
    "\n",
    "# Logging overrides\n",
    "logger_overrides = {'log_root': _log_root, 'log_level': logging.INFO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implements the chicago taxi pipeline with TFX.\"\"\"\n",
    "examples = csv_input(_data_root)\n",
    "\n",
    "# Brings data into the pipeline or otherwise joins/converts training data.\n",
    "train_config = example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2)\n",
    "eval_config = example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        train_config,\n",
    "        eval_config\n",
    "    ]))\n",
    "\n",
    "# Create outputs\n",
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root, 'csv_example_gen/train/')\n",
    "\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root, 'csv_example_gen/eval/')\n",
    "\n",
    "example_outputs = ComponentOutputs({\n",
    "    'examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    ),\n",
    "    'training_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples]\n",
    "    ),\n",
    "    'eval_examples': Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[eval_examples]\n",
    "    ),    \n",
    "})\n",
    "\n",
    "example_gen = CsvExampleGen(\n",
    "    input_base=examples, # A Channel of 'ExternalPath' type, it contains path of data source.\n",
    "    output_config=output_config,  # An example_gen_pb2.Output instance, it contains train-eval split ratio.\n",
    "    outputs=example_outputs # dict from name to output channel, it will be stored example_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='train')\n",
    "train_statistics.uri = os.path.join(_data_root, 'statistics_gen/train/')\n",
    "\n",
    "eval_statistics = types.TfxType(type_name='ExampleStatisticsPath', split='eval')\n",
    "eval_statistics.uri = os.path.join(_data_root, 'statistics_gen/eval/')\n",
    "\n",
    "statistics_outputs = ComponentOutputs({\n",
    "    'output': Channel(\n",
    "        type_name='ExampleStatisticsPath',\n",
    "        static_artifact_collection=[train_statistics, eval_statistics]\n",
    "    )\n",
    "})\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    input_data=example_gen.outputs.examples, # A Channel of 'ExamplesPath' type, it is equal to example_outputs\n",
    "    name='Statistics Generator', # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=statistics_outputs # dict from name to output channel, it will be stored statistics_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs\n",
    "train_schema_path = types.TfxType(type_name='SchemaPath', split='train')\n",
    "train_schema_path.uri = os.path.join(_data_root, 'schema_gen/')\n",
    "\n",
    "# NOTE: SchemaGen.executor can handle JUST ONE SchemaPath.\n",
    "# Two or more SchemaPaths will cause ValueError\n",
    "# such as \"ValueError: expected list length of one but got 2\".\n",
    "schema_outputs = ComponentOutputs({\n",
    "    'output':Channel(\n",
    "        type_name='SchemaPath',\n",
    "        static_artifact_collection=[train_schema_path] \n",
    "    )\n",
    "})\n",
    "\n",
    "infer_schema = SchemaGen(\n",
    "    stats=statistics_gen.outputs.output, # A Channel of 'ExampleStatisticsPath' type, it is equal to statistics_outputs\n",
    "    name='Schema Generator',  # Optional, name should be unique if you are going to use multiple StatisticsGen in same pipeline.\n",
    "    outputs=schema_outputs # dict from name to output channel, it will be stored schema_gen.outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = types.TfxType(type_name='ExamplesPath', split='train')\n",
    "train_examples.uri = os.path.join(_data_root,\n",
    "                                  'transform/transformed_examples/train/')\n",
    "eval_examples = types.TfxType(type_name='ExamplesPath', split='eval')\n",
    "eval_examples.uri = os.path.join(_data_root,\n",
    "                                 'transform/transformed_examples/eval/')\n",
    "transform_output = types.TfxType(type_name='TransformPath')\n",
    "transform_output.uri = os.path.join(_data_root,\n",
    "                                    'transform/transform_output/')\n",
    "\n",
    "transform_outputs = ComponentOutputs({\n",
    "    # Output of 'tf.Transform', which includes an exported \n",
    "    # Tensorflow graph suitable for both training and serving\n",
    "    'transform_output':Channel(\n",
    "        type_name='TransformPath',\n",
    "        static_artifact_collection=[transform_output]\n",
    "    ),\n",
    "    # transformed_examples: Materialized transformed examples, which includes \n",
    "    # both 'train' and 'eval' splits.\n",
    "    'transformed_examples':Channel(\n",
    "        type_name='ExamplesPath',\n",
    "        static_artifact_collection=[train_examples, eval_examples]\n",
    "    )\n",
    "})\n",
    "\n",
    "transform = Transform(\n",
    "    input_data=example_gen.outputs.examples,\n",
    "    schema=infer_schema.outputs.output,\n",
    "#     module_file=_taxi_module_file,\n",
    "    module_file=\"preprocess.py\",\n",
    "    outputs=transform_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"TFX Pipeline\",\n",
    "    pipeline_root=_pipeline_root,\n",
    "    components=[example_gen, statistics_gen, infer_schema, transform]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectRunner(TfxRunner):\n",
    "    \"\"\"Tfx runner on local\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self._config = config or {}\n",
    "    \n",
    "    def run(self, pipeline):\n",
    "        for component in pipeline.components:\n",
    "            self._execute_component(component)\n",
    "            \n",
    "        return pipeline\n",
    "            \n",
    "    def _execute_component(self, component):\n",
    "        input_dict = {key:value.get() for key, value in component.input_dict.items()}\n",
    "        output_dict = {key: value.get() for key, value in component.outputs.get_all().items()}\n",
    "        exec_properties = component.exec_properties\n",
    "        executor = component.executor()\n",
    "        executor.Do(input_dict, output_dict, exec_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-14 02:32:49,709] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input-base\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExternalPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"type_name\": {\"stringValue\": \"ExternalPath\"}, \"split\": {\"stringValue\": \"\"}}}}]}\n",
      "[2019-06-14 02:32:49,713] {base_executor.py:74} INFO - Inputs for Executor is: {\"input-base\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExternalPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple\", \"properties\": {\"type_name\": {\"stringValue\": \"ExternalPath\"}, \"split\": {\"stringValue\": \"\"}}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"eval_examples\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}], \"training_examples\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}], \"examples\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}]}\n",
      "[2019-06-14 02:32:49,723] {base_executor.py:76} INFO - Outputs for Executor is: {\"eval_examples\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}], \"training_examples\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}], \"examples\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hashBuckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hashBuckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"}\n",
      "[2019-06-14 02:32:49,730] {base_executor.py:78} INFO - Execution properties for Executor is: {\"output\": \"{\\n  \\\"splitConfig\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hashBuckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hashBuckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"}\n",
      "INFO:tensorflow:Generating examples.\n",
      "[2019-06-14 02:32:49,738] {base_example_gen_executor.py:122} INFO - Generating examples.\n",
      "[2019-06-14 02:32:49,745] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-14 02:32:49,763] {executor.py:70} INFO - Processing input csv data /root/taxi/data/simple/data.csv to TFExample.\n",
      "[2019-06-14 02:32:50,782] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f5f0b510268> ====================\n",
      "[2019-06-14 02:32:50,785] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f5f0b510378> ====================\n",
      "[2019-06-14 02:32:50,787] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f5f0b510400> ====================\n",
      "[2019-06-14 02:32:50,791] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f5f0b510488> ====================\n",
      "[2019-06-14 02:32:50,794] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f5f0b510510> ====================\n",
      "[2019-06-14 02:32:50,796] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f5f0b510620> ====================\n",
      "[2019-06-14 02:32:50,813] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f5f0b5106a8> ====================\n",
      "[2019-06-14 02:32:50,820] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f5f0b510730> ====================\n",
      "[2019-06-14 02:32:50,824] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f5f0b5107b8> ====================\n",
      "[2019-06-14 02:32:50,825] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f5f0b510950> ====================\n",
      "[2019-06-14 02:32:50,827] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f5f0b5109d8> ====================\n",
      "[2019-06-14 02:32:50,830] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f5f0b510a60> ====================\n",
      "[2019-06-14 02:32:50,847] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_InputSourceToExample/ReadFromText/Read_4)+(((ref_AppliedPTransform_InputSourceToExample/ParseCSV/ParseCSVRecords_6)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/KeyWithVoid_8))+(ref_PCollection_PCollection_2/Write)))+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Precombine))+(InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Write)\n",
      "[2019-06-14 02:32:51,990] {fn_api_runner.py:437} INFO - Running (InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Group/Read)+((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/Merge)+((InputSourceToExample/ParseCSV/InferFeatureTypes/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/UnKey_16)+(ref_PCollection_PCollection_8/Write))))\n",
      "[2019-06-14 02:32:52,007] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/DoOnce/Read_18)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/InferFeatureTypes/InjectDefault_19))+(ref_PCollection_PCollection_10/Write)\n",
      "[2019-06-14 02:32:52,028] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_2/Read)+(ref_AppliedPTransform_InputSourceToExample/ParseCSV/CreateInMemoryDict_20))+((ref_AppliedPTransform_InputSourceToExample/ToTFExample_21)+((ref_AppliedPTransform_SerializeDeterministically_22)+(((ref_AppliedPTransform_SplitData/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_25)+((ref_AppliedPTransform_ShuffleSpliteval/AddRandomKeys_53)+((ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/Map(reify_timestamps)_55)+(ShuffleSpliteval/ReshufflePerKey/GroupByKey/Write))))+((ref_AppliedPTransform_ShuffleSplittrain/AddRandomKeys_27)+((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/Map(reify_timestamps)_29)+(ShuffleSplittrain/ReshufflePerKey/GroupByKey/Write))))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 02:32:57,474] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/DoOnce/Read_40)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/InitializeWrite_41))+(ref_PCollection_PCollection_24/Write))+(ref_PCollection_PCollection_25/Write)\n",
      "[2019-06-14 02:32:57,488] {fn_api_runner.py:437} INFO - Running ((ShuffleSplittrain/ReshufflePerKey/GroupByKey/Read)+(((ref_AppliedPTransform_ShuffleSplittrain/ReshufflePerKey/FlatMap(restore_timestamps)_34)+(ref_AppliedPTransform_ShuffleSplittrain/RemoveRandomKeys_35))+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WriteBundles_42)))+(((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Pair_43)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/WindowInto(WindowIntoFn)_44))+(OutputSplittrain/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-14 02:32:57,634] {tfrecordio.py:57} WARNING - Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "[2019-06-14 02:32:57,964] {fn_api_runner.py:437} INFO - Running (OutputSplittrain/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/Extract_49)+(ref_PCollection_PCollection_32/Write))\n",
      "[2019-06-14 02:32:57,976] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/DoOnce/Read_66)+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/InitializeWrite_67)+(ref_PCollection_PCollection_43/Write)))+(ref_PCollection_PCollection_42/Write)\n",
      "[2019-06-14 02:32:57,996] {fn_api_runner.py:437} INFO - Running (((ShuffleSpliteval/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_ShuffleSpliteval/ReshufflePerKey/FlatMap(restore_timestamps)_60))+((ref_AppliedPTransform_ShuffleSpliteval/RemoveRandomKeys_61)+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WriteBundles_68)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Pair_69))))+((ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/WindowInto(WindowIntoFn)_70)+(OutputSpliteval/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-14 02:32:58,236] {fn_api_runner.py:437} INFO - Running ((OutputSpliteval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/Extract_75))+(ref_PCollection_PCollection_50/Write)\n",
      "[2019-06-14 02:32:58,247] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/PreFinalize_76))+(ref_PCollection_PCollection_51/Write)\n",
      "[2019-06-14 02:32:58,264] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_OutputSpliteval/Write/WriteImpl/FinalizeWrite_77)\n",
      "[2019-06-14 02:32:58,278] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-14 02:32:58,384] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-14 02:32:58,402] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+((ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/PreFinalize_50)+(ref_PCollection_PCollection_33/Write))\n",
      "[2019-06-14 02:32:58,419] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_24/Read)+(ref_AppliedPTransform_OutputSplittrain/Write/WriteImpl/FinalizeWrite_51)\n",
      "[2019-06-14 02:32:58,438] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-14 02:32:58,545] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Examples generated.\n",
      "[2019-06-14 02:32:58,559] {base_example_gen_executor.py:145} INFO - Examples generated.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-14 02:32:58,563] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"input_data\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}]}\n",
      "[2019-06-14 02:32:58,567] {base_executor.py:74} INFO - Inputs for Executor is: {\"input_data\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"output\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}]}\n",
      "[2019-06-14 02:32:58,572] {base_executor.py:76} INFO - Outputs for Executor is: {\"output\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExampleStatisticsPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/statistics_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExampleStatisticsPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {}\n",
      "[2019-06-14 02:32:58,575] {base_executor.py:78} INFO - Execution properties for Executor is: {}\n",
      "[2019-06-14 02:32:58,580] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:tensorflow:Generating statistics for split train\n",
      "[2019-06-14 02:32:58,589] {executor.py:62} INFO - Generating statistics for split train\n",
      "INFO:tensorflow:Generating statistics for split eval\n",
      "[2019-06-14 02:32:59,326] {executor.py:62} INFO - Generating statistics for split eval\n",
      "INFO:tensorflow:Statistics written to /root/taxi/data/simple/statistics_gen/eval/.\n",
      "[2019-06-14 02:33:00,127] {executor.py:78} INFO - Statistics written to /root/taxi/data/simple/statistics_gen/eval/.\n",
      "[2019-06-14 02:33:02,887] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f5f0b510268> ====================\n",
      "[2019-06-14 02:33:02,892] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f5f0b510378> ====================\n",
      "[2019-06-14 02:33:02,895] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f5f0b510400> ====================\n",
      "[2019-06-14 02:33:02,902] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f5f0b510488> ====================\n",
      "[2019-06-14 02:33:02,908] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f5f0b510510> ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 02:33:02,915] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f5f0b510620> ====================\n",
      "[2019-06-14 02:33:02,923] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f5f0b5106a8> ====================\n",
      "[2019-06-14 02:33:02,934] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f5f0b510730> ====================\n",
      "[2019-06-14 02:33:02,938] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f5f0b5107b8> ====================\n",
      "[2019-06-14 02:33:02,941] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f5f0b510950> ====================\n",
      "[2019-06-14 02:33:02,945] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f5f0b5109d8> ====================\n",
      "[2019-06-14 02:33:02,946] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f5f0b510a60> ====================\n",
      "[2019-06-14 02:33:02,977] {fn_api_runner.py:437} INFO - Running ((((((ref_AppliedPTransform_ReadData.eval/Read_106)+((ref_AppliedPTransform_DecodeData.eval/ParseTFExamples_108)+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/KeyWithVoid_111)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_115)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_116)))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_135))))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)))+(((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_136)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_138))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "[2019-06-14 02:33:06,079] {fn_api_runner.py:437} INFO - Running (((((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_146))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_156)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_158)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write))))\n",
      "[2019-06-14 02:33:06,124] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_155)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)))\n",
      "[2019-06-14 02:33:06,154] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_124)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_125)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1))\n",
      "[2019-06-14 02:33:06,536] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "[2019-06-14 02:33:06,728] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)))\n",
      "[2019-06-14 02:33:07,148] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_166))))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 02:33:07,174] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_168))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "[2019-06-14 02:33:07,196] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "[2019-06-14 02:33:07,218] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_177)))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_180)+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write))))\n",
      "[2019-06-14 02:33:07,254] {fn_api_runner.py:437} INFO - Running (((((((ref_AppliedPTransform_ReadData.train/Read_3)+((((ref_AppliedPTransform_DecodeData.train/ParseTFExamples_5)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/KeyWithVoid_8))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_12))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_32)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_33)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_35)))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_13)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)\n",
      "[2019-06-14 02:33:14,664] {fn_api_runner.py:437} INFO - Running (GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_43)))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_53)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_55)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)))))\n",
      "[2019-06-14 02:33:14,702] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_63)))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1)\n",
      "[2019-06-14 02:33:14,724] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs)))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_52))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)\n",
      "[2019-06-14 02:33:14,750] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_65))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "[2019-06-14 02:33:14,774] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs)))+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_21))+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_22)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 02:33:15,201] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "[2019-06-14 02:33:15,364] {fn_api_runner.py:437} INFO - Running ((((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "[2019-06-14 02:33:15,671] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "[2019-06-14 02:33:15,692] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_74))+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_77)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)))\n",
      "[2019-06-14 02:33:15,732] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+((GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_85)+(ref_PCollection_PCollection_51/Write)))\n",
      "[2019-06-14 02:33:15,759] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_87)+((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_88)+(((ref_AppliedPTransform_GenerateStatistics.train/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_89)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/Map(<lambda at iobase.py:984>)_96))+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WindowInto(WindowIntoFn)_97)+(WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Write))))\n",
      "[2019-06-14 02:33:15,824] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/DoOnce/Read_94)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/InitializeWrite_95))+(ref_PCollection_PCollection_55/Write))+(ref_PCollection_PCollection_56/Write)\n",
      "[2019-06-14 02:33:15,848] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.train/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/WriteBundles_102))+(ref_PCollection_PCollection_62/Write)\n",
      "[2019-06-14 02:33:15,873] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+((ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/PreFinalize_103)+(ref_PCollection_PCollection_63/Write))\n",
      "[2019-06-14 02:33:15,895] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+((GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_188)+(ref_PCollection_PCollection_115/Write)))\n",
      "[2019-06-14 02:33:15,918] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_190)+(ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_191))+((ref_AppliedPTransform_GenerateStatistics.eval/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_192)+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/Map(<lambda at iobase.py:984>)_199)+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WindowInto(WindowIntoFn)_200)+(WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Write))))\n",
      "[2019-06-14 02:33:15,964] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/DoOnce/Read_197)+((ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/InitializeWrite_198)+(ref_PCollection_PCollection_120/Write)))+(ref_PCollection_PCollection_119/Write)\n",
      "[2019-06-14 02:33:15,986] {fn_api_runner.py:437} INFO - Running ((WriteStatsOutput.eval/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/WriteBundles_205))+(ref_PCollection_PCollection_126/Write)\n",
      "[2019-06-14 02:33:16,011] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/PreFinalize_206))+(ref_PCollection_PCollection_127/Write)\n",
      "[2019-06-14 02:33:16,032] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_119/Read)+(ref_AppliedPTransform_WriteStatsOutput.eval/Write/WriteImpl/FinalizeWrite_207)\n",
      "[2019-06-14 02:33:16,246] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-14 02:33:16,354] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "[2019-06-14 02:33:16,370] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput.train/Write/WriteImpl/FinalizeWrite_104)\n",
      "[2019-06-14 02:33:16,388] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-14 02:33:16,496] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.11 seconds.\n",
      "INFO:tensorflow:Infering schema from statistics.\n",
      "[2019-06-14 02:33:16,521] {executor.py:62} INFO - Infering schema from statistics.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "[2019-06-14 02:33:16,526] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py:328: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
      "[2019-06-14 02:33:16,547] {executor.py:66} INFO - Schema written to /root/taxi/data/simple/schema_gen/schema.pbtxt.\n",
      "INFO:tensorflow:Starting Executor execution.\n",
      "[2019-06-14 02:33:16,551] {base_executor.py:72} INFO - Starting Executor execution.\n",
      "INFO:tensorflow:Inputs for Executor is: {\"schema\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"SchemaPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"type_name\": {\"stringValue\": \"SchemaPath\"}, \"split\": {\"stringValue\": \"train\"}}}}], \"input_data\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}]}\n",
      "[2019-06-14 02:33:16,555] {base_executor.py:74} INFO - Inputs for Executor is: {\"schema\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"SchemaPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/schema_gen/\", \"properties\": {\"type_name\": {\"stringValue\": \"SchemaPath\"}, \"split\": {\"stringValue\": \"train\"}}}}], \"input_data\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/csv_example_gen/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}]}\n",
      "INFO:tensorflow:Outputs for Executor is: {\"transformed_examples\": [{\"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}], \"transform_output\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"TransformPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"type_name\": {\"stringValue\": \"TransformPath\"}, \"split\": {\"stringValue\": \"\"}}}}]}\n",
      "[2019-06-14 02:33:16,563] {base_executor.py:76} INFO - Outputs for Executor is: {\"transformed_examples\": [{\"artifact_type\": {\"properties\": {\"name\": \"STRING\", \"state\": \"STRING\", \"type_name\": \"STRING\", \"split\": \"STRING\", \"span\": \"INT\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/train/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"train\"}}}}, {\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"ExamplesPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transformed_examples/eval/\", \"properties\": {\"type_name\": {\"stringValue\": \"ExamplesPath\"}, \"split\": {\"stringValue\": \"eval\"}}}}], \"transform_output\": [{\"artifact_type\": {\"properties\": {\"state\": \"STRING\", \"type_name\": \"STRING\", \"span\": \"INT\", \"split\": \"STRING\", \"name\": \"STRING\"}, \"name\": \"TransformPath\"}, \"artifact\": {\"uri\": \"/root/taxi/data/simple/transform/transform_output/\", \"properties\": {\"type_name\": {\"stringValue\": \"TransformPath\"}, \"split\": {\"stringValue\": \"\"}}}}]}\n",
      "INFO:tensorflow:Execution properties for Executor is: {\"module_file\": \"preprocess.py\"}\n",
      "[2019-06-14 02:33:16,566] {base_executor.py:78} INFO - Execution properties for Executor is: {\"module_file\": \"preprocess.py\"}\n",
      "INFO:tensorflow:Inputs to executor.Transform function: {'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt', 'tft_statistics_use_tfdv': True, 'compute_statistics': False, 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'examples_data_format': 'FORMAT_TF_EXAMPLE', 'preprocessing_fn': 'preprocess.py', 'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*'}\n",
      "[2019-06-14 02:33:16,589] {executor.py:567} INFO - Inputs to executor.Transform function: {'schema_path': '/root/taxi/data/simple/schema_gen/schema.pbtxt', 'tft_statistics_use_tfdv': True, 'compute_statistics': False, 'analyze_and_transform_data_paths': '/root/taxi/data/simple/csv_example_gen/train/*', 'examples_data_format': 'FORMAT_TF_EXAMPLE', 'preprocessing_fn': 'preprocess.py', 'transform_only_data_paths': '/root/taxi/data/simple/csv_example_gen/eval/*'}\n",
      "INFO:tensorflow:Outputs to executor.Transform function: {'transform_output_path': '/root/taxi/data/simple/transform/transform_output/', 'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path', 'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples']}\n",
      "[2019-06-14 02:33:16,595] {executor.py:569} INFO - Outputs to executor.Transform function: {'transform_output_path': '/root/taxi/data/simple/transform/transform_output/', 'temp_path': '/root/taxi/data/simple/transform/transform_output/.temp_path', 'transform_materialize_output_paths': ['/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples', '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples']}\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_transform/mappers.py:1027: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[2019-06-14 02:33:16,889] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow_transform/mappers.py:1027: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
      "[2019-06-14 02:33:17,027] {executor.py:653} INFO - Analyze and transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/train/*')]\n",
      "INFO:tensorflow:Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
      "[2019-06-14 02:33:17,031] {executor.py:655} INFO - Transform data patterns: [(0, '/root/taxi/data/simple/csv_example_gen/eval/*')]\n",
      "INFO:tensorflow:Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n",
      "[2019-06-14 02:33:17,034] {executor.py:657} INFO - Transform materialization output paths: [(0, '/root/taxi/data/simple/transform/transformed_examples/train/transformed_examples'), (1, '/root/taxi/data/simple/transform/transformed_examples/eval/transformed_examples')]\n",
      "INFO:tensorflow:Transform output path: /root/taxi/data/simple/transform/transform_output/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 02:33:17,039] {executor.py:658} INFO - Transform output path: /root/taxi/data/simple/transform/transform_output/\n",
      "[2019-06-14 02:33:17,704] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "[2019-06-14 02:33:18,008] {deprecation.py:323} WARNING - From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-14 02:33:18,016] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "[2019-06-14 02:33:18,018] {builder_impl.py:449} INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/0f29a3b9da074a96b2fd8daf96a10d42/saved_model.pb\n",
      "[2019-06-14 02:33:18,054] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/0f29a3b9da074a96b2fd8daf96a10d42/saved_model.pb\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-14 02:33:20,474] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "[2019-06-14 02:33:20,481] {builder_impl.py:449} INFO - No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/cf94582cd1eb4c3aa33bff70e01c6f8e/saved_model.pb\n",
      "[2019-06-14 02:33:20,516] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/cf94582cd1eb4c3aa33bff70e01c6f8e/saved_model.pb\n",
      "[2019-06-14 02:33:26,271] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f5f0b510268> ====================\n",
      "[2019-06-14 02:33:26,283] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f5f0b510378> ====================\n",
      "[2019-06-14 02:33:26,290] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f5f0b510400> ====================\n",
      "[2019-06-14 02:33:26,310] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f5f0b510488> ====================\n",
      "[2019-06-14 02:33:26,315] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f5f0b510510> ====================\n",
      "[2019-06-14 02:33:26,320] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f5f0b510620> ====================\n",
      "[2019-06-14 02:33:26,332] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f5f0b5106a8> ====================\n",
      "[2019-06-14 02:33:26,360] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f5f0b510730> ====================\n",
      "[2019-06-14 02:33:26,367] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f5f0b5107b8> ====================\n",
      "[2019-06-14 02:33:26,372] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f5f0b510950> ====================\n",
      "[2019-06-14 02:33:26,378] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f5f0b5109d8> ====================\n",
      "[2019-06-14 02:33:26,381] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f5f0b510a60> ====================\n",
      "[2019-06-14 02:33:26,430] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_ReadAnalysisDataset[0]/Read/Read_4)+((ref_AppliedPTransform_ReadAnalysisDataset[0]/AddKey_5)+(ref_AppliedPTransform_ReadAnalysisDataset[0]/ParseExamples_6)))+((ref_AppliedPTransform_DecodeAnalysisDataset[0]/ApplyDecodeFn_8)+(FlattenAnalysisDatasets/Write/0))\n",
      "[2019-06-14 02:33:28,411] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/CreateSavedModelForAnalyzerInputs[0]/CreateSavedModel/Read_13)+(ref_PCollection_PCollection_6/Write)\n",
      "[2019-06-14 02:33:28,423] {fn_api_runner.py:437} INFO - Running (((FlattenAnalysisDatasets/Read)+(((((((((((((((((((((((((ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/BatchInputs/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_17)+(ref_AppliedPTransform_AnalyzeDataset/ApplySavedModel[0]/ApplySavedModel_18))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score/mean_and_var]_19))+((ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_1/mean_and_var]_46)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/KeyWithVoid_49)))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[scale_to_z_score_2/mean_and_var]_73))+((ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary/vocabulary]_100)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FlattenStringsAndMaybeWeightsLabels_102)))+((ref_AppliedPTransform_AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_1/vocabulary]_158)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FlattenStringsAndMaybeWeightsLabels_160)))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize/quantiles]_216))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_1/quantiles]_248))+(ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_2/quantiles]_280))+(((ref_AppliedPTransform_AnalyzeDataset/TensorSource[bucketize_3/quantiles]_312)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/KeyWithVoid_315))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/KeyWithVoid_251))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/KeyWithVoid_283)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/KeyWithVoid_76))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write))+((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write)))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CountPerString:PairWithVoid_162))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/KeyWithVoid_22)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/KeyWithVoid_219)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Write)))+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CountPerString:PairWithVoid_104)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine))+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write))))+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Write))+((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Precombine)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-14 02:33:28,883] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-14 02:33:29,973] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score/mean_and_var]/InitialCombineGlobally/UnKey_30)))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_33)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine))+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
      "[2019-06-14 02:33:30,008] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/MergeCombinesGlobally/UnKey_41))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_43)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder]_44))))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score/mean_and_var/Placeholder_1]_45)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/3)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/3))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/2)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/2))\n",
      "[2019-06-14 02:33:30,052] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+((((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2/mean_and_var]/InitialCombineGlobally/UnKey_84))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_87)))+((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n",
      "[2019-06-14 02:33:30,084] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_127)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1)\n",
      "[2019-06-14 02:33:30,103] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge))+((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]/FilterProblematicStrings_112)))+((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Precombine)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Write))\n",
      "[2019-06-14 02:33:30,128] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Group/Read)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/Merge))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerString/ExtractOutputs))+((((ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/SwapStringsAndCounts_121)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_125))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0))\n",
      "[2019-06-14 02:33:30,156] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
      "[2019-06-14 02:33:30,170] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_133))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_134)+(ref_PCollection_PCollection_83/Write))\n",
      "[2019-06-14 02:33:30,193] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/UnKey_323)+(ref_PCollection_PCollection_201/Write))\n",
      "[2019-06-14 02:33:30,328] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/DoOnce/Read_423)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/InitializeWrite_424))+(ref_PCollection_PCollection_262/Write))+(ref_PCollection_PCollection_263/Write)\n",
      "[2019-06-14 02:33:30,350] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/UnKey_291)))+(ref_PCollection_PCollection_181/Write))\n",
      "[2019-06-14 02:33:30,484] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/DoOnce/Read_293)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_2/quantiles]/InitialCombineGlobally/InjectDefault_294)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/KeyWithVoid_297)))+((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 02:33:30,605] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+((AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/UnKey_305)))+(ref_PCollection_PCollection_189/Write))\n",
      "[2019-06-14 02:33:30,782] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/DoOnce/Read_307)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/MergeCombinesGlobally/InjectDefault_308))+(((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_2/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_310)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_2/quantiles/Placeholder]_311))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/0)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/0)))\n",
      "[2019-06-14 02:33:30,829] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/Merge))+(((AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1/mean_and_var]/InitialCombineGlobally/UnKey_57)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/KeyWithVoid_60)))+((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n",
      "[2019-06-14 02:33:30,860] {fn_api_runner.py:437} INFO - Running ((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+(((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/MergeCombinesGlobally/UnKey_68))+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_1/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_70)))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder_1]_72)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/5)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/5))))+(((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1/mean_and_var/Placeholder]_71)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/4))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/4))\n",
      "[2019-06-14 02:33:30,900] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/Merge))+((((AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/MergeCombinesGlobally/UnKey_95))+((((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[scale_to_z_score_2/mean_and_var]/ExtractOutputs/FlatMap(extract_outputs)_97)+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder]_98)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/6)))+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2/mean_and_var/Placeholder_1]_99))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/7)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/7))))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/6))\n",
      "[2019-06-14 02:33:30,966] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/Merge)+((AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/UnKey_259)+(ref_PCollection_PCollection_161/Write))))\n",
      "[2019-06-14 02:33:31,213] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/DoOnce/Read_261)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_1/quantiles]/InitialCombineGlobally/InjectDefault_262))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/KeyWithVoid_265)+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)\n",
      "[2019-06-14 02:33:31,342] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+(((AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/UnKey_273))+(ref_PCollection_PCollection_169/Write)))\n",
      "[2019-06-14 02:33:31,486] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/DoOnce/Read_275)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/MergeCombinesGlobally/InjectDefault_276)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_1/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_278)))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_1/quantiles/Placeholder]_279)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/11)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/11)))\n",
      "[2019-06-14 02:33:31,515] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/DoOnce/Read_325)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize_3/quantiles]/InitialCombineGlobally/InjectDefault_326))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/KeyWithVoid_329)+((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)+(AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)))\n",
      "[2019-06-14 02:33:31,605] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge)+((AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/UnKey_337)+(ref_PCollection_PCollection_209/Write))))\n",
      "[2019-06-14 02:33:31,956] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/DoOnce/Read_339)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/MergeCombinesGlobally/InjectDefault_340))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize_3/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_342)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize_3/quantiles/Placeholder]_343)))+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/1)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 02:33:31,997] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_143)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_144)+(ref_PCollection_PCollection_87/Write)))+(ref_PCollection_PCollection_86/Write)\n",
      "[2019-06-14 02:33:32,019] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/Prepare/Read_137)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/OrderElements_138)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_145)))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_146)+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write))\n",
      "[2019-06-14 02:33:32,061] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_151))+(ref_PCollection_PCollection_93/Write)\n",
      "[2019-06-14 02:33:32,086] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_86/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_152)+(ref_PCollection_PCollection_94/Write))\n",
      "[2019-06-14 02:33:32,115] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_86/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_153)+(ref_PCollection_PCollection_95/Write))\n",
      "[2019-06-14 02:33:32,136] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-14 02:33:32,240] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-14 02:33:32,264] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/CreatePath/Read_155)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary/vocabulary]/WaitForVocabularyFile_156)+(ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary/vocabulary/Placeholder]_157)))+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/8))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/8)\n",
      "[2019-06-14 02:33:32,293] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/DoOnce/Read_201)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/InitializeWrite_202)+(ref_PCollection_PCollection_123/Write)))+(ref_PCollection_PCollection_122/Write)\n",
      "[2019-06-14 02:33:32,321] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Create/Read_185)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/1))+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/1)\n",
      "[2019-06-14 02:33:32,346] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Group/Read)+(((AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/Merge)+(AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/CombinePerKey(CountCombineFn)/ExtractOutputs))+(((ref_AppliedPTransform_AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]/FilterProblematicStrings_170)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Precombine))+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Write)))\n",
      "[2019-06-14 02:33:32,376] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Group/Read)+(((((AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/Merge)+(AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerString/ExtractOutputs))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/SwapStringsAndCounts_179))+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_TopPerBundle)_183))+((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Transcode/0)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Write/0)))\n",
      "[2019-06-14 02:33:32,410] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/Flatten/Read)+(AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Write)\n",
      "[2019-06-14 02:33:32,428] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/Top(1000)/ParDo(_MergeTopPerBundle)_191))+((ref_AppliedPTransform_AnalyzeDataset/VocabularyOrderAndFilter[compute_and_apply_vocabulary_1/vocabulary]/ApplyFrequencyThresholdAndTopK/FlattenList_192)+(ref_PCollection_PCollection_119/Write))\n",
      "[2019-06-14 02:33:32,454] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/Prepare/Read_195)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/OrderElements_196)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/Map(<lambda at iobase.py:984>)_203)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WindowInto(WindowIntoFn)_204))))+(AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Write)\n",
      "[2019-06-14 02:33:32,493] {fn_api_runner.py:437} INFO - Running ((AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/WriteBundles_209))+(ref_PCollection_PCollection_129/Write)\n",
      "[2019-06-14 02:33:32,516] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_122/Read)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/PreFinalize_210))+(ref_PCollection_PCollection_130/Write)\n",
      "[2019-06-14 02:33:32,544] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_122/Read)+((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToFile/Write/WriteImpl/FinalizeWrite_211)+(ref_PCollection_PCollection_131/Write))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 02:33:32,562] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-14 02:33:32,666] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-14 02:33:32,691] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/CreatePath/Read_213)+(ref_AppliedPTransform_AnalyzeDataset/VocabularyWrite[compute_and_apply_vocabulary_1/vocabulary]/WaitForVocabularyFile_214))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1/vocabulary/Placeholder]_215)+((AnalyzeDataset/CreateSavedModel/Flatten/Transcode/9)+(AnalyzeDataset/CreateSavedModel/Flatten/Write/9)))\n",
      "[2019-06-14 02:33:32,725] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/UnKey_227)+(ref_PCollection_PCollection_141/Write))\n",
      "[2019-06-14 02:33:32,899] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/DoOnce/Read_229)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineAccumulate[bucketize/quantiles]/InitialCombineGlobally/InjectDefault_230))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/KeyWithVoid_233)+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine)))+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write)\n",
      "[2019-06-14 02:33:32,995] {fn_api_runner.py:437} INFO - Running (((AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+(AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/UnKey_241)+(ref_PCollection_PCollection_149/Write))\n",
      "[2019-06-14 02:33:33,153] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/DoOnce/Read_243)+((ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/InjectDefault_244)+(ref_AppliedPTransform_AnalyzeDataset/CacheableCombineMerge[bucketize/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_246)))+((ref_AppliedPTransform_AnalyzeDataset/CreateTensorBinding[bucketize/quantiles/Placeholder]_247)+(AnalyzeDataset/CreateSavedModel/Flatten/Transcode/10)))+(AnalyzeDataset/CreateSavedModel/Flatten/Write/10)\n",
      "[2019-06-14 02:33:33,182] {fn_api_runner.py:437} INFO - Running (AnalyzeDataset/CreateSavedModel/Flatten/Read)+(ref_PCollection_PCollection_216/Write)\n",
      "[2019-06-14 02:33:33,212] {fn_api_runner.py:437} INFO - Running ((((ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/CreateSavedModel/Read_346)+((((((ref_AppliedPTransform_AnalyzeDataset/CreateSavedModel/BindTensors_348)+(ref_AppliedPTransform_AnalyzeDataset/ComputeDeferredMetadata_349))+(ref_AppliedPTransform_AnalyzeDataset/MakeCheapBarrier_350))+(ref_AppliedPTransform_WriteTransformFn/WriteTransformFn_361))+(ref_PCollection_PCollection_217/Write))+(ref_PCollection_PCollection_225/Write)))+(ref_AppliedPTransform_WriteTransformFn/WriteMetadata/WriteMetadata_360))+(ref_PCollection_PCollection_224/Write))+(ref_PCollection_PCollection_219/Write)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-14 02:33:33,360] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "[2019-06-14 02:33:33,385] {builder_impl.py:654} INFO - Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/deb9d1419404434e8c70134def715393/assets\n",
      "[2019-06-14 02:33:33,391] {builder_impl.py:763} INFO - Assets written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/deb9d1419404434e8c70134def715393/assets\n",
      "INFO:tensorflow:SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/deb9d1419404434e8c70134def715393/saved_model.pb\n",
      "[2019-06-14 02:33:33,444] {builder_impl.py:414} INFO - SavedModel written to: /root/taxi/data/simple/transform/transform_output/.temp_path/tftransform_tmp/deb9d1419404434e8c70134def715393/saved_model.pb\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-14 02:33:33,533] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-14 02:33:33,534] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-14 02:33:33,537] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-14 02:33:33,648] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_ReadTransformDataset[1]/Read/Read_383)+((ref_AppliedPTransform_ReadTransformDataset[1]/AddKey_384)+((ref_AppliedPTransform_ReadTransformDataset[1]/ParseExamples_385)+(ref_AppliedPTransform_DecodeTransformDataset[1]/ApplyDecodeFn_387))))+(((((ref_AppliedPTransform_TransformDataset[1]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_391)+((((ref_AppliedPTransform_TransformDataset[1]/Transform_392)+(ref_AppliedPTransform_TransformDataset[1]/ConvertAndUnbatch_393))+(ref_AppliedPTransform_TransformDataset[1]/MakeCheapBarrier_394))+(ref_AppliedPTransform_EncodeTransformedDataset[1]_398)))+(ref_AppliedPTransform_Materialize[1]/DropNoneKeys_418))+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WriteBundles_425))+(ref_PCollection_PCollection_245/Write)))+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Pair_426)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_427)+(Materialize[1]/Write/Write/WriteImpl/GroupByKey/Write)))\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-14 02:33:33,819] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-14 02:33:33,822] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-14 02:33:33,828] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 02:33:36,664] {fn_api_runner.py:437} INFO - Running ((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/DoOnce/Read_405)+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/InitializeWrite_406)+(ref_PCollection_PCollection_251/Write)))+(ref_PCollection_PCollection_250/Write)\n",
      "[2019-06-14 02:33:36,686] {fn_api_runner.py:437} INFO - Running (((((ref_AppliedPTransform_ReadTransformDataset[0]/Read/Read_365)+(ref_AppliedPTransform_ReadTransformDataset[0]/AddKey_366))+((ref_AppliedPTransform_ReadTransformDataset[0]/ParseExamples_367)+((ref_AppliedPTransform_DecodeTransformDataset[0]/ApplyDecodeFn_369)+((ref_AppliedPTransform_TransformDataset[0]/Batch/BatchElements/ParDo(_GlobalWindowsBatchingDoFn)_373)+((ref_AppliedPTransform_TransformDataset[0]/Transform_374)+(ref_AppliedPTransform_TransformDataset[0]/ConvertAndUnbatch_375))))))+((ref_AppliedPTransform_TransformDataset[0]/MakeCheapBarrier_376)+(ref_PCollection_PCollection_234/Write)))+((ref_AppliedPTransform_EncodeTransformedDataset[0]_380)+(((ref_AppliedPTransform_Materialize[0]/DropNoneKeys_400)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WriteBundles_407))+((ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Pair_408)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_409)))))+(Materialize[0]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "[2019-06-14 02:33:36,859] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_9:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "[2019-06-14 02:33:36,861] {ops.py:6153} WARNING - Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_10:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-14 02:33:36,867] {saver.py:1483} INFO - Saver not created because there are no variables in the graph to restore\n",
      "[2019-06-14 02:33:41,312] {fn_api_runner.py:437} INFO - Running ((Materialize[0]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/Extract_414))+(ref_PCollection_PCollection_258/Write)\n",
      "[2019-06-14 02:33:41,335] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_AnalyzeDataset/PrepareToClearSharedKeepAlives/Read_352)+(ref_AppliedPTransform_AnalyzeDataset/WaitAndClearSharedKeepAlives_353)\n",
      "[2019-06-14 02:33:41,363] {fn_api_runner.py:437} INFO - Running (Materialize[1]/Write/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/Extract_432)+(ref_PCollection_PCollection_270/Write))\n",
      "[2019-06-14 02:33:41,388] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[0]/PrepareToClearSharedKeepAlives/Read_378)+(ref_AppliedPTransform_TransformDataset[0]/WaitAndClearSharedKeepAlives_379)\n",
      "[2019-06-14 02:33:41,412] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_TransformDataset[1]/PrepareToClearSharedKeepAlives/Read_396)+(ref_AppliedPTransform_TransformDataset[1]/WaitAndClearSharedKeepAlives_397)\n",
      "[2019-06-14 02:33:41,439] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_262/Read)+((ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/PreFinalize_433)+(ref_PCollection_PCollection_271/Write))\n",
      "[2019-06-14 02:33:41,460] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_262/Read)+(ref_AppliedPTransform_Materialize[1]/Write/Write/WriteImpl/FinalizeWrite_434)\n",
      "[2019-06-14 02:33:41,474] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-14 02:33:41,578] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "[2019-06-14 02:33:41,606] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_225/Read)+(ref_AppliedPTransform_WriteTransformFn/WaitOnWriteMetadataDone_362)\n",
      "[2019-06-14 02:33:41,632] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_WriteMetadata/Create/Read_356)+(ref_AppliedPTransform_WriteMetadata/WriteMetadata_357)\n",
      "[2019-06-14 02:33:41,653] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/PreFinalize_415))+(ref_PCollection_PCollection_259/Write)\n",
      "[2019-06-14 02:33:41,675] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_250/Read)+(ref_AppliedPTransform_Materialize[0]/Write/Write/WriteImpl/FinalizeWrite_416)\n",
      "[2019-06-14 02:33:41,692] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-14 02:33:41,795] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n",
      "INFO:tensorflow:Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n",
      "[2019-06-14 02:33:41,825] {executor.py:248} INFO - Cleaning up temp path /root/taxi/data/simple/transform/transform_output/.temp_path on executor success\n"
     ]
    }
   ],
   "source": [
    "pipeline = DirectRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/taxi/data/simple/:\r\n",
      "total 1.9M\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 02:32 csv_example_gen\r\n",
      "1.9M -rw-r--r-- 1 root root 1.9M Jun 14 02:32 data.csv\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:33 schema_gen\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 02:33 statistics_gen\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 02:33 transform\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:32 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:32 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen/eval:\r\n",
      "total 204K\r\n",
      "204K -rw-r--r-- 1 root root 201K Jun 14 02:32 data_tfrecord-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/csv_example_gen/train:\r\n",
      "total 408K\r\n",
      "408K -rw-r--r-- 1 root root 405K Jun 14 02:32 data_tfrecord-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/schema_gen:\r\n",
      "total 8.0K\r\n",
      "8.0K -rw-r--r-- 1 root root 4.5K Jun 14 02:33 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:33 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:33 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen/eval:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 17K Jun 14 02:33 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/statistics_gen/train:\r\n",
      "total 20K\r\n",
      "20K -rw-r--r-- 1 root root 18K Jun 14 02:33 stats_tfrecord\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 5 root root 4.0K Jun 14 02:33 transform_output\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 02:33 transformed_examples\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output:\r\n",
      "total 12K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:33 metadata\r\n",
      "4.0K drwxr-xr-x 4 root root 4.0K Jun 14 02:33 transform_fn\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:33 transformed_metadata\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/metadata:\r\n",
      "total 4.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 916 Jun 14 02:33 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn:\r\n",
      "total 84K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:33 assets\r\n",
      " 76K -rw-r--r-- 1 root root  76K Jun 14 02:33 saved_model.pb\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:33 variables\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn/assets:\r\n",
      "total 8.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 1.3K Jun 14 02:33 vocab_compute_and_apply_vocabulary_1_vocabulary\r\n",
      "4.0K -rw-r--r-- 1 root root   56 Jun 14 02:33 vocab_compute_and_apply_vocabulary_vocabulary\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transform_fn/variables:\r\n",
      "total 0\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transform_output/transformed_metadata:\r\n",
      "total 4.0K\r\n",
      "4.0K -rw-r--r-- 1 root root 2.2K Jun 14 02:33 schema.pbtxt\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples:\r\n",
      "total 8.0K\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:33 eval\r\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 14 02:33 train\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples/eval:\r\n",
      "total 172K\r\n",
      "172K -rw-r--r-- 1 root root 172K Jun 14 02:33 transformed_examples-00000-of-00001.gz\r\n",
      "\r\n",
      "/root/taxi/data/simple/transform/transformed_examples/train:\r\n",
      "total 352K\r\n",
      "352K -rw-r--r-- 1 root root 349K Jun 14 02:33 transformed_examples-00000-of-00001.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -Rlhs /root/taxi/data/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`anomalies.pbtxt` is only simple text file. To visualize it, we are going to;\n",
    "\n",
    "1. get the path of `anomalies.pbtxt`from `example_validator`\n",
    "2. parse `anomalies.pbtxt` into anomalies (protobuf)\n",
    "3. visualize schema with [tfdv](https://www.tensorflow.org/tfx/data_validation/get_started)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the path of `transformed_examples/train`\n",
    "def get_transformed_examples(transform):\n",
    "    artifacts = transform.outputs.transformed_examples.get()\n",
    "    return types.get_split_uri(artifacts, 'train')\n",
    "\n",
    "transformed_examples_dir = get_transformed_examples(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 03:24:10,951] {pipeline.py:143} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "[2019-06-14 03:24:13,747] {fn_api_runner_transforms.py:490} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f5f0b510268> ====================\n",
      "[2019-06-14 03:24:13,752] {fn_api_runner_transforms.py:490} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f5f0b510378> ====================\n",
      "[2019-06-14 03:24:13,756] {fn_api_runner_transforms.py:490} INFO - ==================== <function lift_combiners at 0x7f5f0b510400> ====================\n",
      "[2019-06-14 03:24:13,763] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_sdf at 0x7f5f0b510488> ====================\n",
      "[2019-06-14 03:24:13,766] {fn_api_runner_transforms.py:490} INFO - ==================== <function expand_gbk at 0x7f5f0b510510> ====================\n",
      "[2019-06-14 03:24:13,775] {fn_api_runner_transforms.py:490} INFO - ==================== <function sink_flattens at 0x7f5f0b510620> ====================\n",
      "[2019-06-14 03:24:13,778] {fn_api_runner_transforms.py:490} INFO - ==================== <function greedily_fuse at 0x7f5f0b5106a8> ====================\n",
      "[2019-06-14 03:24:13,790] {fn_api_runner_transforms.py:490} INFO - ==================== <function read_to_impulse at 0x7f5f0b510730> ====================\n",
      "[2019-06-14 03:24:13,791] {fn_api_runner_transforms.py:490} INFO - ==================== <function impulse_to_input at 0x7f5f0b5107b8> ====================\n",
      "[2019-06-14 03:24:13,793] {fn_api_runner_transforms.py:490} INFO - ==================== <function inject_timer_pcollections at 0x7f5f0b510950> ====================\n",
      "[2019-06-14 03:24:13,796] {fn_api_runner_transforms.py:490} INFO - ==================== <function sort_stages at 0x7f5f0b5109d8> ====================\n",
      "[2019-06-14 03:24:13,798] {fn_api_runner_transforms.py:490} INFO - ==================== <function window_pcollection_coders at 0x7f5f0b510a60> ====================\n",
      "[2019-06-14 03:24:13,822] {fn_api_runner.py:437} INFO - Running (((ref_AppliedPTransform_WriteStatsOutput/Write/WriteImpl/DoOnce/Read_94)+(ref_AppliedPTransform_WriteStatsOutput/Write/WriteImpl/InitializeWrite_95))+(ref_PCollection_PCollection_55/Write))+(ref_PCollection_PCollection_56/Write)\n",
      "[2019-06-14 03:24:13,847] {fn_api_runner.py:437} INFO - Running (((((ref_AppliedPTransform_ReadData/Read_3)+((ref_AppliedPTransform_DecodeData/ParseTFExamples_5)+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/KeyWithVoid_8)))+((ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/ParDo(SplitHotCold)/ParDo(SplitHotCold)_12)+((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Transcode/0)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/0))))+((((ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ConvertInputToFeatureValuesWithWeights_32)+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_FlattenToSlicedFeatureNameValueTuples_33))+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/TopKUniques_CountSlicedFeatureNameValueTuple:PairWithVoid_35))+((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Precombine)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Write))))+((ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoDiscarding_13)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Precombine)))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Write)\n",
      "[2019-06-14 03:24:20,152] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/Merge))+((((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_CountSlicedFeatureNameValueTuple/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopKUniques_ModifyKeyToSlicedFeatureName_43))+((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Precombine)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Write)))+(((ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_DropValues_53)+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_55))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "[2019-06-14 03:24:20,182] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Group/Read)+(((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/Merge)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_GetTopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/TopK_ConvertToSingleFeatureStats_52)))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/0)\n",
      "[2019-06-14 03:24:20,200] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_63)))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Write/1)\n",
      "[2019-06-14 03:24:20,222] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesResults/Read)+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/DeserializeTopKUniquesFeatureStatsProto_65))+((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-14 03:24:20,242] {fn_api_runner.py:437} INFO - Running (GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Group/Read)+((((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/Merge)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PreCombineFn)/ExtractOutputs))+((ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Map(StripNonce)_21)+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/WindowIntoOriginal_22)))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Write/1))\n",
      "[2019-06-14 03:24:20,539] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/Flatten/Read)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "[2019-06-14 03:24:20,721] {fn_api_runner.py:437} INFO - Running ((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/Merge))+(((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/BasicStatsGenerator/CombinePerKey(PostCombineFn)/ExtractOutputs)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0))\n",
      "[2019-06-14 03:24:21,068] {fn_api_runner.py:437} INFO - Running (GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write))\n",
      "[2019-06-14 03:24:21,081] {fn_api_runner.py:437} INFO - Running (((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs)+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_74)))+(((ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_77)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write))\n",
      "[2019-06-14 03:24:21,116] {fn_api_runner.py:437} INFO - Running ((((GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_85))+(ref_PCollection_PCollection_51/Write)\n",
      "[2019-06-14 03:24:21,153] {fn_api_runner.py:437} INFO - Running (ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Read_87)+((ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_88)+((ref_AppliedPTransform_GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_89)+((ref_AppliedPTransform_WriteStatsOutput/Write/WriteImpl/Map(<lambda at iobase.py:984>)_96)+((ref_AppliedPTransform_WriteStatsOutput/Write/WriteImpl/WindowInto(WindowIntoFn)_97)+(WriteStatsOutput/Write/WriteImpl/GroupByKey/Write)))))\n",
      "[2019-06-14 03:24:21,196] {fn_api_runner.py:437} INFO - Running (WriteStatsOutput/Write/WriteImpl/GroupByKey/Read)+((ref_AppliedPTransform_WriteStatsOutput/Write/WriteImpl/WriteBundles_102)+(ref_PCollection_PCollection_62/Write))\n",
      "[2019-06-14 03:24:21,220] {fn_api_runner.py:437} INFO - Running ((ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput/Write/WriteImpl/PreFinalize_103))+(ref_PCollection_PCollection_63/Write)\n",
      "[2019-06-14 03:24:21,241] {fn_api_runner.py:437} INFO - Running (ref_PCollection_PCollection_55/Read)+(ref_AppliedPTransform_WriteStatsOutput/Write/WriteImpl/FinalizeWrite_104)\n",
      "[2019-06-14 03:24:21,252] {filebasedsink.py:290} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "[2019-06-14 03:24:21,356] {filebasedsink.py:327} INFO - Renamed 1 shards in 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 2. generate statistics from tfrecord\n",
    "from pathlib import Path\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "def generate_stats(transformed_examples_dir):\n",
    "    path = Path(transformed_examples_dir)\n",
    "    for filepath in path.glob('*'):\n",
    "        # since we are using python 3.5, not 3.6+\n",
    "        return tfdv.generate_statistics_from_tfrecord(str(filepath))\n",
    "\n",
    "stats = generate_stats(transformed_examples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.contentWindow.document.write(facets_html);\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. visualize statistics with tfdv\n",
    "\n",
    "tfdv.visualize_statistics(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
